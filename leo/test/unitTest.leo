<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: http://leoeditor.com/leo_toc.html -->
<leo_file xmlns:leo="http://leoeditor.com/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2"/>
<globals/>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="ekr.20070417092935"><vh>Startup</vh>
<v t="ekr.20140716121225.4354"><vh>@@button print-gnx</vh></v>
<v t="ekr.20161123080832.1"><vh>@button make-table</vh></v>
<v t="ekr.20111112092813.4154"><vh>@command cls</vh></v>
<v t="ekr.20101220161557.6011"><vh>@file unitTestStartup.txt</vh></v>
<v t="ekr.20150216110251.11"><vh>@persistence</vh></v>
<v t="ekr.20041121151002"><vh>@settings</vh>
<v t="ekr.20190113122443.1"><vh>@bool add-context-to-headlines = False</vh></v>
<v t="ekr.20080324133327.2"><vh>@bool allow-middle-button-paste = True</vh></v>
<v t="ekr.20161011095551.1"><vh>@bool allow-section-references-in-at-auto = True</vh></v>
<v t="ekr.20140902101931.4478"><vh>@bool qt-use-scintilla = False</vh></v>
<v t="ekr.20161129030232.1"><vh>@bool run-pyflakes-on-write = False</vh></v>
<v t="ekr.20140217055617.4231"><vh>@bool scriptingatscriptnodes = True</vh></v>
<v t="ekr.20180214042153.1"><vh>@bool select-next-after-delete = False</vh></v>
<v t="ekr.20051013162226"><vh>@bool test-setting = True</vh></v>
<v t="ekr.20150602215639.1"><vh>@bool tidy-autobeautify = False</vh></v>
<v t="ekr.20070503064257"><vh>@bool use-chapters = True</vh></v>
<v t="ekr.20150321155210.11"><vh>@bool verbose-check-outline = False</vh></v>
<v t="ekr.20181031152556.1"><vh>@bool write-script-file = True</vh></v>
<v t="ekr.20070528100318"><vh>@bool write-strips-blank-lines = False</vh></v>
<v t="ekr.20170415084531.1"><vh>@data import-html-tags</vh></v>
<v t="ekr.20111026111009.3972"><vh>@data import-xml-tags</vh></v>
<v t="ekr.20111123042627.6654"><vh>@enabled-plugins</vh></v>
<v t="ekr.20050328101834"><vh>@page http plugin</vh>
<v t="ekr.20050328101834.1"><vh>@bool http-active = False</vh></v>
<v t="ekr.20050328101834.2"><vh>@int port = 8080</vh></v>
<v t="ekr.20050328101834.3"><vh>@string rst-http-attributename = ''</vh></v>
</v>
<v t="ekr.20111112093605.4679"><vh>@shortcuts</vh></v>
<v t="ekr.20110521073115.3494"><vh>colorizer colors</vh>
<v t="ekr.20110521073115.3495"><vh>@@color keyword3-color = orange</vh></v>
<v t="ekr.20110521073115.3496"><vh>@@color keyword4-color = pink</vh></v>
</v>
<v t="ekr.20111124094121.3941"><vh>Empty @buttons and @commands nodes</vh>
<v t="ekr.20111124094121.3942"><vh>@buttons</vh></v>
<v t="ekr.20111124094121.3943"><vh>@commands</vh></v>
</v>
<v t="ekr.20171126153138.1"><vh>Required for reformat-paragraph tests</vh>
<v t="ekr.20171126152936.1"><vh>@int page-width = 80</vh></v>
<v t="ekr.20171126153044.1"><vh>@int tab-width = -4</vh></v>
</v>
<v t="ekr.20131111155830.4249"><vh>Vim settings</vh>
<v t="ekr.20131111155830.4250"><vh>@@@data vim-control-character-commands</vh></v>
<v t="ekr.20131111155830.4251"><vh>@data vim-command-tails</vh></v>
<v t="ekr.20131111155830.4252"><vh>@data vim-commands</vh></v>
<v t="ekr.20131111155830.4253"><vh>@data vim-motions</vh></v>
<v t="ekr.20131111155830.4254"><vh>@data vim-motion-tails</vh></v>
</v>
</v>
<v t="ekr.20100123172713.5114"><vh>Scripts</vh>
<v t="ekr.20100102164959.5088"><vh>Count pages</vh></v>
<v t="ekr.20100123172713.5116"><vh>Clean all tnodeLists</vh></v>
<v t="ekr.20070217065840"><vh>Scripts that make unit tests</vh>
<v t="ekr.20070217065840.1"><vh>@@command make-test @key = Alt-5</vh></v>
<v t="ekr.20070217065840.2"><vh>@@command do-before @key = Alt-6</vh>
<v t="ekr.20070217065840.3"><vh>getSel</vh></v>
<v t="ekr.20070217065840.4"><vh>findNodes</vh></v>
<v t="ekr.20070217065840.5"><vh>putSelectionInHeadline</vh></v>
</v>
<v t="ekr.20070217065840.6"><vh>@@command do-after @key = Alt-7</vh>
<v t="ekr.20070217072822"><vh>getSel</vh></v>
<v t="ekr.20070217065840.8"><vh>findNodes</vh></v>
<v t="ekr.20070217065840.9"><vh>putSelectionInHeadline</vh></v>
</v>
</v>
<v t="ekr.20070113145100"><vh>Create chinese folder</vh></v>
<v t="ekr.20071113140035"><vh>Find unique @ test nodes</vh></v>
<v t="ekr.20091206090247.5060"><vh>Clear all uA's, tnodeLists, etc.</vh>
<v t="ekr.20091206090247.5061"><vh>Clean unused tnodeLists</vh></v>
<v t="ekr.20091206090247.5062"><vh>Clear all timestamps</vh></v>
<v t="ekr.20091206090247.5063"><vh>Clear all uAs (unknown attributes)</vh></v>
</v>
</v>
</v>
<v t="ekr.20051012104957"><vh>@ignore Docs</vh>
<v t="bwmulder.20050108100437.1"><vh>How to run unit tests</vh></v>
<v t="ekr.20050618061835"><vh>How to use the @test directive, by Roger Erens</vh>
<v t="ekr.20050618061835.1"><vh>Intro</vh>
<v t="ekr.20050618061835.2"><vh>@url http://www.onlamp.com/pub/a/python/2005/02/03/tdd_pyunit2.html</vh></v>
</v>
<v t="ekr.20050618061835.3"><vh>Preparations: adding a button</vh>
<v t="ekr.20050618061835.4"><vh>@@button Do @test</vh></v>
</v>
<v t="ekr.20050618061835.5"><vh>Alpha</vh>
<v t="ekr.20050618061835.6"><vh>@test my first Leo test</vh></v>
<v t="ekr.20050618061835.7"><vh>output on the console</vh></v>
</v>
<v t="ekr.20050618061835.8"><vh>Bravo</vh>
<v t="ekr.20050618061835.9"><vh>@@test my second Leo test</vh></v>
<v t="ekr.20050618061835.10"><vh>output on the console</vh></v>
</v>
<v t="ekr.20050618061835.11"><vh>It takes two to tango</vh>
<v t="ekr.20050618061835.6"></v>
<v t="ekr.20050618061835.9"></v>
<v t="ekr.20050618061835.12"><vh>output on the console</vh></v>
</v>
<v t="ekr.20050618061835.13"><vh>Life gets more interesting</vh>
<v t="ekr.20050618061835.14"><vh>@@test koekiemonster.wants()</vh>
<v t="ekr.20050618061835.15"><vh>input data</vh></v>
<v t="ekr.20050618061835.16"><vh>expected result</vh></v>
</v>
<v t="ekr.20050618061835.17"><vh>output on the console</vh></v>
<v t="ekr.20050618061835.18"><vh>output on the console using print statements</vh></v>
</v>
<v t="ekr.20050618061835.19"><vh>How about @suite?</vh></v>
<v t="ekr.20050618061835.20"><vh>Final remarks</vh></v>
</v>
<v t="ekr.20111211094936.3970"><vh>@ignore To do</vh>
<v t="ekr.20111115080347.3872"><vh>To do: tests of the high-level interface</vh>
<v t="ekr.20100131171342.5478"><vh>@@@test that log and body implements high-level interface</vh></v>
</v>
<v t="ekr.20100131171342.5473"><vh>Tk gui tests</vh>
<v t="ekr.20100131171342.5474"><vh>@test leoBody is subset of leoTkBody</vh></v>
<v t="ekr.20100131171342.5475"><vh>@test leoFrame is subset of leoTkFrame</vh></v>
<v t="ekr.20100131171342.5476"><vh>@test leoGui is subset of leoTkGui</vh></v>
<v t="ekr.20100131171342.5477"><vh>@test leoTree is subset of leoTkTree</vh></v>
</v>
<v t="ekr.20111125183140.3952"><vh>@test ic.createOutline changes back-slashes to slashes</vh></v>
<v t="ekr.20111125182408.3947"><vh>@test ic.createImportParent changes back-slashes to slashes</vh></v>
</v>
</v>
<v t="ekr.20190113120618.1"><vh>Files</vh>
<v t="ekr.20190113120734.1"><vh>@asis unittest/at-asis-test.py</vh>
<v t="ekr.20190113120734.2"><vh>spam</vh></v>
<v t="ekr.20190113120734.3"><vh>eggs</vh></v>
</v>
<v t="ekr.20190113123439.5"><vh>@auto unittest/at-auto-line-number-test.py</vh></v>
<v t="ekr.20190113123439.7"><vh>@auto unittest/at-auto-md-line-number-test.md</vh></v>
<v t="ekr.20190113123439.11"><vh>@auto unittest/at-auto-section-ref-test.py</vh></v>
<v t="ekr.20190113121550.1"><vh>@auto unittest/at-auto-test.py</vh></v>
<v t="ekr.20190113123439.16"><vh>@auto unittest/at-auto-unit-test.py</vh></v>
<v t="ekr.20190113123635.2"><vh>@auto-org unittest/at-auto-org-line-number-test.org</vh></v>
<v t="ekr.20190113123635.5"><vh>@auto-otl unittest/at-auto-otl-line-number-test.otl</vh></v>
<v t="ekr.20190113123822.1"><vh>@clean unittest/at-clean-line-number-test.c</vh>
<v t="ekr.20190113123822.2"><vh>spam</vh></v>
<v t="ekr.20190113123822.3"><vh>eggs</vh></v>
</v>
<v t="ekr.20190113123853.1"><vh>@clean unittest/at-clean-line-number-test.py</vh>
<v t="ekr.20190113123853.2"><vh>spam</vh></v>
<v t="ekr.20190113123853.3"><vh>eggs</vh></v>
</v>
<v t="ekr.20160403123754.1"><vh>@file unittest/at-file-line-number-test.c</vh></v>
<v t="ekr.20080904102243.2"><vh>@file unittest/at-file-line-number-test.py</vh></v>
<v t="ekr.20111021115306.3697"><vh>@file unittest/tex-error.tex</vh></v>
<v t="ekr.20130912092638.4150"><vh>@file unittest/utf-16-test.txt</vh></v>
<v t="ekr.20190113124135.1"><vh>@nosent unittest/at-nosent-line-number-test.py</vh>
<v t="ekr.20190113124135.2"><vh>spam</vh></v>
<v t="ekr.20190113124135.3"><vh>eggs</vh></v>
</v>
<v t="ekr.20100731163237.5782"><vh>@thin unittest/at-thin-html-test.html</vh></v>
<v t="ekr.20090704085350.5022"><vh>@thin unittest/at-thin-test.py</vh></v>
</v>
<v t="ekr.20101220161557.6016"><vh>Active Unit Tests</vh>
<v t="edward.20160314170027.56" descendentVnodeUnknownAttributes="7d7100285808000000302e342e31322e3471017d71025809000000756e69745f7465737471035804000000616263647104735809000000302e342e31382e333571057d710658090000006d795f706c7567696e7107580300000076616c710873752e"><vh>@file activeUnitTests.txt</vh></v>
</v>
<v t="ekr.20190923014948.1"><vh>Failures when run from bridge w/ readSettings=False</vh>
<v t="ekr.20150430053825.1"><vh>@test abbrevCommands.next_place</vh>
<v t="ekr.20150430061225.1"><vh>child</vh></v>
</v>
<v t="ekr.20051107115231.18"><vh>@test paste and undo in headline - at end</vh></v>
<v t="ekr.20051107115231.20"><vh>@test paste and undo in headline - with selection</vh></v>
<v t="ekr.20051107115231.16"><vh>@test paste at end of headline</vh></v>
<v t="ekr.20060325071703.1"><vh>@test ifplatform</vh></v>
<v t="ekr.20111124090010.3939"><vh>@test g.app.config @buttons and @commands logic</vh></v>
<v t="ekr.20100212104817.5351"><vh>@test help-for-command</vh></v>
<v t="ekr.20100204165850.5373"><vh>@test most toggle commands</vh></v>
<v t="ekr.20051107115231.17"><vh>@test typing and undo in headline - at end</vh></v>
</v>
<v t="ekr.20150521123343.1"><vh>leoBeautify</vh>
<v t="ekr.20111104171708.3843"><vh>@test leoBeautify.CPrettyPrinter</vh>
<v t="ekr.20111104171708.3844"><vh>c tokenize test</vh></v>
</v>
<v t="ekr.20190912065312.1"><vh>@test SyntaxSanitizer</vh>
<v t="ekr.20190912105517.1"><vh>basic test</vh></v>
<v t="ekr.20190912160752.1"><vh>@others &amp; sections references</vh></v>
<v t="ekr.20190912071608.1"><vh>fast_at.scan_lines</vh>
<v t="ekr.20190912071608.2"><vh>&lt;&lt; init scan_lines &gt;&gt;</vh></v>
<v t="ekr.20190912071608.3"><vh>&lt;&lt; define dump_v &gt;&gt;</vh></v>
<v t="ekr.20190912071608.4"><vh>&lt;&lt; 1. common code for all lines &gt;&gt;</vh></v>
<v t="ekr.20190912071608.5"><vh>&lt;&lt; 2. short-circuit later tests &gt;&gt;</vh></v>
<v t="ekr.20190912071608.6"><vh>&lt;&lt; 3. handle @others &gt;&gt;</vh></v>
<v t="ekr.20190912071608.7"><vh>&lt;&lt; 4. handle section refs &gt;&gt;</vh></v>
<v t="ekr.20190912071608.8"><vh>&lt;&lt; handle node_start &gt;&gt;</vh></v>
<v t="ekr.20190912071608.9"><vh>&lt;&lt; handle end of @doc &amp; @code parts &gt;&gt;</vh></v>
<v t="ekr.20190912071608.10"><vh>&lt;&lt; handle @all &gt;&gt;</vh></v>
<v t="ekr.20190912071608.11"><vh>&lt;&lt; handle afterref &gt;&gt;</vh></v>
<v t="ekr.20190912071608.12"><vh>&lt;&lt; handle @first and @last &gt;&gt;</vh></v>
<v t="ekr.20190912071608.13"><vh>&lt;&lt; handle @comment &gt;&gt;</vh></v>
<v t="ekr.20190912071608.14"><vh>&lt;&lt; handle @delims &gt;&gt;</vh></v>
<v t="ekr.20190912071608.15"><vh>&lt;&lt; handle @raw &gt;&gt;</vh></v>
<v t="ekr.20190912071608.16"><vh>&lt;&lt; handle @-leo &gt;&gt;</vh></v>
<v t="ekr.20190912071608.17"><vh>&lt;&lt; Last 1. handle remaining @@ lines &gt;&gt;</vh></v>
<v t="ekr.20190912071608.18"><vh>&lt;&lt; Last 2. handle remaining @doc lines &gt;&gt;</vh></v>
<v t="ekr.20190912071608.19"><vh>&lt;&lt; Last 3. handle remaining @ lines &gt;&gt;</vh></v>
</v>
</v>
<v t="ekr.20150610064911.1"><vh>@test beautifier (small) </vh>
<v t="ekr.20150610064930.1"><vh>before</vh></v>
<v t="ekr.20150610065241.1"><vh>after</vh></v>
</v>
<v t="ekr.20190914104955.1"><vh>@test black</vh>
<v t="ekr.20190914104955.2"><vh>basic test</vh></v>
<v t="ekr.20190914104955.3"><vh>@others &amp; sections references</vh></v>
<v t="ekr.20190914104955.4"><vh>fast_at.scan_lines</vh>
<v t="ekr.20190914104955.5"><vh>&lt;&lt; init scan_lines &gt;&gt;</vh></v>
<v t="ekr.20190914104955.6"><vh>&lt;&lt; define dump_v &gt;&gt;</vh></v>
<v t="ekr.20190914104955.7"><vh>&lt;&lt; 1. common code for all lines &gt;&gt;</vh></v>
<v t="ekr.20190914104955.8"><vh>&lt;&lt; 2. short-circuit later tests &gt;&gt;</vh></v>
<v t="ekr.20190914104955.9"><vh>&lt;&lt; 3. handle @others &gt;&gt;</vh></v>
<v t="ekr.20190914104955.10"><vh>&lt;&lt; 4. handle section refs &gt;&gt;</vh></v>
<v t="ekr.20190914104955.11"><vh>&lt;&lt; handle node_start &gt;&gt;</vh></v>
<v t="ekr.20190914104955.12"><vh>&lt;&lt; handle end of @doc &amp; @code parts &gt;&gt;</vh></v>
<v t="ekr.20190914104955.13"><vh>&lt;&lt; handle @all &gt;&gt;</vh></v>
<v t="ekr.20190914104955.14"><vh>&lt;&lt; handle afterref &gt;&gt;</vh></v>
<v t="ekr.20190914104955.15"><vh>&lt;&lt; handle @first and @last &gt;&gt;</vh></v>
<v t="ekr.20190914104955.16"><vh>&lt;&lt; handle @comment &gt;&gt;</vh></v>
<v t="ekr.20190914104955.17"><vh>&lt;&lt; handle @delims &gt;&gt;</vh></v>
<v t="ekr.20190914104955.18"><vh>&lt;&lt; handle @raw &gt;&gt;</vh></v>
<v t="ekr.20190914104955.19"><vh>&lt;&lt; handle @-leo &gt;&gt;</vh></v>
<v t="ekr.20190914104955.20"><vh>&lt;&lt; Last 1. handle remaining @@ lines &gt;&gt;</vh></v>
<v t="ekr.20190914104955.21"><vh>&lt;&lt; Last 2. handle remaining @doc lines &gt;&gt;</vh></v>
<v t="ekr.20190914104955.22"><vh>&lt;&lt; Last 3. handle remaining @ lines &gt;&gt;</vh></v>
</v>
</v>
<v t="ekr.20190914105123.1"><vh>@test ptb (large)</vh>
<v t="ekr.20190914105123.2"><vh>basic test</vh></v>
<v t="ekr.20190914105123.3"><vh>@others &amp; sections references</vh></v>
<v t="ekr.20190914105123.4"><vh>fast_at.scan_lines</vh>
<v t="ekr.20190914105123.5"><vh>&lt;&lt; init scan_lines &gt;&gt;</vh></v>
<v t="ekr.20190914105123.6"><vh>&lt;&lt; define dump_v &gt;&gt;</vh></v>
<v t="ekr.20190914105123.7"><vh>&lt;&lt; 1. common code for all lines &gt;&gt;</vh></v>
<v t="ekr.20190914105123.8"><vh>&lt;&lt; 2. short-circuit later tests &gt;&gt;</vh></v>
<v t="ekr.20190914105123.9"><vh>&lt;&lt; 3. handle @others &gt;&gt;</vh></v>
<v t="ekr.20190914105123.10"><vh>&lt;&lt; 4. handle section refs &gt;&gt;</vh></v>
<v t="ekr.20190914105123.11"><vh>&lt;&lt; handle node_start &gt;&gt;</vh></v>
<v t="ekr.20190914105123.12"><vh>&lt;&lt; handle end of @doc &amp; @code parts &gt;&gt;</vh></v>
<v t="ekr.20190914105123.13"><vh>&lt;&lt; handle @all &gt;&gt;</vh></v>
<v t="ekr.20190914105123.14"><vh>&lt;&lt; handle afterref &gt;&gt;</vh></v>
<v t="ekr.20190914105123.15"><vh>&lt;&lt; handle @first and @last &gt;&gt;</vh></v>
<v t="ekr.20190914105123.16"><vh>&lt;&lt; handle @comment &gt;&gt;</vh></v>
<v t="ekr.20190914105123.17"><vh>&lt;&lt; handle @delims &gt;&gt;</vh></v>
<v t="ekr.20190914105123.18"><vh>&lt;&lt; handle @raw &gt;&gt;</vh></v>
<v t="ekr.20190914105123.19"><vh>&lt;&lt; handle @-leo &gt;&gt;</vh></v>
<v t="ekr.20190914105123.20"><vh>&lt;&lt; Last 1. handle remaining @@ lines &gt;&gt;</vh></v>
<v t="ekr.20190914105123.21"><vh>&lt;&lt; Last 2. handle remaining @doc lines &gt;&gt;</vh></v>
<v t="ekr.20190914105123.22"><vh>&lt;&lt; Last 3. handle remaining @ lines &gt;&gt;</vh></v>
</v>
</v>
<v t="ekr.20190914111357.1"><vh>@test ptb (pet peeves) </vh>
<v t="ekr.20190915083933.1"><vh>peeve: two spaces before a trailing comment</vh>
<v t="ekr.20190915083945.1"><vh>test</vh></v>
<v t="ekr.20190915084039.1"><vh>expected</vh></v>
</v>
<v t="ekr.20190914112029.1"><vh>peeve: ws after unary minus</vh>
<v t="ekr.20190914111357.2"><vh>test</vh></v>
<v t="ekr.20190914111357.3"><vh>expected</vh></v>
</v>
<v t="ekr.20190915074109.1"><vh>peeve: ws around arithmetic ops in argument lists</vh>
<v t="ekr.20190915074201.1"><vh>test</vh></v>
<v t="ekr.20190915074205.1"><vh>expected</vh></v>
</v>
<v t="ekr.20190915083243.1"><vh>peeve: ws around word ops</vh>
<v t="ekr.20190915083412.1"><vh>test</vh></v>
<v t="ekr.20190915083418.1"><vh>expected</vh></v>
</v>
<v t="ekr.20190914120318.1"><vh>peeve: ws before comma, semicolon, colon</vh>
<v t="ekr.20190914120402.1"><vh>test</vh></v>
<v t="ekr.20190914120406.1"><vh>expected</vh></v>
</v>
<v t="ekr.20190914123001.1"><vh>peeve: ws before parens</vh>
<v t="ekr.20190914123101.1"><vh>test</vh></v>
<v t="ekr.20190914123104.1"><vh>expected</vh></v>
</v>
<v t="ekr.20190914115821.1"><vh>peeve: ws between trailing comma &amp; close paren</vh>
<v t="ekr.20190914120010.1"><vh>test</vh></v>
<v t="ekr.20190914120016.1"><vh>expected</vh></v>
</v>
<v t="ekr.20190914181159.1"><vh>peeve: ws in slices: arithmetic ops</vh>
<v t="ekr.20190914181159.2"><vh>test</vh></v>
<v t="ekr.20190914181159.3"><vh>expected</vh></v>
</v>
<v t="ekr.20190914181344.1"><vh>peeve: ws in slices: function calls</vh>
<v t="ekr.20190914181344.2"><vh>test</vh></v>
<v t="ekr.20190914181344.3"><vh>expected</vh></v>
</v>
<v t="ekr.20190914122333.1"><vh>peeve: ws in slices: numbers &amp; words</vh>
<v t="ekr.20190914122356.1"><vh>test</vh></v>
<v t="ekr.20190914122359.1"><vh>expected</vh></v>
</v>
<v t="ekr.20190914115215.1"><vh>peeve: ws inside parentheses, brackets or braces.</vh>
<v t="ekr.20190914115300.1"><vh>test</vh></v>
<v t="ekr.20190914115304.1"><vh>expected</vh></v>
</v>
</v>
<v t="ekr.20190914132845.1"><vh>@test black (pet peeves) </vh>
<v t="ekr.20190914132845.2"><vh>peeve: ws in slices</vh>
<v t="ekr.20190914132845.3"><vh>test</vh></v>
<v t="ekr.20190914132845.4"><vh>expected</vh></v>
</v>
<v t="ekr.20190914132845.5"><vh>peeve: ws after unary minus</vh>
<v t="ekr.20190914132845.6"><vh>test</vh></v>
<v t="ekr.20190914132845.7"><vh>expected</vh></v>
</v>
<v t="ekr.20190914132845.8"><vh>peeve: ws between trailing comma &amp; close paren</vh>
<v t="ekr.20190914132845.9"><vh>test</vh></v>
<v t="ekr.20190914132845.10"><vh>expected</vh></v>
</v>
<v t="ekr.20190914132845.11"><vh>peeve: ws inside parentheses, brackets or braces.</vh>
<v t="ekr.20190914132845.12"><vh>test</vh></v>
<v t="ekr.20190914132845.13"><vh>expected</vh></v>
</v>
<v t="ekr.20190914132845.14"><vh>peeve: ws before comma, semicolon, colon</vh>
<v t="ekr.20190914132845.15"><vh>test</vh></v>
<v t="ekr.20190914132845.16"><vh>expected</vh></v>
</v>
<v t="ekr.20190914132845.17"><vh>peeve:  ws before parens</vh>
<v t="ekr.20190914132845.18"><vh>test</vh></v>
<v t="ekr.20190914132845.19"><vh>expected</vh></v>
</v>
</v>
<v t="ekr.20191106035017.1"><vh>@test all beautifiers (bs-nl &amp; indentation)</vh></v>
<v t="ekr.20191106072009.1"><vh>@test fstringify tests</vh></v>
<v t="ekr.20191106195303.1"><vh>@test fstringify should fail</vh></v>
</v>
<v t="ekr.20160318094003.1"><vh>leoAst</vh>
<v t="ekr.20160318094009.1"><vh>@test Python3 features</vh></v>
<v t="ekr.20160523094102.1"><vh>@test leoAst traverser classes</vh></v>
</v>
<v t="ekr.20191121063148.1"><vh>@test tokens: tog vs asttokens</vh></v>
<v t="ekr.20191121163350.1"><vh>@test nodes: tog vs asttokens (fails)</vh></v>
<v t="ekr.20191125235556.1"><vh>@test TOG: Leo's core files</vh>
<v t="ekr.20191125235556.2"><vh>&lt;&lt; helper functions &gt;&gt;</vh>
<v t="ekr.20191125235556.3"><vh>filter_reports</vh></v>
<v t="ekr.20191125235556.4"><vh>make_tests</vh></v>
<v t="ekr.20191125235556.5"><vh>reload_helper</vh></v>
<v t="ekr.20191125235556.6"><vh>run_tests</vh></v>
<v t="ekr.20191125235556.7"><vh>show_status</vh></v>
<v t="ekr.20191125235556.8"><vh>summarize</vh></v>
</v>
</v>
<v t="ekr.20191118154059.1"><vh>@test TokenOrder classes</vh>
<v t="ekr.20191125063105.1"><vh>&lt;&lt; helper functions &gt;&gt;</vh>
<v t="ekr.20191118154839.1"><vh>make_tests</vh></v>
<v t="ekr.20191125065230.1"><vh>reload_helper</vh></v>
<v t="ekr.20191125065806.1"><vh>run_tests &amp; filter reports</vh></v>
<v t="ekr.20191125063255.1"><vh>show_status</vh></v>
<v t="ekr.20191125064740.1"><vh>summarize</vh></v>
</v>
<v t="ekr.20191129042702.1"><vh>test: simple f-string tests</vh></v>
<v t="ekr.20191126002258.1"><vh>xxtest: f-string summary: no joins</vh>
<v t="ekr.20191129035530.1"><vh>more</vh></v>
</v>
<v t="ekr.20191129033147.1"><vh>xx test: f-string summary: joins, no f-expr</vh></v>
<v t="ekr.20191129033245.1"><vh>xx test: f-string summary: joins + 1 f-expr</vh></v>
<v t="ekr.20191129033359.1"><vh>xxtest: f-string summary: joins + 2 f-exprs</vh></v>
<v t="ekr.20191127134726.1"><vh>xx test: leoFind.py: line 861</vh></v>
<v t="ekr.20191126084046.1"><vh>ignore: f-strings</vh>
<v t="ekr.20191125165640.1"><vh>test: f-string + Ternary</vh></v>
<v t="ekr.20191128141922.1"><vh>test: print one f-string</vh></v>
</v>
<v t="ekr.20191127134236.1"><vh>ignore: plain strings</vh>
<v t="ekr.20191126143046.1"><vh>test: empty string</vh></v>
<v t="ekr.20191127172440.1"><vh>test: escaped string delims</vh></v>
<v t="ekr.20191125235415.1"><vh>test: string concatentation</vh></v>
<v t="ekr.20191118155611.1"><vh>test: string with % op</vh></v>
</v>
<v t="ekr.20191128104415.1"><vh>ignore: passed tests</vh></v>
<v t="ekr.20191122201558.1"><vh>ignore: Categories</vh>
<v t="ekr.20191120074811.1"><vh>Contexts...</vh>
<v t="ekr.20191119133927.1"><vh>test: ClassDef</vh></v>
<v t="ekr.20191122203522.1"><vh>test: ClassDef, FunctionDef</vh></v>
<v t="ekr.20191119142414.1"><vh>test: FunctionDef &amp; NamedConstant</vh></v>
</v>
<v t="ekr.20191120075520.1"><vh>Expressions &amp; operators...</vh>
<v t="ekr.20191118155612.1"><vh>test: attribute</vh></v>
<v t="ekr.20191119211714.1"><vh>test: CompareOp</vh></v>
<v t="ekr.20191119213940.1"><vh>test: ListComp and comprehension</vh></v>
<v t="ekr.20191122153258.1"><vh>test: NamedConstant</vh></v>
<v t="ekr.20191118160305.1"><vh>test: Operator: semicolon</vh></v>
<v t="ekr.20191118155612.2"><vh>test: Operator: semicolon between statements</vh></v>
<v t="ekr.20191119210429.1"><vh>test: UnaryOp</vh></v>
</v>
<v t="ekr.20191124040736.1"><vh>Statements...</vh>
<v t="ekr.20191122210522.1"><vh>If tests...</vh>
<v t="ekr.20191119213126.1"><vh>test: For, Tuple</vh></v>
<v t="ekr.20191123210801.1"><vh>test: if unary op</vh></v>
<v t="ekr.20191120171921.1"><vh>test: if, elif</vh></v>
<v t="ekr.20191122213610.1"><vh>test: if, elif + 2</vh></v>
<v t="ekr.20191120064442.1"><vh>test: if, elif, else</vh></v>
<v t="ekr.20191120132531.1"><vh>test: if, else</vh></v>
<v t="ekr.20191122210035.1"><vh>test: if, else, if</vh></v>
<v t="ekr.20191120164539.1"><vh>test: Nested If's</vh></v>
<v t="ekr.20191125161333.1"><vh>test: ternary + if</vh></v>
</v>
<v t="ekr.20191128013505.1"><vh>Call tests...</vh>
<v t="ekr.20191119143517.1"><vh>test: Call, arguments &amp; keyword</vh></v>
<v t="ekr.20191119150253.1"><vh>test: Call, arguments &amp; Starred</vh></v>
<v t="ekr.20191119101124.1"><vh>test: Call, Str</vh></v>
</v>
<v t="ekr.20191125144727.1"><vh>test: simple print</vh></v>
<v t="ekr.20191120163559.1"><vh>test: Try</vh></v>
</v>
</v>
<v t="ekr.20191125151346.1"><vh>fail: file tests</vh>
<v t="ekr.20191118155609.1"><vh>file: core..runLeo.py</vh></v>
<v t="ekr.20191119203945.1"><vh>file: core..leoFind.py</vh></v>
<v t="ekr.20191123203957.1"><vh>file: core..leoGlobals.py</vh></v>
</v>
</v>
</vnodes>
<tnodes>
<t tx="bwmulder.20050108100437.1">@killcolor

Running unit tests from test.leo is easy, provided you have enabled the
Scripting plugin. When this plugin is enabled Leo will create a blue 'script
button' in the icon bar called 'unit test'. 

- To run all unit tests, select the node in test.leo called 'Unit tests...',
then do &lt;alt-4&gt; 

- To run a single test, select an @test node and do &lt;alt-4&gt;.

- To run a suite of tests, select an @suite node and do &lt;alt-4&gt;.

- To run any other collection of tests, create an outline containing those @test
or @suite nodes, select the root of that tree and do &lt;alt-4&gt; .

Several nodes in the tree @thin ../src/leoTest.py (in test.leo) contain support
code for @test, @suite, etc. so if you want all the gory details you can read
the code. It's not complicated: Leo creates UnitTest classes automatically whose
run method is the body of the @suite or @test node.
</t>
<t tx="ekr.20041121151002"># Many of these are required for unit tests.
# Do not change them without running all unit tests.</t>
<t tx="ekr.20050328101834"></t>
<t tx="ekr.20050328101834.1"></t>
<t tx="ekr.20050328101834.2"></t>
<t tx="ekr.20050328101834.3"></t>
<t tx="ekr.20050618061835">@killcolor

Here is a tutorial written by Roger Erens.

Version Date        LeoID       Remarks
------- ----        -----       -------
0.1     20050519    rogererens  Initial version</t>
<t tx="ekr.20050618061835.1">So you think "Well, since I've written this piece of funky Python software, and everybody keeps saying how useful unit testing is, I really ought to start using unit tests." And since Leo's creator has said countless times in the Leo forums how easy unit testing in Leo is, a few uncomplicated examples might help convince you that he's not spamming.

Beware: this How-To should be the last time that you write tests AFTER having written your funky software! Test Driven Development dictates that tests have to be prepared BEFORE you get down to writing your actual code. See a nice tutorial on O'Reilly's website (url given in the descendant node).</t>
<t tx="ekr.20050618061835.10">Start of Do @test
F
======================================================================
FAIL: @test my second Leo test

----------------------------------------------------------------------
Traceback (most recent call last):
  File "C:\Documents and Settings\re1705\My Documents\PythonStuff\leo\src\leoTes
t.py", line 148, in runTest
    exec script + '\n' in {'c':c,'g':g,'p':p}
  File "&lt;string&gt;", line 3, in ?
AssertionError

----------------------------------------------------------------------
Ran 1 test in 0.010s

FAILED (failures=1)
End of Do @test</t>
<t tx="ekr.20050618061835.11">A real fun feature of Leo is that Leo saves you from having to select each and single @test node and press the 'Do @test' button to obtain testing results. Just collecting the @test nodes under an organizing node, selecting that organizing node, and pressing the 'Do @test' button will suffice.
Of course, this was one of the key ideas of unit testing, but it's nice to see it being implemented by Leo so smoothly!

So, press the button while having this node selected, and see if your console's output matches the third child node more or less. </t>
<t tx="ekr.20050618061835.12">Start of Do @test
.F
======================================================================
FAIL: @test my second Leo test

----------------------------------------------------------------------
Traceback (most recent call last):
  File "C:\leo\src\leoTest.py", line 148, in runTest
    exec script + '\n' in {'c':c,'g':g,'p':p}
  File "&lt;string&gt;", line 3, in ?
AssertionError

----------------------------------------------------------------------
Ran 2 tests in 0.040s

FAILED (failures=1)
End of Do @test</t>
<t tx="ekr.20050618061835.13">Okay: so you've seen now some simple stand-alone tests to get your toes wet.
Now, we get to the 'grande finale' and see real-life usage of the @test nodes.

The @test child node below illustrates the following points:

1.  The node imports the module to test (and keeps it up to date by reloading it).
2.  It also obtains data to use as input and referral. This is what you might call
    the setUp methods in traditional unit tests. If more tests need the same data,
    you can put the nodes in a central place where all the @test nodes can find
    them. Likewise, common code for several unit tests might be collected in a
    central place.
3.  Comparable with the traditional unit tests' tearDown method, some statements
    can follow the test itself, if neccessary. You might also consider using a
    try/finally construct.
4.  I have used g.es() statements instead of print statements, since the latter
    clutter the console. Try replacing the g.es() statements by corresponding
    print statements to see what I mean. With more than one @test node being tested,
    this will give quite a dreadful look.</t>
<t tx="ekr.20050618061835.14">@color

try:
    # SETUP
    import koekiemonster # this module defines a function want() which we want to test
    reload(koekiemonster) # changes in koekiemonster need to propagate to the test

    # obtaining the input parameter for the function
    inputNode = p.firstChild()
    inputData = inputNode.b
    
    # obtaining the expected result from the function with above input parameter
    expectedResultNode = inputNode.next()
    expected = expectedResultNode.b
    
    # execute the function with above input parameter
    result = koekiemonster.wants(inputData)
    
    # TEST
    assert(result == expected)
    
    # TEARDOWN
    g.es("Now it's time to clean up")

except AssertionError:
    # TEARDOWN
    g.es("Oh oh! %s failed:" % p.h.strip())
    g.es("koekiemonster.wants(%s)==%s" % (inputData, result))
    g.es("Expected: %s" % expected)
    raise # pass the exception on to the unit test machinery</t>
<t tx="ekr.20050618061835.15">vegetables</t>
<t tx="ekr.20050618061835.16">Yuck!</t>
<t tx="ekr.20050618061835.17">Start of Do @test
F
======================================================================
FAIL: @test koekiemonster.wants()

----------------------------------------------------------------------
Traceback (most recent call last):
  File "C:\leo\src\leoTest.py", line 148, in runTest
    exec script + '\n' in {'c':c,'g':g,'p':p}
  File "&lt;string&gt;", line 22, in ?
AssertionError

----------------------------------------------------------------------
Ran 1 test in 0.070s

FAILED (failures=1)
End of Do @test</t>
<t tx="ekr.20050618061835.18">Start of Do @test
Oh oh: @test koekiemonster.wants() failed:
koekiemonster.wants(vegetables)=Yack!
Expected: Yuck!
F
======================================================================
FAIL: @test koekiemonster.wants()

----------------------------------------------------------------------
Traceback (most recent call last):
  File "C:\leo\src\leoTest.py", line 148, in runTest
    exec script + '\n' in {'c':c,'g':g,'p':p}
  File "&lt;string&gt;", line 22, in ?
AssertionError

----------------------------------------------------------------------
Ran 1 test in 0.051s

FAILED (failures=1)
End of Do @test</t>
<t tx="ekr.20050618061835.19">With the aforementioned possibilities of running all @test nodes in a subtree
with a single click on a button, the sharing of input/expected data, and the
sharing of setUp/tearDown code, there hardly seems a need for the @suite nodes.

On Leo's website
(http://webpages.charter.net/edreamleo/scripting.html#unit-testing-with-test-and-suite)
I could still find some extra info on @suite nodes, not found in LeoDocs.leo:

&lt;quote&gt;

Using @suite nodes

Such nodes create a suite of tests. Leo executes the script in these nodes similar to @test nodes, but Leo assumes that the script in an @suite node will do the following:

    * Create a suite of unit tests
    * Put the suite in g.app.scriptDict["suite"]

The difference between @test and @suite is:

    * Leo creates a unit test from script in an @test node by creating an instance of generalTestCase, a subclass of unittest.TestCase.
    * The script in an @suite node creates the test suite.

An @suite nodes allows us to create "legacy" unit tests simply. For example,
rather than rewriting all the reformatParagraph unit tests to use @test nodes, I
simply wrote an @suite node with the following body.

    suite = c.testManager.makeReformatParagraphSuite()
    g.app.scriptDict['suite'] = suite

&lt;/quote&gt;

Well, that's almost, but not quite completely, totally incomprehensible to me.
Fortunately, since Leo's creator removed most of this quote from LeoDocs.leo and
test.leo, this information appears to be outdated
.</t>
<t tx="ekr.20050618061835.2"></t>
<t tx="ekr.20050618061835.20">Don't forget to read the node
Users Guide--&gt;Chapter 7: Scripting Leo with Python--&gt;Unit testing with @test, @suite
in LeoDocs.leo, especially the final notes. Also investigate a little further in test.leo to see some heavy weight lifting.

This concludes my How-To on using @test. It was not written by an expert on (unit) testing;
it is more or less the result of keeping notes on my way to find out how I could get started
with unit tests in Leo. Maybe it's useful for other newbies as well.

Please post any remarks on one of the Leo Fora, and if deemed helpful, I'll be glad to incorporate them into a next version.

Happy testing!</t>
<t tx="ekr.20050618061835.3">Comment by EKR: There is no need to do this if you use test.leo for your unit tests:  just use the unit test button.

From the console, start up a Leo instance. On MS Windows: go to the folder in which you installed Leo 4.3, open up the 'src' folder and double click the 'leo.py' file. Opening a command window, and typing "python leo.py" may be another way.

Use the File--&gt;Open... menu item to open the outline containing your code.

Choose a good-looking position to insert a new node called 'Test zone'. Copy the child node of the node you're reading right now, and paste it as a child node of 'Test zone'. The code it contains is essentially the same as found in the node
Unit tests...--&gt;Do @test
in the file 'test.leo' in the 'test' folder. I just added my 0.2 cents by adding some starting and finishing remarks. Remove them if they clutter your console too much.

Also, make sure that you have an entry 'Scripting' in the 'Plugins' menu (enable the plugin if neccessary). Now is a good time to save your leo file.

With the 'Scripting' plugin (also known as the 'mod_scripting' plugin) enabled, the net effect will be that the next time you open your outline, a blue button with the caption 'Do @test' shows up in the tool bar of Leo, if your screen is wide enough.
To get the button right away in the tool bar, I have to assume that you started Leo with the scripting plugin enabled. In this case, a yellow button with the caption 'script Button' can be pressed while having the node '@button Do @test' selected. Note: the resulting button will not be blue, but pink. Removing a button from the tool bar can be done by right-clicking it.

We'll see the use of this added button soon, but before that, read up on the 'assert' function in the Python manuals, since it is used a lot in testing. It won't be long before you're back here!</t>
<t tx="ekr.20050618061835.4">@color

g.pr("\nStart of Do @test")
c.testManager.doTests(all=False)
g.pr("End of Do @test")</t>
<t tx="ekr.20050618061835.5">The first child node of the node you're reading right now, contains the simplest succeeding test possible.
Select it, and press the button 'Do @test'.
You can find the verbatim result as it got sent to my console in the second child node.

A few things are worth noting here:

1.  The node containing the test must have its headline start with '@test'.
2.  Whenever the 'assert' statement finds out that the expression given to it
    is 'True', a test passes successfully. There are more ways to pass a test, but
    for now, let us stick to the use of assert functions.
3.  A passed test is denoted with a single dot in the output. See the line between
    'Start of Do @test' and the line filled with dashes. This is compatible with the
    way traditional unit testing shows its progress.
4.  Below the dashed line in the output, a summary is printed. This one surely gives
    us a reason to lean back for a moment, and congratualate ourselves with another
    piece of robust, funky code!</t>
<t tx="ekr.20050618061835.6">@color
assert(True)</t>
<t tx="ekr.20050618061835.7">Start of Do @test
.
----------------------------------------------------------------------
Ran 1 test in 0.010s

OK
End of Do @test</t>
<t tx="ekr.20050618061835.8">Now that you've seen a passing test, it should be obvious to imagine how a failing test would look like. See the first child node, and then run it by pressing the 'Do @test' button as before.

Instead of a dot denoting success, we get an 'F' denoting a failure on the line following 'Start of Do @test'. When running a lot of tests, the next part, following the lines filled with '='s, helps to identify which test failed.
It also includes a trace back, but for AssertionErrors, it does not seem to offer much added value to me right now. On second thought: when you have multiple asserts in a test, the trace back can tell you which assert function failed. Still, I would choose for only one assert function per test and get rid of this trace back altogether. This would keep the console much cleaner, IMHO.</t>
<t tx="ekr.20050618061835.9">@color
assert('Spam' == 'Ham')</t>
<t tx="ekr.20051012104957">@nosearch</t>
<t tx="ekr.20051013162226"></t>
<t tx="ekr.20051107115231.16">import sys
if sys.platform.startswith('linux'):
    self.skipTest('Not for Linux')

k = c.keyHandler
frame = c.frame ; tree = frame.tree ; canvas = tree.canvas
h = 'Test headline abc'
p = c.testManager.findNodeAnywhere(h)
assert p,'node not found: %s' % h
c.redrawAndEdit(p) # To make node visible
w = c.edit_widget(p)
g.app.gui.set_focus(c,w)
w2 = g.app.gui.get_focus(c)
try:
    assert w
    paste = 'ABC'
    g.app.gui.replaceClipboardWith(paste)
    g.app.gui.set_focus(c,w)
    w2 = g.app.gui.get_focus(c)
    w.setSelectionRange('end','end')
    if g.app.gui.guiName() == 'curses':
        c.frame.pasteText(event=g.Bunch(widget=w))
    else:
        stroke = k.getStrokeForCommandName('paste-text')
        if stroke is None:
            self.skipTest('no binding for paste-text') # #1345
        k.manufactureKeyPressForCommandName(w,'paste-text')
        g.app.gui.event_generate(c,'\n','Return',w)
    assert p.h == h + paste,'Expected: %s, got %s' % (
        h + paste,p.h)
finally:
    if 1:
        c.setHeadString(p,h) # Essential
        c.redraw(p)
</t>
<t tx="ekr.20051107115231.17">import sys
if sys.platform.startswith('linux'):
    self.skipTest('skip headline test')
if g.app.gui.guiName() == 'curses':
    # This could be adapted, but not now.
    self.skipTest('Not for curses gui')
k = c.k
if k.defaultUnboundKeyAction != 'insert':
    self.skipTest('defaultUnboundKeyAction != insert')
if not k.getStrokeForCommandName('undo'):
    self.skipTest('no settings')

frame = c.frame ; tree = frame.tree ; canvas = tree.canvas
h = 'Test headline abc'
p = c.testManager.findNodeAnywhere(h)
assert p,'node not found: %s' % h
c.redrawAndEdit(p) # To make the node visible.
w = c.edit_widget(p)
try:
    assert w, 'oops1'
    wName = g.app.gui.widget_name(w)
    assert wName.startswith('head'),'w.name:%s' % wName
    w.setSelectionRange('end','end')
    g.app.gui.event_generate(c,'X','Shift+X',w)
    g.app.gui.event_generate(c,'Y','Shift+Y',w)
    g.app.gui.event_generate(c,'Z','Shift+Z',w)
    g.app.gui.event_generate(c,'\n','Return',w)
    assert p.h == h + 'XYZ',(
        'oops2: expected: %s, got: %s' % (
            h + 'XYZ',p.h))
    if g.app.gui.guiName() != 'nullGui':
        assert c.undoer.undoMenuLabel == 'Undo Typing','oops3: %s' % (
            c.undoer.undoMenuLabel)
    k.manufactureKeyPressForCommandName(w,'undo')
    if g.app.gui.guiName() != 'nullGui':
        assert c.undoer.redoMenuLabel == 'Redo Typing','oops4'
    assert p.h == h,'oops5 got: %s, expected: %s' % (
        p.h,h)
finally:
    if 1:
        c.setHeadString(p,h) # Essential
        c.redraw(p)
</t>
<t tx="ekr.20051107115231.18">import sys
if sys.platform.startswith('linux'):
    self.skipTest('Not for Linux')

k = c.keyHandler
h = 'Test headline abc'
p = c.testManager.findNodeAnywhere(h)
assert p,'node not found: %s' % h
frame = c.frame
tree = frame.tree
canvas = tree.canvas
c.redrawAndEdit(p) # To make node visible
w = c.edit_widget(p)
try:
    assert w,'oops1'
    w.setSelectionRange('end','end')
    paste = 'ABC'
    g.app.gui.replaceClipboardWith(paste)
    w.setSelectionRange('end','end')
    if g.app.gui.guiName() == 'curses':
        c.frame.pasteText(event=g.Bunch(widget=w))
    else:
        stroke = k.getStrokeForCommandName('paste-text')
        if stroke is None:
            self.skipTest('no binding for paste-text') # #1345
        k.manufactureKeyPressForCommandName(w,'paste-text')
        g.app.gui.event_generate(c,'\n','Return',w)
    assert p.h == h + paste,'oops2 got: %s' % p.h
    k.manufactureKeyPressForCommandName(w,'undo')
    assert p.h == h,'oops3 got: %s' % p.h
finally:
    if 1:
        c.setHeadString(p,h) # Essential
        c.redraw(p)
</t>
<t tx="ekr.20051107115231.20">import sys
if sys.platform.startswith('linux'):
    self.skipTest('skip headline test')
else:
    k = c.keyHandler
    frame = c.frame ; tree = frame.tree ; canvas = tree.canvas
    h = 'Test headline abc'
    p = c.testManager.findNodeAnywhere(h)
    assert p,'node not found: %s' % h
    c.redraw(p) # To make node visible
    tree.editLabel(p)
    w = c.edit_widget(p)
    try:
        assert w, 'Null w'
        paste = 'ABC'
        g.app.gui.replaceClipboardWith(paste)
        w.setSelectionRange('1.1','1.2')
        if g.app.gui.guiName() == 'curses':
            c.frame.pasteText(event=g.Bunch(widget=w))
        else:
            stroke = k.getStrokeForCommandName('paste-text')
            if stroke is None:
                self.skipTest('no binding for paste-text') # #1345
            k.manufactureKeyPressForCommandName(w,'paste-text')
            g.app.gui.event_generate(c,'\n','Return',w)
        assert p.h == h[0] + paste + h[2:]
        k.manufactureKeyPressForCommandName(w,'undo')
        assert p.h == h, 'head mismatch'
    finally:
        if 1:
            c.setHeadString(p,h) # Essential
            c.redraw(p)
</t>
<t tx="ekr.20060325071703.1">import sys

win32  = c.config.getBool('test_win32_setting')
darwin = c.config.getBool('test_darwin_setting')

if win32 is None and darwin is None:
    self.skipTest('settings not loaded') # #1345

if sys.platform == 'win32':
    assert(win32)
    assert(not darwin)

elif sys.platform== 'darwin':
    assert(not win32)
    assert(darwin)

</t>
<t tx="ekr.20070113145100"># Not part of cvs distributions, but needed for two unit tests.

dir = g.os_path_join(g.app.loadDir,'..','test','unittest',g.u('chinese\u8116folder'),encoding='utf-8')
s   = g.os_path_join(dir,g.u('chinese\u8116test.leo'),encoding='utf-8')
    
if not g.os_path_exists(dir):
    import os
    os.mkdir(dir)
    g.pr('created chinese folder')
    
if not g.os_path_exists(s):
    f = file(s,'w')
    f.close()
    g.pr('created chinese file')
    
</t>
<t tx="ekr.20070217065840">@nocolor-node

@
To make unit tests, do the following:
    
- Use the make-test script (Alt-5) to create a suboutline for a unit test.
- Put text in the before node, selected desired text, then do the do-before script (Alt-6).
- Execute the command, then do the do-after script (Alt-7).
</t>
<t tx="ekr.20070217065840.1">try:
    p1 = p.insertAfter()
    c.setHeadString(p1,'@test ')
    body = 'c.testManager.runEditCommandTest(c,p)'
    c.setBodyString(p1,body)
    for s in ('work','before','after'):
        p2 = p1.insertAsLastChild()
        c.setHeadString(p2,s)
    p1.expand()
finally:
    c.redraw()
    c.editPosition(p1)</t>
<t tx="ekr.20070217065840.2">@
p should be in tree whose root is a @test node containing 'work', 'before' and
'after' children. The work node should have body text. If all is as expected,
copy the body text the work node to the before node, and represent the selection
range of the work in the headline of the before node.
@c

@others

sel = getSel(c)
top,work,before,after = findNodes(p)
if top and work.b:

    c.setBodyString(before,work.b)
    c.setBodyString(after,'')
    putSelectionInHeadline(c,before,'before',sel)
    c.redraw()
else:
    g.es_print('do-before: not in a proper @test tree')</t>
<t tx="ekr.20070217065840.3">def getSel(c):
    
    w = c.frame.body.bodyCtrl
    i,j= w.getSelectionRange()
    if i == j:
        i = j = w.getInsertPoint()
        sel = (i,i)
    return i,j</t>
<t tx="ekr.20070217065840.4">def findNodes(p):
    
    '''Find the top, work, before and after nodes.
    p should be in tree whose root is a @test node containing
    'work', 'before' and 'after' children.'''
    
    for p in p.self_and_parents_iter():
        if p.h.startswith('@test '):
            break
    top    = p and p.copy()
    work   = top and top.firstChild() 
    before = work and work.next()     
    after  = before and before.next()
    if (
        work   and work.h.startswith('work') and
        before and before.h.startswith('before') and
        after  and after.h.startswith('after')
    ):
        return top,work,before,after
    else:
        return None,None,None,None</t>
<t tx="ekr.20070217065840.5">def putSelectionInHeadline (c,p,prefix,sel):
    
    # g.trace(p.h,repr(sel))

    w = c.frame.body.bodyCtrl
    i,j = sel
    i,j = w.toGuiIndex(i),w.toGuiIndex(j)
    s = '%s sel=%s,%s' % (prefix,i,j)
    c.setHeadString(p,s)
</t>
<t tx="ekr.20070217065840.6">@
p should be in tree whose root is a @test node containing 'work', 'before' and
'after' children. If all is as expected, copy the work node to the after node,
and represent the selection range of the work node in the headline of the after node.
@c

@others

sel = getSel(c)
top,work,before,after = findNodes(p)
if top:
    c.setBodyString(after,work.b)
    putSelectionInHeadline(c,after,'after',sel)
    c.redraw()
else:
    g.es_print('do-after: not in @test tree')</t>
<t tx="ekr.20070217065840.8">def findNodes(p):
    
    '''Find the top, work, before and after nodes.
    p should be in tree whose root is a @test node containing
    'work', 'before' and 'after' children.'''
    
    for p in p.self_and_parents_iter():
        if p.h.startswith('@test '):
            break
    top    = p and p.copy()
    work   = top and top.firstChild()
    before = work and work.next()
    after  = before and before.next()
    if (
        work   and work.h.startswith('work') and
        before and before.h.startswith('before') and
        after  and after.h.startswith('after')
    ):
        return top,work,before,after
    else:
        return None,None,None,None</t>
<t tx="ekr.20070217065840.9">def putSelectionInHeadline (c,p,prefix,sel):
    
    # g.trace(p.h,repr(sel))
    
    w = c.frame.body.bodyCtrl
    i,j = sel
    i,j = w.toGuiIndex(i),w.toGuiIndex(j)
    s = '%s sel=%s,%s' % (prefix,i,j)
    c.setHeadString(p,s)
</t>
<t tx="ekr.20070217072822">def getSel(c):
    
    w = c.frame.body.bodyCtrl
    i,j= w.getSelectionRange()
    if i == j:
        i = j = w.getInsertPoint()
        sel = (i,i)
    return i,j</t>
<t tx="ekr.20070417092935">@nosearch</t>
<t tx="ekr.20070503064257"></t>
<t tx="ekr.20070528100318"># Required to make a typing test work.
</t>
<t tx="ekr.20071113140035">fn = g.os_path_abspath(g.os_path_join(g.app.loadDir,'..','core','leoPy.leo'))
assert g.os_path_exists(fn),fn
c1 = c
c2 = g.openWithFileName(fn,old_c=None,enableLog=False)
assert c2
c.frame.bringToFront()
g.app.setLog(c.frame.log)

d1 = {} ; d2 = {}
for c,d in ( (c1,d1),(c2,d2)):
    for p in c.all_unique_positions():
        if p.h.startswith('@test'):
            d[p.h]=p.h

if 0: # not important
    g.pr()
    g.pr('----- Only in unitTest.leo')
    for h in sorted(d1.keys()):
        if not d2.get(h):
            print(h)

print('\n----- Only in leoPy.leo')
for h in sorted(d2.keys()):
    if not d1.get(h):
        print(h)</t>
<t tx="ekr.20080324133327.2">True: allow linux-like pastes using a mouse's middle button.

Important: this may cause crashes on some platforms.
</t>
<t tx="ekr.20091206090247.5060"># Use these with caution.</t>
<t tx="ekr.20091206090247.5061">count = 0
for p in c.all_unique_positions():
    count += 1
    # Empty tnodeLists are not errors because they never get written to the .leo file.
    v = p.v
    if hasattr(v,"tnodeList") and len(v.tnodeList) &gt; 0 and not v.isAnyAtFileNode():
        g.es("deleting tnodeList for " + `v`,color="blue")
        delattr(v,"tnodeList")
        c.setChanged(True)

s = "%d nodes" % count
print(s) ; g.es(s)</t>
<t tx="ekr.20091206090247.5062"># About the only time you should run this script is when:
# - changing the format of timestamps in nodeIndices.setTimestamp or
# - when making a retroactive change to leoID.txt.

if 0: # This is usually a very bad idea.

    for p in c.all_positions():
        p.v.fileIndex = None

    g.es("all timestamps cleared")</t>
<t tx="ekr.20091206090247.5063">doDelete = False
put = g.es_print
for p in c.all_positions():
    if p.v.u:
        put("found v.u:",p.h,
            g.listToString(p.v.u.keys()))
        if doDelete:
            p.v.u = None
put('done') 
c.redraw()</t>
<t tx="ekr.20100102164959.5088">nodes = 0 ; lines = 0
for p in c.all_unique_positions():
    nodes += 1
    lines += len(g.splitLines(p.b))

pages = ((nodes * 10) + lines) / 50
s = "%d nodes,  %d lines, %d pages" % (nodes,lines,pages)
print(s); g.es(s)</t>
<t tx="ekr.20100123172713.5114"></t>
<t tx="ekr.20100123172713.5116">count = 0
for p in c.all_unique_positions():
    count += 1
    # Empty tnodeLists are not errors because they never get written to the .leo file.
    v = p.v
    if hasattr(v,"tnodeList"): # and len(v.tnodeList) &gt; 0 and not v.isAnyAtFileNode():
        g.es("deleting tnodeList for " + `v`,color="blue")
        delattr(v,"tnodeList")
        c.setChanged(True)

s = "%d nodes" % count
print s ; g.es(s)</t>
<t tx="ekr.20100131171342.5473"></t>
<t tx="ekr.20100131171342.5474">if g.app.gui.guiName() == 'tkinter':

    pc = g.app.pluginsController
    tkGui = pc.loadOnePlugin('leo.plugins.tkGui',verbose=False)
    assert(tkGui)

    import leo.core.leoFrame as leoFrame
    import inspect,sys

    baseClass = leoFrame.leoBody
    subClasses  = (tkGui.leoTkinterBody,leoFrame.nullBody)
    baseObject = c.frame.body

    methods = inspect.getmembers(baseClass,inspect.ismethod)
    methodNames = [z[0] for z in methods]

    for name in baseObject.mustBeDefinedOnlyInBaseClass:
        try:
            assert name in methodNames, 'not defined in base class %s.%s' % (baseClass.__name__,name)
        except AssertionError:
            exctype, value = sys.exc_info()[:2]
            print(value)
            raise

    for subClass in subClasses:
        subclassName = subClass.__name__
        for name in methodNames:
            base_func = getattr(baseClass,name)
            sub_func =  getattr(subClass,name)
            try:
                if name in baseObject.mustBeDefinedOnlyInBaseClass:
                    assert base_func.im_func == sub_func.im_func, 'defined in subclass %s.%s' % (subclassName,name)
                if name in baseObject.mustBeDefinedInSubclasses:
                    assert base_func.im_func != sub_func.im_func, 'not defined in subclass %s.%s' % (subclassName,name)
            except AssertionError:
                #raise
                exctype, value = sys.exc_info()[:2]
                print(value)
</t>
<t tx="ekr.20100131171342.5475">if g.app.gui.guiName() == 'tkinter':

    pc = g.app.pluginsController
    tkGui = pc.loadOnePlugin('leo.plugins.tkGui',verbose=False)

    import leo.core.leoFrame as leoFrame
    import inspect

    baseClass = leoFrame.leoFrame
    subClasses  = (tkGui.leoTkinterFrame,leoFrame.NullFrame)
    baseObject = c.frame

    methods = inspect.getmembers(baseClass,inspect.ismethod)
    methodNames = [z[0] for z in methods]

    for name in baseObject.mustBeDefinedOnlyInBaseClass:
        assert name in methodNames, 'not defined in base class %s.%s' % (baseClass.__name__,name)

    for subClass in subClasses:
        subclassName = subClass.__name__
        for name in methodNames:
            base_func = getattr(baseClass,name)
            sub_func =  getattr(subClass,name)
            if name in baseObject.mustBeDefinedOnlyInBaseClass:
                assert base_func.im_func == sub_func.im_func, 'defined in subclass %s.%s' % (subclassName,name)
            if name in baseObject.mustBeDefinedInSubclasses:
                assert base_func.im_func != sub_func.im_func, 'not defined in subclass %s.%s' % (subclassName,name)
</t>
<t tx="ekr.20100131171342.5476">if g.app.gui.guiName() == 'tkinter':

    pc = g.app.pluginsController
    tkGui = pc.loadOnePlugin('leo.plugins.tkGui',verbose=False)

    import leo.core.leoGui as leoGui
    import inspect

    baseClass = leoGui.leoGui
    subClasses  = (tkGui.tkinterGui,) # nullGui can inherit almost all leoGui dummy methods.
    baseObject = g.app.gui

    methods = inspect.getmembers(baseClass,inspect.ismethod)
    methodNames = [z[0] for z in methods]

    for name in baseObject.mustBeDefinedOnlyInBaseClass:
        assert name in methodNames, 'not defined in base class %s.%s' % (baseClass.__name__,name)

    for subClass in subClasses:
        subclassName = subClass.__name__
        for name in methodNames:
            base_func = getattr(baseClass,name)
            sub_func =  getattr(subClass,name)
            try:
                if name in baseObject.mustBeDefinedOnlyInBaseClass:
                    assert base_func.im_func == sub_func.im_func, 'defined in subclass %s.%s' % (subclassName,name)
                if name in baseObject.mustBeDefinedInSubclasses:
                    assert base_func.im_func != sub_func.im_func, 'not defined in subclass %s.%s' % (subclassName,name)
            except AssertionError:
                raise
</t>
<t tx="ekr.20100131171342.5477">if g.app.gui.guiName() == 'tkinter':

    pc = g.app.pluginsController
    tkGui = pc.loadOnePlugin('leo.plugins.tkGui',verbose=False)

    import leo.core.leoFrame as leoFrame
    import inspect

    baseClass = leoFrame.leoTree
    subClasses  = (tkGui.leoTkinterTree,leoFrame.nullTree)
    baseObject = c.frame.tree

    methods = inspect.getmembers(baseClass,inspect.ismethod)
    methodNames = [z[0] for z in methods]

    for name in baseObject.mustBeDefinedOnlyInBaseClass:
        assert name in methodNames, 'not defined in base class %s.%s' % (baseClass.__name__,name)

    for subClass in subClasses:
        subclassName = subClass.__name__
        for name in methodNames:
            base_func = getattr(baseClass,name)
            sub_func =  getattr(subClass,name)
            if name in baseObject.mustBeDefinedOnlyInBaseClass:
                assert base_func.im_func == sub_func.im_func, 'defined in subclass %s.%s' % (subclassName,name)
            if name in baseObject.mustBeDefinedInSubclasses:
                assert base_func.im_func != sub_func.im_func, 'not defined in subclass %s.%s' % (subclassName,name)
</t>
<t tx="ekr.20100131171342.5478">logCtrl = c.frame.log.logCtrl

table = (
    ('mustBeDefinedInSubclasses',logCtrl.mustBeDefinedInSubclasses),
    ('mustBeDefinedInBaseClass',logCtrl.mustBeDefinedOnlyInBaseClass),
    ('mustBeDefined',logCtrl.mustBeDefined),
)

# Check existence.
for tag,aList in table:
    for z in aList:
        assert hasattr(c.frame.log,z),'%s %s %s' % (tag,c.frame.log,z)
        assert hasattr(c.frame.body,z),'%s %s %s' % (tag,c.frame.body,z)

# Check signatures.
import inspect
for tag,aList in table:
    for z in aList:
        func = getattr(c.frame.body.bodyCtrl,z)
        func2 = getattr(c.frame.log.logCtrl,z)
        assert func,z
        assert func2,z
        d1 = inspect.getargspec(func)
        d2 = inspect.getargspec(func2)
        assert d1==d2,'\n%s\n\nd1 %s\n\nd2 %s' % (z,d1,d2)
</t>
<t tx="ekr.20100204165850.5373">if g.app.inBridge:
    self.skipTest('in bridge')
k = c.k
colorizer = c.frame.body.getColorizer()
ed = c.editCommands
# These don't set ivars
    # 'toggle-active-pane'),
    # 'toggle-angle-brackets',
    # 'toggle-input-state'),
    # 'toggle-mini-buffer'),
    # 'toggle-split-direction'),
table = [
    (k,'abbrevOn','toggle-abbrev-mode'),
    (ed,'extendMode','toggle-extend-mode'),
]
# Not valid for external tests.
table2 = [
    (k,'enable_autocompleter','toggle-autocompleter'),
    (k,'enable_calltips','toggle-calltips'),
    (c,'sparse_find','toggle-find-collapses-nodes'),
    (colorizer,'showInvisibles','toggle-invisibles'),
    (c,'sparse_move','toggle-sparse-move'),
]
if not g.app.isExternalUnitTest:
    table.extend(table2)
for obj,ivar,command in table:
    val1 = getattr(obj,ivar)
    try:
        k.simulateCommand(command)
        val2 = getattr(obj,ivar)
        assert val2 == (not val1),'failed 1 %s' % command
        k.simulateCommand(command)
        val3 = getattr(obj,ivar)
        assert val3 == val1,'failed 2 %s' % command
    finally:
        setattr(obj,ivar,val1)
</t>
<t tx="ekr.20100212104817.5351">result = c.helpCommands.getBindingsForCommand('help')
if not result:
    self.skipTest('no settings')
assert result.strip().lower()=='f1', repr(result)
</t>
<t tx="ekr.20101220161557.6016"># Nov. 2016: 878 tests.
# Nov. 2017: 916 tests, 25 skipped.

# Some tests are disabled when g.app.isExternalUnitTest is True.
# Using self.skipTest(reason) is now preferred.
</t>
<t tx="ekr.20110521073115.3494"></t>
<t tx="ekr.20110521073115.3495">builtins, including cython builtins
</t>
<t tx="ekr.20110521073115.3496">cython keywords
</t>
<t tx="ekr.20111026111009.3972"># lowercase xml tags, one per line.

html
body
head
div
table
nodeA
nodeB
</t>
<t tx="ekr.20111104171708.3843">import leo.core.leoBeautify as leoBeautify
cpp = leoBeautify.CPrettyPrinter(c)
fn = 'c tokenize test'
p2 = g.findNodeInTree(c,p,fn)
assert p2,'not found: %s' % (fn)

if 1: # test of indent.
    # import os ; os.system('cls')
    cpp.indent(p2)
if 0: # test of tokenize.
    aList = cpp.tokenize(p2.b)
    assert(p2.b == ''.join(aList))
</t>
<t tx="ekr.20111104171708.3844">@language c

static exit_values_ty indent_main_loop(void)
{
    codes_ty         hd_type         = code_eof;
    char           * t_ptr           = NULL;
    codes_ty         type_code       = start_token;
    exit_values_ty   file_exit_value = total_success;
    int              dec_ind         = 0; /* current indentation for declarations */

    BOOLEAN          scase           = false; /* true when we've just see a "case";
                                               * determines what to do with the
                                               * following colon */
    BOOLEAN          flushed_nl;              /* Used when buffering up comments to remember that
                                               * a newline was passed over */
    BOOLEAN          sp_sw           = false; /* true when in the expression part of if(...),
                                               * while(...), etc. */
    BOOLEAN          force_nl        = false;

    /* last_token_ends_sp: True if we have just encountered the end of an if (...),
     * etc. (i.e. the ')' of the if (...) was the last token).  The variable is
     * set to 2 in the middle of the main token reading loop and is decremented
     * at the beginning of the loop, so it will reach zero when the second token
     * after the ')' is read.
     */

    BOOLEAN          last_token_ends_sp = false;

    BOOLEAN          last_else = false; /* true if last keyword was an else */

    for (;;)
    {
        /* this is the main loop.  it will go until
         * we reach eof */

        BOOLEAN is_procname_definition;
        bb_code_ty can_break;

        if (type_code != newline)
        {
            can_break = parser_state_tos-&gt;can_break;
        }

        parser_state_tos-&gt;last_saw_nl = false;
        parser_state_tos-&gt;can_break = bb_none;

        type_code = lexi ();    /* lexi reads one token.  "token" points to
                                 * the actual characters. lexi returns a code
                                 * indicating the type of token */

        /* If the last time around we output an identifier or
         * a paren, then consider breaking the line here if it's
         * too long.
         *
         * A similar check is performed at the end of the loop, after
         * we've put the token on the line. */

        if ((settings.max_col &gt; 0) &amp;&amp;
            (buf_break != NULL) &amp;&amp;
            ( ( (parser_state_tos-&gt;last_token == ident) &amp;&amp;
                (type_code != comma) &amp;&amp;
                (type_code != semicolon) &amp;&amp;
                (type_code != newline) &amp;&amp;
                (type_code != form_feed) &amp;&amp;
                (type_code != rparen) &amp;&amp;
                (type_code != struct_delim)) ||
              ( (parser_state_tos-&gt;last_token == rparen) &amp;&amp;
                (type_code != comma) &amp;&amp;
                (type_code != rparen) ) ) &amp;&amp;
            (output_line_length () &gt; settings.max_col))
        {
            break_line = 1;
        }

        if (last_token_ends_sp &gt; 0)
        {
            last_token_ends_sp--;
        }

        is_procname_definition =
                (((parser_state_tos-&gt;procname[0] != '\0') &amp;&amp;
                  parser_state_tos-&gt;in_parameter_declaration) ||
                 (parser_state_tos-&gt;classname[0] != '\0'));

        /* The following code moves everything following an if (), while (),
         * else, etc. up to the start of the following stmt to a buffer. This
         * allows proper handling of both kinds of brace placement.
         */

        flushed_nl = false;

        if (!search_brace(&amp;type_code, &amp;force_nl, &amp;flushed_nl, &amp;last_else, &amp;is_procname_definition))
        {
            /* Hit EOF unexpectedly in comment. */
            return indent_punt;
        }
        
        if (type_code == code_eof)
        {
            /* we got eof */
            if (s_lab != e_lab || s_code != e_code || s_com != e_com)   /* must dump end of line */
            {
                dump_line(true, &amp;paren_target);
            }

            if (parser_state_tos-&gt;tos &gt; 1)      /* check for balanced braces */
            {
                ERROR (_("Unexpected end of file"), 0, 0);
                file_exit_value = indent_error;
            }

            if (settings.verbose)
            {
                printf (_("There were %d non-blank output lines and %d comments\n"),
                        (int) out_lines, (int) com_lines);
                if (com_lines &gt; 0 &amp;&amp; code_lines &gt; 0)
                {
                    printf (_("(Lines with comments)/(Lines with code): %6.3f\n"),
                            (1.0 * com_lines) / code_lines);
                }
            }
            flush_output ();

            return file_exit_value;                                              /* RETURN */
        }

        if ((type_code != comment) &amp;&amp;
            (type_code != cplus_comment) &amp;&amp;
            (type_code != newline) &amp;&amp;
            (type_code != preesc) &amp;&amp;
            (type_code != form_feed))
        {
            if (force_nl &amp;&amp;
                (type_code != semicolon) &amp;&amp;
                ( (type_code != lbrace) ||
                  (!parser_state_tos-&gt;in_decl &amp;&amp; !settings.btype_2) ||
                  (parser_state_tos-&gt;in_decl &amp;&amp; !settings.braces_on_struct_decl_line) ||
                  (parser_state_tos-&gt;last_token == rbrace)))
            {
                if (settings.verbose &amp;&amp; !flushed_nl)
                {
                    WARNING (_("Line broken 2"), 0, 0);
                }

                flushed_nl = false;
                dump_line(true, &amp;paren_target);
                parser_state_tos-&gt;want_blank = false;
                force_nl = false;
            }

            parser_state_tos-&gt;in_stmt = true;   /* turn on flag which causes
                                                 * an extra level of
                                                 * indentation. this is
                                                 * turned off by a ; or } */
            if (s_com != e_com)
            {
                /* the code has an embedded comment in the
                 * line. Move it from the com buffer to the
                 * code buffer.
                 *
                 * Do not add a space before the comment if it is the first
                 * thing on the line.
                 */

                if (e_code != s_code)
                {
                    set_buf_break (bb_embedded_comment_start, paren_target);
                    *e_code++ = ' ';
                    embedded_comment_on_line = 2;
                }
                else
                {
                    embedded_comment_on_line = 1;
                }

                for (t_ptr = s_com; *t_ptr; ++t_ptr)
                {
                    check_code_size();
                    *e_code++ = *t_ptr;
                }

                set_buf_break (bb_embedded_comment_end, paren_target);
                *e_code++ = ' ';
                *e_code = '\0'; /* null terminate code sect */
                parser_state_tos-&gt;want_blank = false;
                e_com = s_com;
            }
        }
        else if ((type_code != comment) &amp;&amp;
                 (type_code != cplus_comment) &amp;&amp;
                 !(settings.break_function_decl_args &amp;&amp;
                   (parser_state_tos-&gt;last_token == comma)) &amp;&amp;
                 !( (parser_state_tos-&gt;last_token == comma) &amp;&amp;
                    !settings.leave_comma))
        {
            /* preserve force_nl thru a comment but
             * cancel forced newline after newline, form feed, etc.
             * however, don't cancel if last thing seen was comma-newline
             * and -bc flag is on. */

            force_nl = false;
        }

        /* Main switch on type of token scanned */

        check_code_size();
        
        /* now, decide what to do with the token */

        handle_the_token(type_code, &amp;scase, &amp;force_nl, &amp;sp_sw, &amp;flushed_nl,
                         &amp;hd_type, &amp;dec_ind, &amp;last_token_ends_sp, &amp;file_exit_value,
                         can_break, &amp;last_else, is_procname_definition);
        
        *e_code = '\0';         /* make sure code section is null terminated */

        if ((type_code != comment) &amp;&amp;
            (type_code != cplus_comment) &amp;&amp;
            (type_code != newline) &amp;&amp;
            (type_code != preesc) &amp;&amp;
            (type_code != form_feed))
        {
            parser_state_tos-&gt;last_token = type_code;
        }

        /* Now that we've put the token on the line (in most cases),
         * consider breaking the line because it's too long.
         *
         * Don't consider the cases of `unary_op', newlines,
         * declaration types (int, etc.), if, while, for,
         * identifiers (handled at the beginning of the loop),
         * periods, or preprocessor commands. */

        if ((settings.max_col &gt; 0) &amp;&amp; (buf_break != NULL))
        {
            if ( ( (type_code == binary_op) ||
                   (type_code == postop) ||
                   (type_code == question) ||
                   ((type_code == colon) &amp;&amp; (scase || (squest &lt;= 0))) ||
                   (type_code == semicolon) ||
                   (type_code == sp_nparen) ||
                   (type_code == sp_else) ||
                   ((type_code == ident) &amp;&amp; (*token == '\"')) ||
                   (type_code == struct_delim) ||
                   (type_code == comma)) &amp;&amp;
                 (output_line_length () &gt; settings.max_col))
            {
                break_line = 1;
            }
        }
    }                           /* end of main infinite loop */
}
</t>
<t tx="ekr.20111112092813.4154">g.cls()</t>
<t tx="ekr.20111112093605.4679"># leoSettings.leo no longer sets any bindings for run-xxx-unit-test.
# These are now EKR's preferred settings everywhere:
# there should be little need to run unit tests externally.

run-selected-unit-tests-locally     = Alt-4
run-marked-unit-tests-locally       = Alt-5
run-all-unit-tests-locally          = Alt-6

# Important: Alt-9 is used by a unit test
</t>
<t tx="ekr.20111115080347.3872"></t>
<t tx="ekr.20111123042627.6654"># Leo loads plugins in the order they appear here.

# **Important**: to change these defaults, put
# an @enabled-plugins node in myLeoSettings.leo.

# Highly-recommended plugins:
plugins_menu.py
free_layout.py # needs to be early
viewrendered.py

# Recommended plugins:
### contextmenu.py
# leo_to_html.py
mod_scripting.py
# nav_qt.py
# quicksearch.py
# stickynotes.py
# todo.py
</t>
<t tx="ekr.20111124090010.3939">if g.app.gui.guiName() == 'browser':
    self.skipTest('browser gui')
if g.app.isExternalUnitTest:
    self.skipTest('Can not be run externally')
if getattr(g.app, 'isBrowserTest', None):
    # Set only in test_browser_gui.py.
    self.skipTest('Browser Gui test')
d = g.app.config.unitTestDict # Always created for this unit test.
keys = ('config.doButtons-file-names','config.doCommands-file-names')
for key in keys:
    aList = d.get(key,[])
    if 'leoSettings' not in aList:
        self.skipTest('no settings') # #1345
    for base in ('leoSettings', 'unitTest'):
        for ext in ('.leo', '.db'):
            if base+ext in aList:
                break
        else:
            print('key', key, 'ext', ext, 'base', base)
            g.printObj(aList)
            assert False,'%s not in unitTestDict[%s]' % (base,key)
</t>
<t tx="ekr.20111124094121.3941"># These exist for a unit test.</t>
<t tx="ekr.20111124094121.3942"></t>
<t tx="ekr.20111124094121.3943"></t>
<t tx="ekr.20111125182408.3947">def setup():
    while p.hasChildren():
        p.firstChild().doDelete()

setup()

try:
    files = (r'a\b.c',r'a\b.h',)
    c.importCommands.createImportParent(p,files)
    child = p.firstChild()
    assert child
    assert child.h == 'a/b',child.h
finally:
    setup()</t>
<t tx="ekr.20111125183140.3952">child = p.firstChild()
def setup():
    while p.hasChildren():
        p.firstChild().doDelete()

setup()
try:
    c.importCommands.createOutline(
        fileName=r'a\b\c.xyzzy',
        parent=p,
        atAuto=False,atShadow=False,
        s='test body',
        ext='xyzzy'
    )
    child = p.firstChild()
    assert child
    h = g.os_path_finalize_join(g.app.loadDir,'..','test','a','b','c.xyzzy')
    h = h.replace('\\','/')
    h = '@file ' + h
    # C: vs c: is not relevant here.
    assert child.h.lower() == h.lower(),child.h
finally:
    setup()</t>
<t tx="ekr.20111211094936.3970"></t>
<t tx="ekr.20131111155830.4249"></t>
<t tx="ekr.20131111155830.4250"># Not yet...

    &lt;BS&gt;        delete the character in front of the cursor
N   &lt;Del&gt;       delete N characters under and after the cursor
    &lt;Del&gt;       delete the character under the cursor
    &lt;Del&gt;       while entering a count: delete last character
    &lt;Down&gt;      recall newer command-line that starts with current command
    &lt;Esc&gt;       abandon command-line (if 'wildchar' is &lt;Esc&gt;, type it twice)
    &lt;Left&gt;      (motion) cursor left
    &lt;Right&gt;     (motion) cursor right
    &lt;S-Down&gt;    recall newer command-line from history
    &lt;S-Left&gt;    (motion) cursor one word left
    &lt;S-Right&gt;   (motion) cursor one word right
    &lt;S-Up&gt;      recall older command-line from history
    &lt;Up&gt;        recall older command-line that starts with current command

N   CTRL-^                  Edit alternate file N (equivalent to ":e #N").
N   CTRL-A                  add N to the number at or after the cursor
N   CTRL-B                  window N pages Backwards (upwards)
    CTRL-B                  (motion?) cursor to beginning of command-line
    CTRL-BREAK              MS-DOS: during searches: interrupt the search
    CTRL-C                  during searches: interrupt the search
N   CTRL-D                  window N lines Downwards (default: 1/2 window)
N   CTRL-E                  window N lines downwards (default: 1)
    CTRL-E                  (motion?) cursor to end of command-line
N   CTRL-F                  (motion) window N pages Forwards (downwards)
    CTRL-G                  show current file name (with path) and cursor position
N   CTRL-I                  (motion) go to Nth newer position in jump list
    CTRL-K {char1} {char2}  enter digraph
    CTRL-L                  Clear and redraw the screen.
N   CTRL-O                  (motion) go to Nth older position in jump list
N   CTRL-R                  redo last N undone changes
    CTRL-R &lt;0-9a-z"%:-&gt;     insert contents of register &lt;0-9a-z"%:-&gt;
N   CTRL-T                  (motion) Jump back from Nth older tag in tag list
N   CTRL-U                  window N lines Upwards (default: 1/2 window)
    CTRL-U                  remove all characters
    CTRL-V                  highlight blockwise or stop highlighting
    CTRL-V                  start highlighting blockwise   }  highlighted text
    CTRL-V {char}           insert {char} literally
    CTRL-V {number}         enter decimal value of character (up to three digits)
    CTRL-W                  delete the word in front of the cursor
    CTRL-W +                Increase current window height
    CTRL-W -                Decrease current window height
    CTRL-W =                Make all windows equal height
    CTRL-W CTRL-W           Move cursor to window below (wrap)
    CTRL-W CTRL-^           Split window and edit alternate file
    CTRL-W R                Rotate windows upwards
    CTRL-W W                Move cursor to window above (wrap)
    CTRL-W ]                Split window and jump to tag under cursor
    CTRL-W _                Set current window height (default: very high)
    CTRL-W b                Move cursor to bottom window
    CTRL-W c  or :cl[ose]   Make buffer hidden and close window
    CTRL-W f                Split window and edit file name under the cursor
    CTRL-W j                Move cursor to window below
    CTRL-W k                Move cursor to window above
    CTRL-W n  or :new       Create new empty window
    CTRL-W o  or :on[ly]    Make current window only one on the screen
    CTRL-W p                Move cursor to previous active window
    CTRL-W q  or :q[uit]    Quit editing and close window
    CTRL-W r                Rotate windows downwards
    CTRL-W s                Split window into two parts
    CTRL-W t                Move cursor to top window
    CTRL-W x                Exchange current window with next one
N   CTRL-X                  subtract N from the number at or after the cursor
N   CTRL-Y                  window N lines upwards (default: 1)
    CTRL-Z                  Same as ":stop!"
    CTRL-]                  Jump to the tag under cursor, unless changes have been made</t>
<t tx="ekr.20131111155830.4251">char F
char T
char f
char r
char t
letter m
letter q
motion &lt;
motion &gt;
motion c
motion d
motion gU
motion gq
motion gu
motion g~
motion y
pattern /
pattern ?
register @
</t>
<t tx="ekr.20131111155830.4252"># http://tnerual.eriogerg.free.fr/vimqrc.html
vim_0 0
vim_tilda ~
vim_plus +
vim_underscore _
vim_minus -
vim_comma ,
vim_dot .
vim_semicolon ;
vim_lparen (
vim_rparen )
vim_lcurly {
vim_rcurly }
vim_vertical |
vim_backtick `
vim_dollar $
vim_caret ^
vim_percent %
vim_langle &lt;
vim_langle &lt;&lt;
vim_rangle &gt;
vim_rangle &gt;&gt;
vim_pound #
vim_star *
vim_slash /\\n
vim_slash /
vim_question ?\\n
vim_question ?
vim_at @
vim_at @@
vim_dquote "
vim_lsquare [#
vim_lsquare [(
vim_lsquare [*
vim_lsquare [[
vim_lsquare []
vim_lsquare [p
vim_lsquare [{
vim_rsquare ]#
vim_rsquare ])
vim_rsquare ]*
vim_rsquare ][
vim_rsquare ]]
vim_rsquare ]p
vim_rsquare ]}
vim_A A
vim_B B
vim_C C
vim_D D
vim_E E
vim_F F
vim_G G
vim_H H
vim_I I
vim_J J
vim_K K
vim_M M
vim_L L
vim_N N
vim_O O
vim_P P
vim_R R
vim_S S
vim_T T
vim_U U
vim_V V
vim_W W
vim_X X
vim_Y Y
vim_Z ZQ
vim_Z ZZ
vim_a a
vim_b b
vim_c c
vim_d dd
vim_d d
vim_g g~
vim_g g^
vim_g g#
vim_g g$
vim_g g*
vim_g g0
vim_g gD
vim_g gE
vim_g gI
vim_g gU
vim_g ga
vim_g gd
vim_g ge
vim_g gf
vim_g gg
vim_g gj
vim_g gk
vim_g gq
vim_g gs
vim_g gu
vim_g gv
vim_h h
vim_i i
vim_j j
vim_k k
vim_l l
vim_n n
vim_m m
vim_o o
vim_p p
vim_q q
vim_r r
vim_s s
vim_t t
vim_u u
vim_v v
vim_w w
vim_x x
vim_y y
vim_y yy
vim_z z-
vim_z z.
vim_z z&lt;CR&gt;
vim_z zb
vim_z zh
vim_z zl
vim_z zt
vim_z zz
</t>
<t tx="ekr.20131111155830.4253"># CR
# Ctrl-End
# Ctrl-Home
# Ctrl-Left
# Ctrl-M
# Ctrl-N
# Ctrl-P
# Ctrl-Right
# End
# Home
# Shift-Left
# Shift-Right

( 	
)
{
}
[[
[]
][
]]
$
^	
+
,
-
;
_
0
B
E
F
G
T
W
b
e
f
g$
g^
g0
gE
# gEnd
# gHome	
ge
gg
h
j
k
t
w
</t>
<t tx="ekr.20131111155830.4254">char F
char T
char f
char t
</t>
<t tx="ekr.20140217055617.4231"># For a unit test.</t>
<t tx="ekr.20140716121225.4354">print(p.v.gnx)</t>
<t tx="ekr.20140902101931.4478"></t>
<t tx="ekr.20150216110251.11"># Do not delete this node.
# It is used by unit tests.</t>
<t tx="ekr.20150321155210.11"></t>
<t tx="ekr.20150430053825.1">ac = c.abbrevCommands
assert ac
if c.abbrev_place_start is None or c.abbrev_place_end is None:
    self.skipTest('no abbreviation settings') # #1345.
child = g.findNodeInTree(c,p,'child')
assert child
old_b = child.b
try:
    i,j,val = 0,0,child.b
    # ac.make_script_substitutions(i,j,val)
    # ac.find_place_holder(child,True)
    new_s,i,j = ac.next_place(child.b,offset=0)
    assert i == 34 and j == 40,(i,j)
    new_s2,i,j = ac.next_place(new_s,offset=40)
    assert i == 54 and j == 58,(i,j)
finally:
    child.b = old_b
</t>
<t tx="ekr.20150430061225.1">def spam ():
    """None - Return &lt;|return|&gt;
    """

    &lt;|code|&gt;
</t>
<t tx="ekr.20150521123343.1"></t>
<t tx="ekr.20150602215639.1">True: Automatically beautify all @&lt;file&gt; nodes when saving an outline.

# This *must* be False in unitTest.leo!</t>
<t tx="ekr.20150610064911.1"># This tests only recent bugs.
p1 = p.firstChild()
s = p1.b
p2 = p1.next()
try:
    c.selectPosition(p1)
    c.k.simulateCommand('beautify-tree')
    assert p1.b == p2.b, (
        f"{g.objToString(p1.b, tag='BEFORE')}\n"
        f"{g.objToString(p2.b, tag='AFTER')}")
finally:
    p1.b = s
    c.setChanged(False)
</t>
<t tx="ekr.20150610064930.1">def spam():
    if - 1 &lt; 2:
        pass
</t>
<t tx="ekr.20150610065241.1">def spam():
    if -1 &lt; 2:
        pass
</t>
<t tx="ekr.20160318094003.1"></t>
<t tx="ekr.20160318094009.1">import leo.core.leoAst as leoAst
import leo.external.make_stub_files as msf
import leo.external.py2cs as py2cs
import ast, tokenize
path = g.os_path_finalize_join(g.app.loadDir,
    '..', 'test', 'unittest', 'python3_test.py')
assert g.os_path_exists(path), path
fn = g.shortFileName(path)
source = open(path, 'r').read()
node = ast.parse(source, filename=fn, mode='exec')
src = open(path, 'r').read()
readlines = g.ReadLinesClass(src).next
tokens = list(tokenize.generate_tokens(readlines))
leoAst.AstFullTraverser().visit(node)
s = leoAst.HTMLReportTraverser(debug=True).main(fn, node)
assert s
s = leoAst.AstFormatter().format(node,level=0)
assert s
s = py2cs.CoffeeScriptTraverser(controller=g.NullObject()).format(node, src, tokens)
assert s
controller = msf.StandAloneMakeStubFile()
msf.StubTraverser(controller).visit(node)
</t>
<t tx="ekr.20160523094102.1"># Great ast docs: http://greentreesnakes.readthedocs.io/en/latest/nodes.html

import leo.core.leoAst as leoAst
leoAst.unit_test(raise_on_fail=True)

import leo.external.make_stub_files as msf
msf.unit_test(raise_on_fail=True)

import leo.external.py2cs as py2cs
py2cs.unit_test(raise_on_fail=True)

</t>
<t tx="ekr.20161011095551.1">True: (Experimental): The @auto write code expands section references.
False: (Legacy):      The @auto write code ignores section references.
</t>
<t tx="ekr.20161123080832.1">'''
Create a table of expected headlines in a unit test.

Usage: select the desired subnode of an @test node.
'''
g.cls()
# Proper escapes are tricky.
if p.parent() and p.parent().h.startswith('@test'):
    table = [
        '(%s, "%s"),' % (
            p.level()-c.p.level(),
            p.h.replace('\\', '\\\\').replace('"', '\\"'),
        )
            for p in p.subtree()
    ]
    print("table = (\n    %s\n)" % '\n    '.join(table))
else:
    print('select a child of an @test node node')</t>
<t tx="ekr.20161129030232.1"></t>
<t tx="ekr.20170415084531.1"># lowercase xml tags, one per line.

html
body
head
div
table
nodeA
nodeB
</t>
<t tx="ekr.20171126152936.1"># Required for unit tests: See #577.</t>
<t tx="ekr.20171126153044.1"># Required for unit tests: See #577.</t>
<t tx="ekr.20171126153138.1">See #577: https://github.com/leo-editor/leo-editor/issues/577
</t>
<t tx="ekr.20180214042153.1">False is the legacy value.</t>
<t tx="ekr.20181031152556.1"></t>
<t tx="ekr.20190113120618.1"># At least one unit test uses each of these files.</t>
<t tx="ekr.20190113120734.1">@language python
@others
</t>
<t tx="ekr.20190113120734.2">def spam():
    pass</t>
<t tx="ekr.20190113120734.3">def eggs():
    pass</t>
<t tx="ekr.20190113122443.1"># Must be False for @auto unit tests.</t>
<t tx="ekr.20190113123822.1">@language c
@tabwidth -4
// before @others // line 1
@others
// last line: line 6
</t>
<t tx="ekr.20190113123822.2">def spam(): // line 2
    pass
</t>
<t tx="ekr.20190113123822.3">def eggs(): // line 4
    pass
</t>
<t tx="ekr.20190113123853.1">@language python
@tabwidth -4
# Before @others: line 1
@others
# Last line: line 6
</t>
<t tx="ekr.20190113123853.2">def spam(): # line 2
    pass
</t>
<t tx="ekr.20190113123853.3">def eggs(): # line 4
    pass
</t>
<t tx="ekr.20190113124135.1">@language python
@tabwidth -4
# before @others: line 1
@others
# last line: line 6</t>
<t tx="ekr.20190113124135.2">def spam(): # line 2
    pass
</t>
<t tx="ekr.20190113124135.3">def eggs(): # line 4
    pass</t>
<t tx="ekr.20190912065312.1">from leo.core.leoBeautify import  SyntaxSanitizer
for child in p.subtree():
    child_s = child.b
    for keep in (True, False):
        # Setup.
        sanitizer = SyntaxSanitizer(c, keep)
        comment, result1 = sanitizer.comment_leo_lines(child)
        # Test basic round-tripping.
        result2 = sanitizer.uncomment_leo_lines(comment, child, s0=result1)
        assert child_s.rstrip() == result2.rstrip(), (
            f"Round-trip FAIL: keep: {keep}, {child.h}\n"
            f"{g.objToString(child_s, tag='child_s')}\n"
            f"{g.objToString(result1, tag='result1')}\n"
            f"{g.objToString(result2, tag='result2')}")
</t>
<t tx="ekr.20190912071608.1">@language python

def scan_lines(self, delims, first_lines, lines, path, start, test=False):
    '''Scan all lines of the file, creating vnodes.'''
    &lt;&lt; init scan_lines &gt;&gt;
    &lt;&lt; define dump_v &gt;&gt;
    i = 0 # To keep pylint happy.
    for i, line in enumerate(lines[start:]):
        # Order matters.
        &lt;&lt; 1. common code for all lines &gt;&gt;
        &lt;&lt; 2. short-circuit later tests &gt;&gt;
        &lt;&lt; 3. handle @others &gt;&gt; # clears in_doc
        &lt;&lt; 4. handle section refs &gt;&gt; # clears in_doc.
        # Order doesn't matter, but match more common sentinels first.
        &lt;&lt; handle node_start &gt;&gt;
        &lt;&lt; handle end of @doc &amp; @code parts &gt;&gt;
        &lt;&lt; handle @all &gt;&gt;
        &lt;&lt; handle afterref &gt;&gt;
        &lt;&lt; handle @first and @last &gt;&gt;
        &lt;&lt; handle @comment &gt;&gt;
        &lt;&lt; handle @delims &gt;&gt;
        &lt;&lt; handle @raw &gt;&gt;
        &lt;&lt; handle @-leo &gt;&gt;
        # These must be last, in this order.
        &lt;&lt; Last 1. handle remaining @@ lines &gt;&gt;
        &lt;&lt; Last 2. handle remaining @doc lines &gt;&gt;
        &lt;&lt; Last 3. handle remaining @ lines &gt;&gt;
    else:
        # No @-leo sentinel
        return None, []
    # Handle @last lines.
    last_lines = lines[start+i:]
    if last_lines:
        last_lines = ['@last ' + z for z in last_lines]
        gnx2body[root_gnx] = gnx2body[root_gnx] + last_lines
    self.post_pass(gnx2body, gnx2vnode, root_v)
    return root_v, last_lines
</t>
<t tx="ekr.20190912071608.10">m = all_pat.match(line)
if m:
    # @all tells Leo's *write* code not to check for undefined sections.
    # Here, in the read code, we merely need to add it to the body.
    # Pushing and popping the stack may not be necessary, but it can't hurt.
    if m.group(2) == '+': # opening sentinel
        body.append('%s@all%s\n' % (m.group(1), m.group(3) or ''))
        stack.append((gnx, indent, body))
    else: # closing sentinel.
        # m.group(2) is '-' because the pattern matched.
        gnx, indent, body = stack.pop()
        gnx2body[gnx] = body
    continue
</t>
<t tx="ekr.20190912071608.11">m = after_pat.match(line)
if m:
    afterref = True
    verbatim = True
        # Avoid an extra test in the main loop.
    continue
</t>
<t tx="ekr.20190912071608.12">m = first_pat.match(line)
if m:
    if 0 &lt;= first_i &lt; len(first_lines):
        body.append('@first ' + first_lines[first_i])
        first_i += 1
    else:
        g.trace('\ntoo many @first lines: %s' %  path)
        print('@first is valid only at the start of @&lt;file&gt; nodes\n')
        g.printObj(first_lines, tag='first_lines')
        g.printObj(lines[start:i+2], tag='lines[start:i+2]')
    continue
m = last_pat.match(line)
if m:
    n_last_lines += 1
    continue
</t>
<t tx="ekr.20190912071608.13"># http://leoeditor.com/directives.html#part-4-dangerous-directives
m = comment_pat.match(line)
if m:
    # &lt;1, 2 or 3 comment delims&gt;
    delims = m.group(1).strip()
    # Whatever happens, retain the @delims line.
    body.append('@comment %s\n' % delims)
    delim1, delim2, delim3 = g.set_delims_from_string(delims)
        # delim1 is always the single-line delimiter.
    if delim1:
        delim_start, delim_end = delim1, ''
    else:
        delim_start, delim_end = delim2, delim3
    #
    # Within these delimiters:
    # - double underscores represent a newline.
    # - underscores represent a significant space,
    delim_start = delim_start.replace('__','\n').replace('_',' ')
    delim_end = delim_end.replace('__','\n').replace('_',' ')
    # Recalculate all delim-related values
    doc_skip = (delim_start + '\n', delim_end + '\n')
    is_cweb = delim_start == '@q@' and delim_end == '@&gt;'
    sentinel = delim_start + '@'
    #
    # Recalculate the patterns.
    delims = delim_start, delim_end
    (
        after_pat, all_pat, code_pat, comment_pat, delims_pat,
        doc_pat, end_raw_pat, first_pat, last_pat,
        node_start_pat, others_pat, raw_pat, ref_pat
    ) = self.get_patterns(delims)
    continue
</t>
<t tx="ekr.20190912071608.14">m = delims_pat.match(line)
if m:
    # Get 1 or 2 comment delims
    # Whatever happens, retain the original @delims line.
    delims = m.group(1).strip()
    body.append('@delims %s\n' % delims)
    #
    # Parse the delims.
    delims_pat = re.compile(r'^([^ ]+)\s*([^ ]+)?')
    m2 = delims_pat.match(delims)
    if not m2:
        g.trace('Ignoring invalid @comment: %r' % line)
        continue
    delim_start = m2.group(1)
    delim_end = m2.group(2) or ''
    #
    # Within these delimiters:
    # - double underscores represent a newline.
    # - underscores represent a significant space,
    delim_start = delim_start.replace('__','\n').replace('_',' ')
    delim_end = delim_end.replace('__','\n').replace('_',' ')
    # Recalculate all delim-related values
    doc_skip = (delim_start + '\n', delim_end + '\n')
    is_cweb = delim_start == '@q@' and delim_end == '@&gt;'
    sentinel = delim_start + '@'
    #
    # Recalculate the patterns
    delims = delim_start, delim_end
    (
        after_pat, all_pat, code_pat, comment_pat, delims_pat,
        doc_pat, end_raw_pat, first_pat, last_pat,
        node_start_pat, others_pat, raw_pat, ref_pat
    ) = self.get_patterns(delims)
    continue
</t>
<t tx="ekr.20190912071608.15"># http://leoeditor.com/directives.html#part-4-dangerous-directives
m = raw_pat.match(line)
if m:
    in_raw = True
    verbatim = True
        # Avoid an extra test in the main loop.
    continue
</t>
<t tx="ekr.20190912071608.16">if line.startswith(delim_start + '@-leo'):
    i += 1
    break
</t>
<t tx="ekr.20190912071608.17"># @first, @last, @delims and @comment generate @@ sentinels,
# So this must follow all of those.
if line.startswith(delim_start + '@@'):
    ii = len(delim_start) + 1 # on second '@'
    jj = line.rfind(delim_end) if delim_end else -1
    body.append(line[ii:jj] + '\n')
    continue
</t>
<t tx="ekr.20190912071608.18">if in_doc:
    if delim_end:
        # doc lines are unchanged.
        body.append(line)
    else:
        # Doc lines start with start_delim + one blank.
        body.append(line[len(delim_start)+1:])
    continue
</t>
<t tx="ekr.20190912071608.19"># Handle an apparent sentinel line.
# This *can* happen, as the result of the git-diff command.
#
# This assert verifies the short-circuit test.
assert strip_line.startswith(sentinel), (repr(sentinel), repr(line))
#
# This trace is less important, but interesting.
g.trace(f"{g.shortFileName(self.path)}: unexpected line: {line.strip()!r}")
body.append(line)
</t>
<t tx="ekr.20190912071608.2">#
# Simple vars...
afterref = False
    # A special verbatim line follows @afterref.
clone_v = None
    # The root of the clone tree.
    # When not None, we are scanning a clone and all it's descendants.
delim_start, delim_end = delims
    # The start/end delims.
doc_skip = (delim_start + '\n', delim_end + '\n')
    # To handle doc parts.
first_i = 0
    # Index into first array.
in_doc = False
    # True: in @doc parts.
in_raw = False
    # True: @raw seen.
is_cweb = delim_start == '@q@' and delim_end == '@&gt;'
    # True: cweb hack in effect.
indent = 0 
    # The current indentation.
level_stack = []
    # Entries are (vnode, in_clone_tree)
n_last_lines = 0
    # The number of @@last directives seen.
root_seen = False
    # False: The next +@node sentinel denotes the root, regardless of gnx.
    # Needed to handle #1065 so reads will not create spurious child nodes.
sentinel = delim_start + '@'
    # Faster than a regex!
stack = []
    # Entries are (gnx, indent, body)
    # Updated when at+others, at+&lt;section&gt;, or at+all is seen.
verbline = delim_start + '@verbatim' + delim_end + '\n'
    # The spelling of at-verbatim sentinel
verbatim = False
    # True: the next line must be added without change.
#
# Init the data for the root node.
#

#
# Init the parent vnode for testing.
#
if self.test:
    root_gnx = gnx = 'root-gnx'
        # The node that we are reading.
        # start with the gnx for the @file node.
    gnx_head =  '&lt;hidden top vnode&gt;'
        # The headline of the root node.
    context = None
    parent_v = self.VNode(context=context, gnx=gnx)
    parent_v._headString = gnx_head
        # Corresponds to the @files node itself.
else:
    # Production.
    root_gnx = gnx = self.root.gnx
    context = self.c
    parent_v = self.root.v
root_v = parent_v
    # Does not change.
level_stack.append((root_v, False),)
#
# Init the gnx dict last.
#
gnx2vnode = self.gnx2vnode
    # Keys are gnx's, values are vnodes.
gnx2body = {}
    # Keys are gnxs, values are list of body lines.
gnx2vnode[gnx] = parent_v
    # Add gnx to the keys
gnx2body[gnx] = body = first_lines
    # Add gnx to the keys.
    # Body is the list of lines presently being accumulated.
#
# get the patterns.
after_pat, all_pat, code_pat, comment_pat, delims_pat,\
doc_pat, end_raw_pat, first_pat, last_pat, \
node_start_pat, others_pat, raw_pat, ref_pat = self.get_patterns(delims)
</t>
<t tx="ekr.20190912071608.3">def dump_v():
    '''Dump the level stack and v.'''
    print('----- LEVEL', level, v.h)
    print('       PARENT', parent_v.h)
    print('[')
    for i, data in enumerate(level_stack):
        v2, in_tree = data
        print('%2s %5s %s' % (i+1, in_tree, v2.h))
    print(']')
    print('PARENT.CHILDREN...')
    g.printObj([v3.h for v3 in parent_v.children])
    print('PARENTS...')
    g.printObj([v4.h for v4 in v.parents])
</t>
<t tx="ekr.20190912071608.4">if verbatim:
    # We are in raw mode, or other special situation.
    # Previous line was verbatim sentinel. Append this line as it is.
    if afterref:
        afterref = False
        if body: # a List of lines.
            body[-1] = body[-1].rstrip() + line
        else:
            body = [line]
        verbatim = False
    elif in_raw:
        m = end_raw_pat.match(line)
        if m:
            in_raw = False
            verbatim = False
        else:
             body.append(line)
             # Continue verbatim/raw mode.
    else:
        body.append(line)
        verbatim = False
    continue
if line == verbline: # &lt;delim&gt;@verbatim.
    verbatim = True
    continue
#
# Strip the line only once.
strip_line = line.strip()
#
# Undo the cweb hack.
if is_cweb and line.startswith(sentinel):
    line = line[:len(sentinel)] + line[len(sentinel):].replace('@@', '@')
# Adjust indentation.
if indent and line[:indent].isspace() and len(line) &gt; indent:
    line = line[indent:]
</t>
<t tx="ekr.20190912071608.5"># This is valid because all following sections are either:
# 1. guarded by 'if in_doc' or
# 2. guarded by a pattern that matches the start of the sentinel.   
#
if not in_doc and not strip_line.startswith(sentinel):
    # lstrip() is faster than using a regex!
    body.append(line)
    continue
</t>
<t tx="ekr.20190912071608.6">m = others_pat.match(line)
if m:
    in_doc = False
    if m.group(2) == '+': # opening sentinel
        body.append('%s@others%s\n' % (m.group(1), m.group(3) or ''))
        stack.append((gnx, indent, body))
        indent += m.end(1) # adjust current identation
    else: # closing sentinel.
        # m.group(2) is '-' because the pattern matched.
        gnx, indent, body = stack.pop()
    continue
</t>
<t tx="ekr.20190912071608.7">m = ref_pat.match(line)
if m:
    in_doc = False
    if m.group(2) == '+':
        # open sentinel.
        body.append(m.group(1) + g.angleBrackets(m.group(3)) + '\n')
        stack.append((gnx, indent, body))
        indent += m.end(1)
    else:
        # close sentinel.
        # m.group(2) is '-' because the pattern matched.
        gnx, indent, body = stack.pop()
    continue
</t>
<t tx="ekr.20190912071608.8">m = node_start_pat.match(line)
if m:
    in_doc, in_raw = False, False
    gnx, head = m.group(2), m.group(5)
    level = int(m.group(3)) if m.group(3) else 1 + len(m.group(4))
        # m.group(3) is the level number, m.group(4) is the number of stars.
    v = gnx2vnode.get(gnx)
    #
    # Case 1: The root @file node. Don't change the headline.
    if not root_seen:
        # Fix #1064: The node represents the root, regardless of the gnx!
        root_seen = True
        clone_v = None
        gnx2body[gnx] = body = []
        if not v:
            # Fix #1064.
            v = root_v
            # This message is annoying when using git-diff.
                # if gnx != root_gnx:
                    # g.es_print("using gnx from external file: %s" % (v.h), color='blue')
            gnx2vnode [gnx] = v
            v.fileIndex = gnx
        v.children = []
        continue
    #
    # Case 2: We are scanning the descendants of a clone.
    parent_v, clone_v = level_stack[level-2]
    if v and clone_v:
        # The last version of the body and headline wins..
        gnx2body[gnx] = body = []
        v._headString = head
        # Update the level_stack.
        level_stack = level_stack[:level-1]
        level_stack.append((v, clone_v),)
        # Always clear the children!
        v.children=[]
        parent_v.children.append(v)
        continue
    #
    # Case 3: we are not already scanning the descendants of a clone.
    if v:
        # The *start* of a clone tree. Reset the children.
        clone_v = v
        v.children = []
    else:
        # Make a new vnode.
        v = self.VNode(context=context, gnx=gnx)
    #
    # The last version of the body and headline wins.
    gnx2vnode[gnx] = v
    gnx2body[gnx] = body = []
    v._headString = head
    #
    # Update the stack.
    level_stack = level_stack[:level-1]
    level_stack.append((v, clone_v),)
    #
    # Update the links.
    assert v != root_v
    parent_v.children.append(v)
    v.parents.append(parent_v)
    # dump_v()
    continue
</t>
<t tx="ekr.20190912071608.9">if in_doc:
    # When delim_end exists the doc block:
    # - begins with the opening delim, alonw on its own line
    # - ends with the closing delim, alone on its own line.
    # Both of these lines should be skipped
    if line in doc_skip:
        # doc_skip is (delim_start + '\n', delim_end + '\n')
        continue
    #
    # Check for @c or @code.
    m = code_pat.match(line)
    if m:
        in_doc = False 
        body.append('@code\n' if m.group(1) else '@c\n')
        continue
else:
    m = doc_pat.match(line)
    if m:
        # @+at or @+doc?
        doc = '@doc' if m.group(1) == 'doc' else '@'
        doc2 = m.group(2) or '' # Trailing text.
        if doc2:
            body.append('%s%s\n'%(doc, doc2))
        else:
            body.append(doc + '\n')
        # Enter @doc mode.
        in_doc = True
        continue
</t>
<t tx="ekr.20190912105517.1">@language python
@
This is a doc part.
@c

def spam():
    if 1:
        # Regular comment.
        print('-----')
            # Indented comment.
    else:
        pass
</t>
<t tx="ekr.20190912160752.1">a = 1
&lt;&lt; section ref &gt;&gt;
b = 2

if 1:
    @others

# Note: section references will fail in if statements, because pass is not valid there:
    
    # if (
        # &lt;section ref&gt;
    # ):
        # pass
</t>
<t tx="ekr.20190914104955.1">from leo.core.leoBeautify import BlackCommand, should_beautify
try:
    import black
except ImportError:
    self.skipTest('can not import black')
try:
    mode = black.FileMode()
except TypeError:
    self.skipTest('old version of black')

for child in p.subtree():
    child_s = child.b
    try:
        # Blacken all nodes.
        assert should_beautify(child), repr(child.h)
        c.selectPosition(child)
        BlackCommand(c).blacken_node(child, diff_flag=False, check_flag=False)
        # Check only those nodes that should not have been changed.
        if child.h in ('basic test', 'sections references'):
            assert child_s.rstrip() == child.b.rstrip(), (
                f"FAIL: blacken-node: {child.h}\n"
                f"{g.objToString(child_s, tag='OLD')}\n"
                f"{g.objToString(child.b, tag='NEW')}")
    finally:
        child.b = child_s
        c.selectPosition(p)
</t>
<t tx="ekr.20190914104955.10">m = ref_pat.match(line)
if m:
    in_doc = False
    if m.group(2) == '+':
        # open sentinel.
        body.append(m.group(1) + g.angleBrackets(m.group(3)) + '\n')
        stack.append((gnx, indent, body))
        indent += m.end(1)
    else:
        # close sentinel.
        # m.group(2) is '-' because the pattern matched.
        gnx, indent, body = stack.pop()
    continue
</t>
<t tx="ekr.20190914104955.11">m = node_start_pat.match(line)
if m:
    in_doc, in_raw = False, False
    gnx, head = m.group(2), m.group(5)
    level = int(m.group(3)) if m.group(3) else 1 + len(m.group(4))
        # m.group(3) is the level number, m.group(4) is the number of stars.
    v = gnx2vnode.get(gnx)
    #
    # Case 1: The root @file node. Don't change the headline.
    if not root_seen:
        # Fix #1064: The node represents the root, regardless of the gnx!
        root_seen = True
        clone_v = None
        gnx2body[gnx] = body = []
        if not v:
            # Fix #1064.
            v = root_v
            # This message is annoying when using git-diff.
                # if gnx != root_gnx:
                    # g.es_print("using gnx from external file: %s" % (v.h), color='blue')
            gnx2vnode [gnx] = v
            v.fileIndex = gnx
        v.children = []
        continue
    #
    # Case 2: We are scanning the descendants of a clone.
    parent_v, clone_v = level_stack[level-2]
    if v and clone_v:
        # The last version of the body and headline wins..
        gnx2body[gnx] = body = []
        v._headString = head
        # Update the level_stack.
        level_stack = level_stack[:level-1]
        level_stack.append((v, clone_v),)
        # Always clear the children!
        v.children=[]
        parent_v.children.append(v)
        continue
    #
    # Case 3: we are not already scanning the descendants of a clone.
    if v:
        # The *start* of a clone tree. Reset the children.
        clone_v = v
        v.children = []
    else:
        # Make a new vnode.
        v = self.VNode(context=context, gnx=gnx)
    #
    # The last version of the body and headline wins.
    gnx2vnode[gnx] = v
    gnx2body[gnx] = body = []
    v._headString = head
    #
    # Update the stack.
    level_stack = level_stack[:level-1]
    level_stack.append((v, clone_v),)
    #
    # Update the links.
    assert v != root_v
    parent_v.children.append(v)
    v.parents.append(parent_v)
    # dump_v()
    continue
</t>
<t tx="ekr.20190914104955.12">if in_doc:
    # When delim_end exists the doc block:
    # - begins with the opening delim, alonw on its own line
    # - ends with the closing delim, alone on its own line.
    # Both of these lines should be skipped
    if line in doc_skip:
        # doc_skip is (delim_start + '\n', delim_end + '\n')
        continue
    #
    # Check for @c or @code.
    m = code_pat.match(line)
    if m:
        in_doc = False 
        body.append('@code\n' if m.group(1) else '@c\n')
        continue
else:
    m = doc_pat.match(line)
    if m:
        # @+at or @+doc?
        doc = '@doc' if m.group(1) == 'doc' else '@'
        doc2 = m.group(2) or '' # Trailing text.
        if doc2:
            body.append('%s%s\n'%(doc, doc2))
        else:
            body.append(doc + '\n')
        # Enter @doc mode.
        in_doc = True
        continue
</t>
<t tx="ekr.20190914104955.13">m = all_pat.match(line)
if m:
    # @all tells Leo's *write* code not to check for undefined sections.
    # Here, in the read code, we merely need to add it to the body.
    # Pushing and popping the stack may not be necessary, but it can't hurt.
    if m.group(2) == '+': # opening sentinel
        body.append('%s@all%s\n' % (m.group(1), m.group(3) or ''))
        stack.append((gnx, indent, body))
    else: # closing sentinel.
        # m.group(2) is '-' because the pattern matched.
        gnx, indent, body = stack.pop()
        gnx2body[gnx] = body
    continue
</t>
<t tx="ekr.20190914104955.14">m = after_pat.match(line)
if m:
    afterref = True
    verbatim = True
        # Avoid an extra test in the main loop.
    continue
</t>
<t tx="ekr.20190914104955.15">m = first_pat.match(line)
if m:
    if 0 &lt;= first_i &lt; len(first_lines):
        body.append('@first ' + first_lines[first_i])
        first_i += 1
    else:
        g.trace('\ntoo many @first lines: %s' %  path)
        print('@first is valid only at the start of @&lt;file&gt; nodes\n')
        g.printObj(first_lines, tag='first_lines')
        g.printObj(lines[start:i+2], tag='lines[start:i+2]')
    continue
m = last_pat.match(line)
if m:
    n_last_lines += 1
    continue
</t>
<t tx="ekr.20190914104955.16"># http://leoeditor.com/directives.html#part-4-dangerous-directives
m = comment_pat.match(line)
if m:
    # &lt;1, 2 or 3 comment delims&gt;
    delims = m.group(1).strip()
    # Whatever happens, retain the @delims line.
    body.append('@comment %s\n' % delims)
    delim1, delim2, delim3 = g.set_delims_from_string(delims)
        # delim1 is always the single-line delimiter.
    if delim1:
        delim_start, delim_end = delim1, ''
    else:
        delim_start, delim_end = delim2, delim3
    #
    # Within these delimiters:
    # - double underscores represent a newline.
    # - underscores represent a significant space,
    delim_start = delim_start.replace('__','\n').replace('_',' ')
    delim_end = delim_end.replace('__','\n').replace('_',' ')
    # Recalculate all delim-related values
    doc_skip = (delim_start + '\n', delim_end + '\n')
    is_cweb = delim_start == '@q@' and delim_end == '@&gt;'
    sentinel = delim_start + '@'
    #
    # Recalculate the patterns.
    delims = delim_start, delim_end
    (
        after_pat, all_pat, code_pat, comment_pat, delims_pat,
        doc_pat, end_raw_pat, first_pat, last_pat,
        node_start_pat, others_pat, raw_pat, ref_pat
    ) = self.get_patterns(delims)
    continue
</t>
<t tx="ekr.20190914104955.17">m = delims_pat.match(line)
if m:
    # Get 1 or 2 comment delims
    # Whatever happens, retain the original @delims line.
    delims = m.group(1).strip()
    body.append('@delims %s\n' % delims)
    #
    # Parse the delims.
    delims_pat = re.compile(r'^([^ ]+)\s*([^ ]+)?')
    m2 = delims_pat.match(delims)
    if not m2:
        g.trace('Ignoring invalid @comment: %r' % line)
        continue
    delim_start = m2.group(1)
    delim_end = m2.group(2) or ''
    #
    # Within these delimiters:
    # - double underscores represent a newline.
    # - underscores represent a significant space,
    delim_start = delim_start.replace('__','\n').replace('_',' ')
    delim_end = delim_end.replace('__','\n').replace('_',' ')
    # Recalculate all delim-related values
    doc_skip = (delim_start + '\n', delim_end + '\n')
    is_cweb = delim_start == '@q@' and delim_end == '@&gt;'
    sentinel = delim_start + '@'
    #
    # Recalculate the patterns
    delims = delim_start, delim_end
    (
        after_pat, all_pat, code_pat, comment_pat, delims_pat,
        doc_pat, end_raw_pat, first_pat, last_pat,
        node_start_pat, others_pat, raw_pat, ref_pat
    ) = self.get_patterns(delims)
    continue
</t>
<t tx="ekr.20190914104955.18"># http://leoeditor.com/directives.html#part-4-dangerous-directives
m = raw_pat.match(line)
if m:
    in_raw = True
    verbatim = True
        # Avoid an extra test in the main loop.
    continue
</t>
<t tx="ekr.20190914104955.19">if line.startswith(delim_start + '@-leo'):
    i += 1
    break
</t>
<t tx="ekr.20190914104955.2">@language python
@
This is a doc part.
@c


def spam():
    if 1:
        # Regular comment.
        print('-----')
            # Indented comment.
    else:
        pass
</t>
<t tx="ekr.20190914104955.20"># @first, @last, @delims and @comment generate @@ sentinels,
# So this must follow all of those.
if line.startswith(delim_start + '@@'):
    ii = len(delim_start) + 1 # on second '@'
    jj = line.rfind(delim_end) if delim_end else -1
    body.append(line[ii:jj] + '\n')
    continue
</t>
<t tx="ekr.20190914104955.21">if in_doc:
    if delim_end:
        # doc lines are unchanged.
        body.append(line)
    else:
        # Doc lines start with start_delim + one blank.
        body.append(line[len(delim_start)+1:])
    continue
</t>
<t tx="ekr.20190914104955.22"># Handle an apparent sentinel line.
# This *can* happen, as the result of the git-diff command.
#
# This assert verifies the short-circuit test.
assert strip_line.startswith(sentinel), (repr(sentinel), repr(line))
#
# This trace is less important, but interesting.
g.trace(f"{g.shortFileName(self.path)}: unexpected line: {line.strip()!r}")
body.append(line)
</t>
<t tx="ekr.20190914104955.3">a = 1
&lt;&lt; section ref &gt;&gt;
b = 2

if 1:
    @others

# Note: section references will fail in if statements, because pass is not valid there:
    
    # if (
        # &lt;section ref&gt;
    # ):
        # pass
</t>
<t tx="ekr.20190914104955.4">@language python

def scan_lines(self, delims, first_lines, lines, path, start, test=False):
    '''Scan all lines of the file, creating vnodes.'''
    &lt;&lt; init scan_lines &gt;&gt;
    &lt;&lt; define dump_v &gt;&gt;
    i = 0 # To keep pylint happy.
    for i, line in enumerate(lines[start:]):
        # Order matters.
        &lt;&lt; 1. common code for all lines &gt;&gt;
        &lt;&lt; 2. short-circuit later tests &gt;&gt;
        &lt;&lt; 3. handle @others &gt;&gt; # clears in_doc
        &lt;&lt; 4. handle section refs &gt;&gt; # clears in_doc.
        # Order doesn't matter, but match more common sentinels first.
        &lt;&lt; handle node_start &gt;&gt;
        &lt;&lt; handle end of @doc &amp; @code parts &gt;&gt;
        &lt;&lt; handle @all &gt;&gt;
        &lt;&lt; handle afterref &gt;&gt;
        &lt;&lt; handle @first and @last &gt;&gt;
        &lt;&lt; handle @comment &gt;&gt;
        &lt;&lt; handle @delims &gt;&gt;
        &lt;&lt; handle @raw &gt;&gt;
        &lt;&lt; handle @-leo &gt;&gt;
        # These must be last, in this order.
        &lt;&lt; Last 1. handle remaining @@ lines &gt;&gt;
        &lt;&lt; Last 2. handle remaining @doc lines &gt;&gt;
        &lt;&lt; Last 3. handle remaining @ lines &gt;&gt;
    else:
        # No @-leo sentinel
        return None, []
    # Handle @last lines.
    last_lines = lines[start+i:]
    if last_lines:
        last_lines = ['@last ' + z for z in last_lines]
        gnx2body[root_gnx] = gnx2body[root_gnx] + last_lines
    self.post_pass(gnx2body, gnx2vnode, root_v)
    return root_v, last_lines
</t>
<t tx="ekr.20190914104955.5">#
# Simple vars...
afterref = False
    # A special verbatim line follows @afterref.
clone_v = None
    # The root of the clone tree.
    # When not None, we are scanning a clone and all it's descendants.
delim_start, delim_end = delims
    # The start/end delims.
doc_skip = (delim_start + '\n', delim_end + '\n')
    # To handle doc parts.
first_i = 0
    # Index into first array.
in_doc = False
    # True: in @doc parts.
in_raw = False
    # True: @raw seen.
is_cweb = delim_start == '@q@' and delim_end == '@&gt;'
    # True: cweb hack in effect.
indent = 0 
    # The current indentation.
level_stack = []
    # Entries are (vnode, in_clone_tree)
n_last_lines = 0
    # The number of @@last directives seen.
root_seen = False
    # False: The next +@node sentinel denotes the root, regardless of gnx.
    # Needed to handle #1065 so reads will not create spurious child nodes.
sentinel = delim_start + '@'
    # Faster than a regex!
stack = []
    # Entries are (gnx, indent, body)
    # Updated when at+others, at+&lt;section&gt;, or at+all is seen.
verbline = delim_start + '@verbatim' + delim_end + '\n'
    # The spelling of at-verbatim sentinel
verbatim = False
    # True: the next line must be added without change.
#
# Init the data for the root node.
#

#
# Init the parent vnode for testing.
#
if self.test:
    root_gnx = gnx = 'root-gnx'
        # The node that we are reading.
        # start with the gnx for the @file node.
    gnx_head =  '&lt;hidden top vnode&gt;'
        # The headline of the root node.
    context = None
    parent_v = self.VNode(context=context, gnx=gnx)
    parent_v._headString = gnx_head
        # Corresponds to the @files node itself.
else:
    # Production.
    root_gnx = gnx = self.root.gnx
    context = self.c
    parent_v = self.root.v
root_v = parent_v
    # Does not change.
level_stack.append((root_v, False),)
#
# Init the gnx dict last.
#
gnx2vnode = self.gnx2vnode
    # Keys are gnx's, values are vnodes.
gnx2body = {}
    # Keys are gnxs, values are list of body lines.
gnx2vnode[gnx] = parent_v
    # Add gnx to the keys
gnx2body[gnx] = body = first_lines
    # Add gnx to the keys.
    # Body is the list of lines presently being accumulated.
#
# get the patterns.
after_pat, all_pat, code_pat, comment_pat, delims_pat,\
doc_pat, end_raw_pat, first_pat, last_pat, \
node_start_pat, others_pat, raw_pat, ref_pat = self.get_patterns(delims)
</t>
<t tx="ekr.20190914104955.6">def dump_v():
    '''Dump the level stack and v.'''
    print('----- LEVEL', level, v.h)
    print('       PARENT', parent_v.h)
    print('[')
    for i, data in enumerate(level_stack):
        v2, in_tree = data
        print('%2s %5s %s' % (i+1, in_tree, v2.h))
    print(']')
    print('PARENT.CHILDREN...')
    g.printObj([v3.h for v3 in parent_v.children])
    print('PARENTS...')
    g.printObj([v4.h for v4 in v.parents])
</t>
<t tx="ekr.20190914104955.7">if verbatim:
    # We are in raw mode, or other special situation.
    # Previous line was verbatim sentinel. Append this line as it is.
    if afterref:
        afterref = False
        if body: # a List of lines.
            body[-1] = body[-1].rstrip() + line
        else:
            body = [line]
        verbatim = False
    elif in_raw:
        m = end_raw_pat.match(line)
        if m:
            in_raw = False
            verbatim = False
        else:
             body.append(line)
             # Continue verbatim/raw mode.
    else:
        body.append(line)
        verbatim = False
    continue
if line == verbline: # &lt;delim&gt;@verbatim.
    verbatim = True
    continue
#
# Strip the line only once.
strip_line = line.strip()
#
# Undo the cweb hack.
if is_cweb and line.startswith(sentinel):
    line = line[:len(sentinel)] + line[len(sentinel):].replace('@@', '@')
# Adjust indentation.
if indent and line[:indent].isspace() and len(line) &gt; indent:
    line = line[indent:]
</t>
<t tx="ekr.20190914104955.8"># This is valid because all following sections are either:
# 1. guarded by 'if in_doc' or
# 2. guarded by a pattern that matches the start of the sentinel.   
#
if not in_doc and not strip_line.startswith(sentinel):
    # lstrip() is faster than using a regex!
    body.append(line)
    continue
</t>
<t tx="ekr.20190914104955.9">m = others_pat.match(line)
if m:
    in_doc = False
    if m.group(2) == '+': # opening sentinel
        body.append('%s@others%s\n' % (m.group(1), m.group(3) or ''))
        stack.append((gnx, indent, body))
        indent += m.end(1) # adjust current identation
    else: # closing sentinel.
        # m.group(2) is '-' because the pattern matched.
        gnx, indent, body = stack.pop()
    continue
</t>
<t tx="ekr.20190914105123.1">from leo.core.leoBeautify import PythonTokenBeautifier, should_beautify

for child in p.subtree():
    child_s = child.b
    try:
        # Beautify all nodes.
        assert should_beautify(child), repr(child.h)
        c.selectPosition(child)
        PythonTokenBeautifier(c).prettyPrintNode(child)
        # Check only those nodes that should not have been changed.
        if child.h in ('basic test', 'sections references'):
            assert child_s.rstrip() == child.b.rstrip(), (
                f"FAIL: beautify-node: {child.h}\n"
                f"{g.objToString(child_s, tag='OLD')}\n"
                f"{g.objToString(child.b, tag='NEW')}")
    finally:
        child.b = child_s
        c.selectPosition(p)
</t>
<t tx="ekr.20190914105123.10">m = ref_pat.match(line)
if m:
    in_doc = False
    if m.group(2) == '+':
        # open sentinel.
        body.append(m.group(1) + g.angleBrackets(m.group(3)) + '\n')
        stack.append((gnx, indent, body))
        indent += m.end(1)
    else:
        # close sentinel.
        # m.group(2) is '-' because the pattern matched.
        gnx, indent, body = stack.pop()
    continue
</t>
<t tx="ekr.20190914105123.11">m = node_start_pat.match(line)
if m:
    in_doc, in_raw = False, False
    gnx, head = m.group(2), m.group(5)
    level = int(m.group(3)) if m.group(3) else 1 + len(m.group(4))
        # m.group(3) is the level number, m.group(4) is the number of stars.
    v = gnx2vnode.get(gnx)
    #
    # Case 1: The root @file node. Don't change the headline.
    if not root_seen:
        # Fix #1064: The node represents the root, regardless of the gnx!
        root_seen = True
        clone_v = None
        gnx2body[gnx] = body = []
        if not v:
            # Fix #1064.
            v = root_v
            # This message is annoying when using git-diff.
                # if gnx != root_gnx:
                    # g.es_print("using gnx from external file: %s" % (v.h), color='blue')
            gnx2vnode [gnx] = v
            v.fileIndex = gnx
        v.children = []
        continue
    #
    # Case 2: We are scanning the descendants of a clone.
    parent_v, clone_v = level_stack[level-2]
    if v and clone_v:
        # The last version of the body and headline wins..
        gnx2body[gnx] = body = []
        v._headString = head
        # Update the level_stack.
        level_stack = level_stack[:level-1]
        level_stack.append((v, clone_v),)
        # Always clear the children!
        v.children=[]
        parent_v.children.append(v)
        continue
    #
    # Case 3: we are not already scanning the descendants of a clone.
    if v:
        # The *start* of a clone tree. Reset the children.
        clone_v = v
        v.children = []
    else:
        # Make a new vnode.
        v = self.VNode(context=context, gnx=gnx)
    #
    # The last version of the body and headline wins.
    gnx2vnode[gnx] = v
    gnx2body[gnx] = body = []
    v._headString = head
    #
    # Update the stack.
    level_stack = level_stack[:level-1]
    level_stack.append((v, clone_v),)
    #
    # Update the links.
    assert v != root_v
    parent_v.children.append(v)
    v.parents.append(parent_v)
    # dump_v()
    continue
</t>
<t tx="ekr.20190914105123.12">if in_doc:
    # When delim_end exists the doc block:
    # - begins with the opening delim, alonw on its own line
    # - ends with the closing delim, alone on its own line.
    # Both of these lines should be skipped
    if line in doc_skip:
        # doc_skip is (delim_start + '\n', delim_end + '\n')
        continue
    #
    # Check for @c or @code.
    m = code_pat.match(line)
    if m:
        in_doc = False 
        body.append('@code\n' if m.group(1) else '@c\n')
        continue
else:
    m = doc_pat.match(line)
    if m:
        # @+at or @+doc?
        doc = '@doc' if m.group(1) == 'doc' else '@'
        doc2 = m.group(2) or '' # Trailing text.
        if doc2:
            body.append('%s%s\n'%(doc, doc2))
        else:
            body.append(doc + '\n')
        # Enter @doc mode.
        in_doc = True
        continue
</t>
<t tx="ekr.20190914105123.13">m = all_pat.match(line)
if m:
    # @all tells Leo's *write* code not to check for undefined sections.
    # Here, in the read code, we merely need to add it to the body.
    # Pushing and popping the stack may not be necessary, but it can't hurt.
    if m.group(2) == '+': # opening sentinel
        body.append('%s@all%s\n' % (m.group(1), m.group(3) or ''))
        stack.append((gnx, indent, body))
    else: # closing sentinel.
        # m.group(2) is '-' because the pattern matched.
        gnx, indent, body = stack.pop()
        gnx2body[gnx] = body
    continue
</t>
<t tx="ekr.20190914105123.14">m = after_pat.match(line)
if m:
    afterref = True
    verbatim = True
        # Avoid an extra test in the main loop.
    continue
</t>
<t tx="ekr.20190914105123.15">m = first_pat.match(line)
if m:
    if 0 &lt;= first_i &lt; len(first_lines):
        body.append('@first ' + first_lines[first_i])
        first_i += 1
    else:
        g.trace('\ntoo many @first lines: %s' %  path)
        print('@first is valid only at the start of @&lt;file&gt; nodes\n')
        g.printObj(first_lines, tag='first_lines')
        g.printObj(lines[start:i+2], tag='lines[start:i+2]')
    continue
m = last_pat.match(line)
if m:
    n_last_lines += 1
    continue
</t>
<t tx="ekr.20190914105123.16"># http://leoeditor.com/directives.html#part-4-dangerous-directives
m = comment_pat.match(line)
if m:
    # &lt;1, 2 or 3 comment delims&gt;
    delims = m.group(1).strip()
    # Whatever happens, retain the @delims line.
    body.append('@comment %s\n' % delims)
    delim1, delim2, delim3 = g.set_delims_from_string(delims)
        # delim1 is always the single-line delimiter.
    if delim1:
        delim_start, delim_end = delim1, ''
    else:
        delim_start, delim_end = delim2, delim3
    #
    # Within these delimiters:
    # - double underscores represent a newline.
    # - underscores represent a significant space,
    delim_start = delim_start.replace('__','\n').replace('_',' ')
    delim_end = delim_end.replace('__','\n').replace('_',' ')
    # Recalculate all delim-related values
    doc_skip = (delim_start + '\n', delim_end + '\n')
    is_cweb = delim_start == '@q@' and delim_end == '@&gt;'
    sentinel = delim_start + '@'
    #
    # Recalculate the patterns.
    delims = delim_start, delim_end
    (
        after_pat, all_pat, code_pat, comment_pat, delims_pat,
        doc_pat, end_raw_pat, first_pat, last_pat,
        node_start_pat, others_pat, raw_pat, ref_pat
    ) = self.get_patterns(delims)
    continue
</t>
<t tx="ekr.20190914105123.17">m = delims_pat.match(line)
if m:
    # Get 1 or 2 comment delims
    # Whatever happens, retain the original @delims line.
    delims = m.group(1).strip()
    body.append('@delims %s\n' % delims)
    #
    # Parse the delims.
    delims_pat = re.compile(r'^([^ ]+)\s*([^ ]+)?')
    m2 = delims_pat.match(delims)
    if not m2:
        g.trace('Ignoring invalid @comment: %r' % line)
        continue
    delim_start = m2.group(1)
    delim_end = m2.group(2) or ''
    #
    # Within these delimiters:
    # - double underscores represent a newline.
    # - underscores represent a significant space,
    delim_start = delim_start.replace('__','\n').replace('_',' ')
    delim_end = delim_end.replace('__','\n').replace('_',' ')
    # Recalculate all delim-related values
    doc_skip = (delim_start + '\n', delim_end + '\n')
    is_cweb = delim_start == '@q@' and delim_end == '@&gt;'
    sentinel = delim_start + '@'
    #
    # Recalculate the patterns
    delims = delim_start, delim_end
    (
        after_pat, all_pat, code_pat, comment_pat, delims_pat,
        doc_pat, end_raw_pat, first_pat, last_pat,
        node_start_pat, others_pat, raw_pat, ref_pat
    ) = self.get_patterns(delims)
    continue
</t>
<t tx="ekr.20190914105123.18"># http://leoeditor.com/directives.html#part-4-dangerous-directives
m = raw_pat.match(line)
if m:
    in_raw = True
    verbatim = True
        # Avoid an extra test in the main loop.
    continue
</t>
<t tx="ekr.20190914105123.19">if line.startswith(delim_start + '@-leo'):
    i += 1
    break
</t>
<t tx="ekr.20190914105123.2">@language python
@
This is a doc part.
@c

def spam():
    if 1:
        # Regular comment.
        print('-----')
            # Indented comment.
    else:
        pass
</t>
<t tx="ekr.20190914105123.20"># @first, @last, @delims and @comment generate @@ sentinels,
# So this must follow all of those.
if line.startswith(delim_start + '@@'):
    ii = len(delim_start) + 1 # on second '@'
    jj = line.rfind(delim_end) if delim_end else -1
    body.append(line[ii:jj] + '\n')
    continue
</t>
<t tx="ekr.20190914105123.21">if in_doc:
    if delim_end:
        # doc lines are unchanged.
        body.append(line)
    else:
        # Doc lines start with start_delim + one blank.
        body.append(line[len(delim_start)+1:])
    continue
</t>
<t tx="ekr.20190914105123.22"># Handle an apparent sentinel line.
# This *can* happen, as the result of the git-diff command.
#
# This assert verifies the short-circuit test.
assert strip_line.startswith(sentinel), (repr(sentinel), repr(line))
#
# This trace is less important, but interesting.
g.trace(f"{g.shortFileName(self.path)}: unexpected line: {line.strip()!r}")
body.append(line)
</t>
<t tx="ekr.20190914105123.3">a = 1
&lt;&lt; section ref &gt;&gt;
b = 2

if 1:
    @others

# Note: section references will fail in if statements, because pass is not valid there:
    
    # if (
        # &lt;section ref&gt;
    # ):
        # pass
</t>
<t tx="ekr.20190914105123.4">@language python

def scan_lines(self, delims, first_lines, lines, path, start, test=False):
    '''Scan all lines of the file, creating vnodes.'''
    &lt;&lt; init scan_lines &gt;&gt;
    &lt;&lt; define dump_v &gt;&gt;
    i = 0 # To keep pylint happy.
    for i, line in enumerate(lines[start:]):
        # Order matters.
        &lt;&lt; 1. common code for all lines &gt;&gt;
        &lt;&lt; 2. short-circuit later tests &gt;&gt;
        &lt;&lt; 3. handle @others &gt;&gt; # clears in_doc
        &lt;&lt; 4. handle section refs &gt;&gt; # clears in_doc.
        # Order doesn't matter, but match more common sentinels first.
        &lt;&lt; handle node_start &gt;&gt;
        &lt;&lt; handle end of @doc &amp; @code parts &gt;&gt;
        &lt;&lt; handle @all &gt;&gt;
        &lt;&lt; handle afterref &gt;&gt;
        &lt;&lt; handle @first and @last &gt;&gt;
        &lt;&lt; handle @comment &gt;&gt;
        &lt;&lt; handle @delims &gt;&gt;
        &lt;&lt; handle @raw &gt;&gt;
        &lt;&lt; handle @-leo &gt;&gt;
        # These must be last, in this order.
        &lt;&lt; Last 1. handle remaining @@ lines &gt;&gt;
        &lt;&lt; Last 2. handle remaining @doc lines &gt;&gt;
        &lt;&lt; Last 3. handle remaining @ lines &gt;&gt;
    else:
        # No @-leo sentinel
        return None, []
    # Handle @last lines.
    last_lines = lines[start+i:]
    if last_lines:
        last_lines = ['@last ' + z for z in last_lines]
        gnx2body[root_gnx] = gnx2body[root_gnx] + last_lines
    self.post_pass(gnx2body, gnx2vnode, root_v)
    return root_v, last_lines
</t>
<t tx="ekr.20190914105123.5">#
# Simple vars...
afterref = False
    # A special verbatim line follows @afterref.
clone_v = None
    # The root of the clone tree.
    # When not None, we are scanning a clone and all it's descendants.
delim_start, delim_end = delims
    # The start/end delims.
doc_skip = (delim_start + '\n', delim_end + '\n')
    # To handle doc parts.
first_i = 0
    # Index into first array.
in_doc = False
    # True: in @doc parts.
in_raw = False
    # True: @raw seen.
is_cweb = delim_start == '@q@' and delim_end == '@&gt;'
    # True: cweb hack in effect.
indent = 0 
    # The current indentation.
level_stack = []
    # Entries are (vnode, in_clone_tree)
n_last_lines = 0
    # The number of @@last directives seen.
root_seen = False
    # False: The next +@node sentinel denotes the root, regardless of gnx.
    # Needed to handle #1065 so reads will not create spurious child nodes.
sentinel = delim_start + '@'
    # Faster than a regex!
stack = []
    # Entries are (gnx, indent, body)
    # Updated when at+others, at+&lt;section&gt;, or at+all is seen.
verbline = delim_start + '@verbatim' + delim_end + '\n'
    # The spelling of at-verbatim sentinel
verbatim = False
    # True: the next line must be added without change.
#
# Init the data for the root node.
#

#
# Init the parent vnode for testing.
#
if self.test:
    root_gnx = gnx = 'root-gnx'
        # The node that we are reading.
        # start with the gnx for the @file node.
    gnx_head =  '&lt;hidden top vnode&gt;'
        # The headline of the root node.
    context = None
    parent_v = self.VNode(context=context, gnx=gnx)
    parent_v._headString = gnx_head
        # Corresponds to the @files node itself.
else:
    # Production.
    root_gnx = gnx = self.root.gnx
    context = self.c
    parent_v = self.root.v
root_v = parent_v
    # Does not change.
level_stack.append((root_v, False),)
#
# Init the gnx dict last.
#
gnx2vnode = self.gnx2vnode
    # Keys are gnx's, values are vnodes.
gnx2body = {}
    # Keys are gnxs, values are list of body lines.
gnx2vnode[gnx] = parent_v
    # Add gnx to the keys
gnx2body[gnx] = body = first_lines
    # Add gnx to the keys.
    # Body is the list of lines presently being accumulated.
#
# get the patterns.
after_pat, all_pat, code_pat, comment_pat, delims_pat,\
doc_pat, end_raw_pat, first_pat, last_pat, \
node_start_pat, others_pat, raw_pat, ref_pat = self.get_patterns(delims)
</t>
<t tx="ekr.20190914105123.6">def dump_v():
    '''Dump the level stack and v.'''
    print('----- LEVEL', level, v.h)
    print('       PARENT', parent_v.h)
    print('[')
    for i, data in enumerate(level_stack):
        v2, in_tree = data
        print('%2s %5s %s' % (i+1, in_tree, v2.h))
    print(']')
    print('PARENT.CHILDREN...')
    g.printObj([v3.h for v3 in parent_v.children])
    print('PARENTS...')
    g.printObj([v4.h for v4 in v.parents])
</t>
<t tx="ekr.20190914105123.7">if verbatim:
    # We are in raw mode, or other special situation.
    # Previous line was verbatim sentinel. Append this line as it is.
    if afterref:
        afterref = False
        if body: # a List of lines.
            body[-1] = body[-1].rstrip() + line
        else:
            body = [line]
        verbatim = False
    elif in_raw:
        m = end_raw_pat.match(line)
        if m:
            in_raw = False
            verbatim = False
        else:
             body.append(line)
             # Continue verbatim/raw mode.
    else:
        body.append(line)
        verbatim = False
    continue
if line == verbline: # &lt;delim&gt;@verbatim.
    verbatim = True
    continue
#
# Strip the line only once.
strip_line = line.strip()
#
# Undo the cweb hack.
if is_cweb and line.startswith(sentinel):
    line = line[:len(sentinel)] + line[len(sentinel):].replace('@@', '@')
# Adjust indentation.
if indent and line[:indent].isspace() and len(line) &gt; indent:
    line = line[indent:]
</t>
<t tx="ekr.20190914105123.8"># This is valid because all following sections are either:
# 1. guarded by 'if in_doc' or
# 2. guarded by a pattern that matches the start of the sentinel.   
#
if not in_doc and not strip_line.startswith(sentinel):
    # lstrip() is faster than using a regex!
    body.append(line)
    continue
</t>
<t tx="ekr.20190914105123.9">m = others_pat.match(line)
if m:
    in_doc = False
    if m.group(2) == '+': # opening sentinel
        body.append('%s@others%s\n' % (m.group(1), m.group(3) or ''))
        stack.append((gnx, indent, body))
        indent += m.end(1) # adjust current identation
    else: # closing sentinel.
        # m.group(2) is '-' because the pattern matched.
        gnx, indent, body = stack.pop()
    continue
</t>
<t tx="ekr.20190914111357.1">from leo.core.leoBeautify import PythonTokenBeautifier, should_beautify
errors, fails, tests = [], [], []
try:
    for peeve in p.children():
        if peeve.h.startswith('SKIP:'):
            continue
        test = peeve.firstChild()
        tests.append(peeve.h)
        expected = test.next()
        assert test.h.lower() == 'test', repr(test.h)
        assert expected.h.lower() == 'expected', repr(expected.h)
        expected_s = expected.b
        test_s = test.b
        try:
            # Beautify "test" node in place.
            assert should_beautify(test), repr(test.h)
            ok = PythonTokenBeautifier(c).prettyPrintNode(test) or test.b.strip() == expected.b.strip()
            if not ok:
                errors.append(peeve.h)
                print(
                    f"ERROR: {peeve.h}\n"
                    f"{g.objToString(test_s, tag='TEST')}\n")
            elif test.b.rstrip() != expected.b.rstrip():
                fails.append(peeve.h)
                # self.assertEqual(test_s, test.b, f"Beautify-node FAIL: {peeve.h}\n")
                print(
                    f"Beautify-node FAIL: {peeve.h}\n"
                    f"{g.objToString(test_s,     tag='    TEST')}\n"
                    f"{g.objToString(test.b,     tag='     GOT')}\n"
                    f"{g.objToString(expected.b, tag='EXPECTED')}\n")
        finally:
            test.b = test_s
            expected.b = expected_s
finally:
    c.setChanged(False)
if 0:
    g.printObj(tests, tag='TESTS')
if fails or errors:
    self.fail("\n"
        f"{g.objToString(fails,  tag='Failures')}\n"
        f"{g.objToString(errors, tag='  Errors')}")
</t>
<t tx="ekr.20190914111357.2">def spam():
    if - 1 &lt; 2:
        pass
</t>
<t tx="ekr.20190914111357.3">def spam():
    if -1 &lt; 2:
        pass
</t>
<t tx="ekr.20190914112029.1"></t>
<t tx="ekr.20190914115215.1"></t>
<t tx="ekr.20190914115300.1">spam(ham[1], {eggs: 2})
spam( ham[ 1 ], { eggs: 2 } )
</t>
<t tx="ekr.20190914115304.1">spam(ham[1], {eggs: 2})
spam(ham[1], {eggs: 2})
</t>
<t tx="ekr.20190914115821.1"></t>
<t tx="ekr.20190914120010.1">foo = (0,)
bar = (0, )
</t>
<t tx="ekr.20190914120016.1">foo = (0,)
bar = (0,)
</t>
<t tx="ekr.20190914120318.1"></t>
<t tx="ekr.20190914120402.1">if x == 4: pass
if x == 4 : pass
print (x, y); x, y = y, x
print (x , y) ; x , y = y , x
x, y = y, x
x , y = y , x
</t>
<t tx="ekr.20190914120406.1">if x == 4: pass
if x == 4: pass
print(x, y); x, y = y, x
print(x, y); x, y = y, x
x, y = y, x
x, y = y, x
</t>
<t tx="ekr.20190914122333.1"></t>
<t tx="ekr.20190914122356.1">ham[1: 9], ham[1: 9: 3], ham[: 9: 3], ham[1:: 3], ham[1: 9:]
ham[ lower: upper ], ham[lower : upper: ], ham[lower :: step]
ham[: upper]
</t>
<t tx="ekr.20190914122359.1">ham[1 : 9], ham[1 : 9 : 3], ham[:9 : 3], ham[1 :: 3], ham[1 : 9:]
ham[lower : upper], ham[lower : upper:], ham[lower :: step]
ham[: upper]
</t>
<t tx="ekr.20190914123001.1"></t>
<t tx="ekr.20190914123101.1">spam(1)
spam ( 1 )
dct ['key'] = lst [index]
dct['key'] = lst[index]
</t>
<t tx="ekr.20190914123104.1">spam(1)
spam(1)
dct['key'] = lst[index]
dct['key'] = lst[index]
</t>
<t tx="ekr.20190914132845.1">from leo.core.leoBeautify import BlackCommand, should_beautify
try:
    import black
except ImportError:
    self.skipTest('can not import black')
try:
    mode = black.FileMode()
except TypeError:
    self.skipTest('old version of black')
fails = []
try:
    for peeve in p.children():
        if peeve.h.startswith('SKIP:'):
            continue
        test = peeve.firstChild()
        expected = test.next()
        assert test.h.lower() == 'test', repr(test.h)
        assert expected.h.lower() == 'expected', repr(expected.h)
        expected_s = expected.b
        test_s = test.b
        try:
            # Beautify "test" node in place.
            assert should_beautify(test), repr(test.h)
            c.selectPosition(test)
            BlackCommand(c).blacken_node(test, diff_flag=False, check_flag=False)
            if test.b.rstrip() != expected.b.rstrip():
                fails.append(peeve.h)
                print(
                    f"blacken-node FAIL: {peeve.h}\n"
                    f"{g.objToString(test_s, tag='TEST')}\n"
                    f"{g.objToString(test.b, tag='GOT')}\n"
                    f"{g.objToString(expected.b, tag='EXPECTED')}")
        finally:
            test.b = test_s
            expected.b = expected_s
finally:
    c.selectPosition(p)
    c.setChanged(False)
assert not fails, g.objToString(fails, tag='FAILED tests')
</t>
<t tx="ekr.20190914132845.10">foo = (0,)
bar = (0,)
</t>
<t tx="ekr.20190914132845.11"># Avoid extraneous whitespace in the following situations:
    
# Immediately inside parentheses, brackets or braces.

# Yes: spam(ham[1], {eggs: 2})
# No:  spam( ham[ 1 ], { eggs: 2 } )
</t>
<t tx="ekr.20190914132845.12">spam(ham[1], {eggs: 2})
spam( ham[ 1 ], { eggs: 2 } )
</t>
<t tx="ekr.20190914132845.13">spam(ham[1], {eggs: 2})
spam(ham[1], {eggs: 2})
</t>
<t tx="ekr.20190914132845.14"># Avoid extraneous whitespace in the following situations:

# Immediately before a comma, semicolon, or colon:

# Yes: if x == 4: pass
# No:  if x == 4 : pass
if x == 4: pass

# YES: print (x, y); x, y = y, x)
# NO: print (x , y) ; x , y = y , x
print(x, y); x, y = y, x
</t>
<t tx="ekr.20190914132845.15">if x == 4: pass
if x == 4 : pass
print (x, y); x, y = y, x
print (x , y) ; x , y = y , x
x, y = y, x
x , y = y , x
</t>
<t tx="ekr.20190914132845.16">if x == 4:
    pass
if x == 4:
    pass
print(x, y)
x, y = y, x
print(x, y)
x, y = y, x
x, y = y, x
x, y = y, x
</t>
<t tx="ekr.20190914132845.17"># Avoid extraneous whitespace in the following situations:
    
# Immediately before the open parenthesis that starts the argument list of a function call:

# Yes: spam(1)
spam(1)

# Immediately before the open parenthesis that starts an indexing or slicing:

# Yes: dct['key'] = lst[index]
dct['key'] = lst[index]
</t>
<t tx="ekr.20190914132845.18">spam(1)
spam ( 1 )
dct ['key'] = lst [index]
dct['key'] = lst[index]
</t>
<t tx="ekr.20190914132845.19">spam(1)
spam(1)
dct['key'] = lst[index]
dct['key'] = lst[index]
</t>
<t tx="ekr.20190914132845.2"># Avoid extraneous whitespace in the following situations:

# However, in a slice the colon acts like a binary operator,
# and should have equal amounts on either side (treating it as the operator with the lowest priority).
# In an extended slice, both colons must have the same amount of spacing applied.
# Exception: when a slice parameter is omitted, the space is omitted.

# Yes:
# ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:]
# ham[lower:upper], ham[lower:upper:], ham[lower::step]
# ham[lower+offset : upper+offset]
# ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)]
# ham[lower + offset : upper + offset]

# FAIL
ham[1: 9], ham[1: 9: 3], ham[: 9: 3], ham[1:: 3], ham[1: 9:]
ham[lower + offset: upper + offset]
ham[1: 9], ham[1: 9], ham[1: 9: 3]
ham[lower:: upper]
ham[: upper]
</t>
<t tx="ekr.20190914132845.3"># FAIL
ham[1: 9], ham[1: 9: 3], ham[: 9: 3], ham[1:: 3], ham[1: 9:]
ham[lower + offset: upper + offset]
ham[1: 9], ham[1: 9], ham[1: 9: 3]
ham[lower:: upper]
ham[: upper]

# PASS
ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:]
ham[lower:upper], ham[lower:upper:], ham[lower::step]
ham[lower+offset : upper+offset]
ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)]
ham[lower + offset : upper + offset]
</t>
<t tx="ekr.20190914132845.4"># FAIL
ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:]
ham[lower + offset : upper + offset]
ham[1:9], ham[1:9], ham[1:9:3]
ham[lower::upper]
ham[:upper]

# PASS
ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:]
ham[lower:upper], ham[lower:upper:], ham[lower::step]
ham[lower + offset : upper + offset]
ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)]
ham[lower + offset : upper + offset]
</t>
<t tx="ekr.20190914132845.5"># Avoid extraneous whitespace in the following situations:

# Immediately inside parentheses, brackets or braces.

# Yes: spam(ham[1], {eggs: 2})
# No:  spam( ham[ 1 ], { eggs: 2 } )
spam(ham[1], {eggs: 2})
</t>
<t tx="ekr.20190914132845.6">def spam():
    if - 1 &lt; 2:
        pass
</t>
<t tx="ekr.20190914132845.7">def spam():
    if -1 &lt; 2:
        pass
</t>
<t tx="ekr.20190914132845.8"># Avoid extraneous whitespace in the following situations:

# Between a trailing comma and a following close parenthesis.

# Yes: foo = (0,)
# No:  bar = (0, )
bar = (0,)
</t>
<t tx="ekr.20190914132845.9">foo = (0,)
bar = (0, )
</t>
<t tx="ekr.20190914181159.1"></t>
<t tx="ekr.20190914181159.2">ham[lower + offset: upper + offset]
ham[lower+offset : upper+offset]
</t>
<t tx="ekr.20190914181159.3">ham[lower + offset : upper + offset]
ham[lower + offset : upper + offset]
</t>
<t tx="ekr.20190914181344.1"></t>
<t tx="ekr.20190914181344.2">ham[ : upper_fn(x) : step_fn(x)]
ham[ : : step_fn(x)]
</t>
<t tx="ekr.20190914181344.3">ham[: upper_fn(x) : step_fn(x)]
ham[:: step_fn(x)]
</t>
<t tx="ekr.20190915074109.1"></t>
<t tx="ekr.20190915074201.1">def foo(a, b):
    foo(a = 2+3, b = 4-5, c = 6*7, d = 8 / 9, e =10 // 11)
    bar(2 + baz)
</t>
<t tx="ekr.20190915074205.1">def foo(a, b):
    foo(a=2+3, b=4-5, c=6*7, d=8/9, e=10//11)
    bar(2+baz)
</t>
<t tx="ekr.20190915083243.1"></t>
<t tx="ekr.20190915083412.1">b =    ('and', 'in', 'not', 'not in', 'or')
# def foo():
    # return ''.join([z.to_string() for z in self.code_list])
</t>
<t tx="ekr.20190915083418.1">b = ('and', 'in', 'not', 'not in', 'or')
# def foo():
    # return ''.join([z.to_string() for z in self.code_list])
</t>
<t tx="ekr.20190915083933.1"></t>
<t tx="ekr.20190915083945.1">a = b # comment
c = d# comment
e - f   # comment
# Single-line comment.
</t>
<t tx="ekr.20190915084039.1">a = b  # comment
c = d  # comment
e - f  # comment
# Single-line comment.
</t>
<t tx="ekr.20190923014948.1"></t>
<t tx="ekr.20191106035017.1">import io
import tokenize
import leo.core.leoBeautify as leoBeautify

table = (

# New fail 1 from leoAst.py
r'''
def get_fields(self, node):
    return (
        (a, b) for a, b in ast.iter_fields(node)
            if a not in self.disabled_fields and b not in (None, [])
    )
''',
# Fail 1 from leoAst.py
r'''
raise AstNotEqual(
    f"node1.__class__.__name__: {node1.__class__.__name__}\n"
    f"node2.__class__.__name__: {node2.__class__.__name_}"
)
''',
# Fail 2 from leoAst.py
r'''
remove = [
    'Interactive', 'Suite',  # Not necessary.
    'PyCF_ONLY_AST',  # A constant,
    'AST',  # The base class,
]
''',
# Fail 3 from leoAst.py.
'''
table = (
    AstFullTraverser,
    AstFormatter,
    AstPatternFormatter,
    HTMLReportTraverser,
)
''',
# Basic indentation test.
r'''
def foo(a, b):
    foo()
    bar()
''',

# Backslash-newline test.
# There must be a single blank before the bs-nl
# because *regularizes* whitespace before bs-nl.
r'''
print \
    ( \
    a, \
    b \
)
a = "b \
c \
d"
print \
    ('done')
''',

# Basic Indentation test.
# Note: ptb retains the exact spelling of all strings.
r'''
print('hi')
print(
    'thing 1',
    'thing 2',
)
print(
    'thing 3\
    thing 4 \
    thing5'
)
''',
) # End of table.

beautifiers = (
    ('NullTokenBeautifier', leoBeautify.NullTokenBeautifier),
    ('FstringifyTokens', leoBeautify.FstringifyTokens),
    ('PythonTokenBeautifier', leoBeautify.PythonTokenBeautifier),
)
for i, contents in enumerate(table):
    for name, beautifier_class in beautifiers:
        # Careful: must retokenize each time.
        x = beautifier_class(c)
        # x.dump_input_tokens = True
        # x.dump_output_tokens = True
        tokens = list(tokenize.tokenize(io.BytesIO(contents.encode('utf-8')).readline))
        results = x.scan_all_tokens(contents, tokens)
        if results.strip() != contents.strip():
            for j, s in enumerate(g.splitLines(contents.strip())):
                print(f"{j:2}: {s!r}")
            print('\nResults...')
            for j, s in enumerate(g.splitLines(results.strip())):
                print(f"{j:2}: {s!r}")
            assert False, f"FAIL: Test {i} for {name}"
# print('done')
</t>
<t tx="ekr.20191106072009.1">import io
import tokenize
import leo.core.leoBeautify as leoBeautify

table = (

( # Test 1.
r'''
g.es('%s blah blah' % (
    g.angleBrackets('*')))
''',
r'''
g.es(f"{g.angleBrackets('*')} blah blah")
''',
),

( # Test 2.
r'''
mods = ''.join(['%s+' % z.capitalize() for z in self.mods])
''',
r'''
mods = ''.join([f"{z.capitalize()}+" for z in self.mods])
''',
),

( # Test 3.
r'''
ret = '[%s]' % ','.join([self.show(z) for z in arg])
''',
r'''
ret = f"[{','.join([self.show(z) for z in arg])}]"
''',
),

( # Test 4.
r'''
return '[%s]' % ' '.join(result).strip()
''',
r'''
return f"[{' '.join(result).strip()}]"
'''
),

( # Test 5.
r'''
return'%s' % a
return '%s' % b
''',
r'''
return f"{a}"
return f"{b}"
''',
),

( # Test 6.
r'''
print("%dx%d%+d%+d" % (300, 200, 50, 50))
print("%dx%d%-d%-d" % (300, 200, 50, 50))
''',
r'''
print(f"{300:d}x{200:d}{50:+d}{50:+d}")
print(f"{300:d}x{200:d}{50:&gt;d}{50:&gt;d}")
''',
),

( # Test 7
r'''
return "^@(%s)" % "|".join(aList)
''',
r'''
return f"^@({'|'.join(aList)})"
'''
),

( # Test 8.
r'''
g.trace('done: %5s page: %3s found: %s label: %s' % (
    done, page, n, label))
''',
r'''
g.trace(f"done: {done:5} page: {page:3} found: {n} label: {label}")
'''
),

) # End of all tests.

for i, data in enumerate(table):
    test, expected = data
    contents = test.strip() ###### + '\n'
    x = leoBeautify.FstringifyTokens(c)
    # x.dump_input_tokens = True
    # x.dump_output_tokens = True
    tokens = list(tokenize.tokenize(io.BytesIO(contents.encode('utf-8')).readline))
    results = x.scan_all_tokens(contents, tokens)
    if results.strip() != expected.strip():
        print('\nFAIL test', i+1)
        print('\nContents...')
        for j, s in enumerate(g.splitLines(contents.strip())):
            print(f"{j:2}: {s.rstrip()}")
        print('\nExpected...')
        for j, s in enumerate(g.splitLines(expected.strip())):
            print(f"{j:2}: {s.rstrip()}")
        print('\nResults...')
        for j, s in enumerate(g.splitLines(results.strip())):
            print(f"{j:2}: {s.rstrip()}")
        assert False, f"FAIL"
</t>
<t tx="ekr.20191106195303.1">'''These tests cover potential fstrings that can't be automatically converted.'''

import io
import tokenize
import leo.core.leoBeautify as leoBeautify

table = (

# Test 1.
r'''
ret = '[\n%s]' % ('\n,'.join([self.show(z) for z in arg]))
''',

# Test 2.
r'''
print('Test %s' % '\none')
'''

)

for test in table:
    contents = test.strip()
    x = leoBeautify.FstringifyTokens(c)
    # x.dump_input_tokens = True
    # x.dump_output_tokens = True
    tokens = list(tokenize.tokenize(io.BytesIO(contents.encode('utf-8')).readline))
    results = x.scan_all_tokens(contents, tokens)
    # print(results)
    if results.strip() != test.strip():
        print('\nContents...')
        for j, s in enumerate(g.splitLines(contents.strip())):
            print(f"{j:2}: {s.rstrip()}")
        print('\nResults...')
        for j, s in enumerate(g.splitLines(results.strip())):
            print(f"{j:2}: {s.rstrip()}")
        assert False, f"FAIL"
</t>
<t tx="ekr.20191118154059.1">"""
The master unit test for leoAst.TokenOrderGenerator and related classes.
"""
import time
import leo.core.leoAst as leoAst
&lt;&lt; helper functions &gt;&gt;
# Flags for *this* code.
flags = [
    # 'all',
    'g.cls',  ### Don't use in production.
    'reload',
    'show-test-kind', # all vs selected.
    # 'show-test-description', 
    # 'show-reload-time',
    # 'show-test-time',
    'summarize',
]
if 'g.cls' in flags:
    g.cls()
#
# Reports, for TestRunner.run_tests.
if 'all' in flags:
    reports = [
         # 'no-sync',
    ]
else:
    reports = [
        # 'no-sync',
        'dump-sources-first',
        # 'trace-tokenizer-tokens',
        # 'dump-tokens-first',
        'set-trace-mode',  # Sets TOG.trace_mode.
        # 'show-exception-after-fail',
        # 'dump-tokens-after-fail',
        'dump-tree-after-fail',
        # 'dump-all-after-fail',
        # 'dump-contents',
        # 'dump-raw-tree',
        # 'dump-tokens',
        'dump-tree',
        # 'trace-times',  # Only for'file:' tests.
        # 'use-asttokens',
]
# Reload (if enabled), then make tests, run tests, and summarize.
reload_time = reload_helper()
show_status()
tests = make_tests()
fails, test_time = run_tests()
summarize()</t>
<t tx="ekr.20191118154839.1">def make_tests():
    """
    Return a list of tuples (contents, description) found in all children of the
    root, except this node.
    """
    import os
    root = c.p
    tests = []
    contents_tag = 'test:'
    file_tag = 'file:'
    after = root.nodeAfterTree()
    p = root.copy()
    while p and p != after:
        # g.trace(p.h)
        if 'helper functions' in p.h:
            # skip this node.
            p.moveToNext()
        elif p.h.startswith(('fail:', 'fails')):
            # Ignore all fails, regardless of 'all' flag.
            p.moveToNodeAfterTree()
        elif 'all' not in flags and p.h.startswith('ignore:'):
            # Honor 'ignore' only when *not* runnining all tests.
            p.moveToNodeAfterTree()
        elif p.h.startswith(contents_tag):
            description = p.h
            contents = p.b.strip() + '\n'
            tests.append((contents, description))
            p.moveToThreadNext()
        elif p.h.startswith(file_tag):
            description = p.h
            s = p.h[len(file_tag):].strip()
            parts = [g.app.loadDir, '..'] + s.split('..')
            path = os.path.sep.join(parts)
            if os.path.exists(path):
                with open(path, 'r') as f:
                    contents = f.read()
                tests.append((contents, description))
                p.moveToThreadNext()
            else:
                assert False, f"file not found: {path}"
        else:
            # Ignore organizer nodes.
            p.moveToThreadNext()
    if not tests:
        print(f"no tests in {root.h}")
    return tests
</t>
<t tx="ekr.20191118155609.1"></t>
<t tx="ekr.20191118155611.1">print('test %s=%s'%(a, 2))
print('done')
</t>
<t tx="ekr.20191118155612.1">open(os.devnull, "w")
</t>
<t tx="ekr.20191118155612.2">a = 1 ; b = 2
print('a') ; print('b')
</t>
<t tx="ekr.20191118160305.1">print('c');
print('d')
</t>
<t tx="ekr.20191119101124.1">name = str(g.os_path_normpath(g.os_path_join(theDir, 'leoProfile')))
</t>
<t tx="ekr.20191119133927.1">class TestClass1:
    pass
    
def decorator():
    pass
    
@decorator
class TestClass2:
    pass
    
@decorator
class TestClass(base1, base2):
    pass
</t>
<t tx="ekr.20191119142414.1">def run(fileName=None, pymacs=None, *args, **keywords):
    pass
</t>
<t tx="ekr.20191119143517.1">f2(a,b=2)
</t>
<t tx="ekr.20191119150253.1">f1(arg, *args, **kwargs)
</t>
<t tx="ekr.20191119203945.1"></t>
<t tx="ekr.20191119210429.1">print(-(2))</t>
<t tx="ekr.20191119211714.1">if a and not b and c:
    pass
</t>
<t tx="ekr.20191119213126.1">for i, j in b:
    pass
</t>
<t tx="ekr.20191119213940.1">any([p2.isDirty() for p2 in p.subtree()])
</t>
<t tx="ekr.20191120064442.1">if (a):
    print('a1')
    print('a2')
elif b:
    print('b1')
    print('b2')
else:
    print('c1')
    print('c2')
</t>
<t tx="ekr.20191120074811.1"></t>
<t tx="ekr.20191120075520.1"></t>
<t tx="ekr.20191120132531.1">if 1:
    print('a')
else:
    print('b')</t>
<t tx="ekr.20191120163559.1">try:
    print('a1')
    print('a2')
except ImportError:
    print('b1')
    print('b2')
except SyntaxError:
    print('c1')
    print('c2')
finally:
    print('d1')
    print('d2')
</t>
<t tx="ekr.20191120164539.1">if a:
    if b:
        print('b')
else:
    if d:
        print('d')
</t>
<t tx="ekr.20191120171921.1">if 1:
    print('a')
elif 2:
    print('b')
elif 3:
    print('c')
    print('d')
print('-')
if 1:
    print('e')
elif 2:
    print('f')
    print('g')


</t>
<t tx="ekr.20191121063148.1">import leo.core.leoAst as leoAst
if 1: ###
    import imp
    imp.reload(leoAst)
import asttokens
import glob
import os
import time
import token as tm
# Calculate files
core_directory = g.os_path_finalize_join(g.app.loadDir, '..', 'core')
assert os.path.exists(core_directory), core_directory
file_names = glob.glob(core_directory + os.path.sep + 'leo*.py')
for file_name in file_names[:1]:
    path = g.os_path_finalize_join(core_directory, file_name)
    assert os.path.exists(path), path
    with open(path, 'r') as f:
        contents = f.read()
    # Setup asttokens data.
    t1 = time.process_time()
    atok = asttokens.ASTTokens(contents, parse=True, filename=file_name)
    ast_tree = atok.tree
    ast_tokens = atok._tokens
    # Setup TOG data.
    t2 = time.process_time()
    x = leoAst.TokenOrderGenerator() # Use the base class.
    tog_tokens = x.make_tokens(contents)
    tog_tree = leoAst.parse_ast(contents)
    # Compare tokens
    t3 = time.process_time()
    def ast_key(t):
        kind = tm.tok_name[t.type].lower()
        return f"{kind}:{t.string}"

    def token_filter(t):
        return t.kind not in ('encoding', 'ws')
    
    ast_list = [ast_key(z) for z in ast_tokens]
    tog_compare_list = list(filter(token_filter, tog_tokens))
    tog_list = [f"{z.kind}:{z.value}" for z in tog_compare_list]
    # Compare.
    i = leoAst.compare_lists(ast_list, tog_list)
    t4 = time.process_time()
    ok = i is None
    if not ok:
        print('Token mismatch at', i)
        print('ast', repr(ast_list[i]))
        print('tog', repr(tog_compare_list[i]))
    # Print tokens.
    if 0:
        import token as tm
        print('asttokens.tokens...')
        for z in ast_tokens[:10]:
            kind = tm.tok_name[z.type].lower()
            print(f"{z.index:4} {kind:&gt;12} {z.string!r}")
        print('tog.tokens...')
        for z in tog_tokens[:10]:
            print(f"{z.index:4} {z.kind:&gt;12} {z.value!r}")
    # Print summary.
    if not ok:
        print(
            f"{g.shortFileName(file_name)}:\n"
            f"ast_tokens: {len(ast_tokens):&gt;5} "
            f"{t2-t1:4.2f} sec.\n"
            f"tog.tokens: {len(tog_tokens):&gt;5} "
            f"{t3-t2:4.2f} sec.\n"
            f"compare tokens: {t4-t3:4.2f}")
    assert ok
            </t>
<t tx="ekr.20191121163350.1">import leo.core.leoAst as leoAst
if 1: ###
    import imp
    imp.reload(leoAst)
import ast
import asttokens
import glob
import os
import time
if 0:
    file_names = ['contents']
    contents = '''
g.es_print(f"removing callback: {callback}")
'''
else:
    core_directory = g.os_path_finalize_join(g.app.loadDir, '..', 'core')
    assert os.path.exists(core_directory), core_directory
    file_names = glob.glob(core_directory + os.path.sep + 'leo*.py')

for file_name in file_names[:1]:
    if file_name == 'contents':
        pass
    else:
        path = g.os_path_finalize_join(core_directory, file_name)
        assert os.path.exists(path), path
        with open(path, 'r') as f:
            contents = f.read()
    # Setup asttokens data.
    t1 = time.process_time()
    atok = asttokens.ASTTokens(contents, parse=True, filename=file_name)
    ast_tree = atok.tree
    ast_nodes = list(asttokens.util.walk(ast_tree))
    # Compute tog nodes.
    t2 = time.process_time()
    x = leoAst.TokenOrderNodeGenerator()
    tog_tree = leoAst.parse_ast(contents)
    tog_nodes = list(x.generate_nodes(tog_tree))
    # Filter the nodes.
    t3 = time.process_time()
    ast_nodes2 = [z.__class__.__name__ for z in ast_nodes
        if not isinstance(z, (ast.alias, int, bool, str))]
    tog_nodes2 = [z.__class__.__name__ for z in tog_nodes]
    t4 = time.process_time()
    print('Comparing trees in', g.shortFileName(file_name), '...')
    print('len(ast):', len(ast_nodes2), 'len(tog)', len(tog_nodes2))
    ### g.printObj([z.__class__.__name__ for z in tog_nodes[:10]])
    i = leoAst.compare_lists(ast_nodes2, tog_nodes2)
    ok = i is None
    if ok:
        print('Tree match!', file_name)
    else:
        print('Tree mismatch at', i)
        # limit = min(len(ast_nodes2), len(tog_nodes2))
        for j in range(i-5, i+5):
            if j &gt;= 0:
                print('ast', j, ast_nodes2[j-1:j][0])
                print('tog', j, tog_nodes2[j-1:j][0])
                print('')
        print('Tree mismatch at', i)
    # Print tokens.
    if 1:
        print('')
        print(
            f"file: {g.shortFileName(file_name)}:\n"
            f"ast_nodes: {len(ast_nodes):&gt;5} "
            f"{t2-t1:5.2f} sec.\n"
            f"tog.nodes: {len(tog_nodes):&gt;5} "
            f"{t3-t2:5.2f} sec.\n"
            f"compare nodes: {t4-t3:4.2f}")
    assert ok</t>
<t tx="ekr.20191122153258.1">run(a=None, b=str)</t>
<t tx="ekr.20191122201558.1"></t>
<t tx="ekr.20191122203522.1">"""ds 1"""
class TestClass:
    """ds 2"""
    def long_name(a, b=2):
        """ds 3"""
        print('done')
</t>
<t tx="ekr.20191122210035.1">if 1:
    print('a')
else:
    if 2:
        print('b')
</t>
<t tx="ekr.20191122210522.1"></t>
<t tx="ekr.20191122213610.1">if 1:
    pass
elif 2:
    pass
    pass
</t>
<t tx="ekr.20191123203957.1"></t>
<t tx="ekr.20191123210801.1">if -(2):
    pass
</t>
<t tx="ekr.20191124040736.1"></t>
<t tx="ekr.20191125063105.1"># All helpers have access to symbols defined at the top level,
# namely flags, reports, reload_time and test_time.

@others</t>
<t tx="ekr.20191125063255.1">def show_status():
    print('')
    if 'show-test-kind' in flags:
        kind = 'all' if 'all' in flags else 'selected'
        print(f"Running *{kind}*' unit tests...\n")
    if 'asttokens' in reports:
        print('\nUsing asttokens, *not* the TOG classes')
    if 'reload' in flags and 'show-reload-time' in flags:
        print(f"\n   Reload time: {reload_time:4.2f} sec.")
</t>
<t tx="ekr.20191125064740.1">def summarize():
    if 'summarize' in flags:
        status = 'FAIL' if fails else 'PASS'
        if fails:
            print('')
            g.printObj(fails, tag='Failed tests')
        print(f"\n{status} Ran {len(tests)} test{g.plural(len(tests))}")
    if 'show-test-time' in flags:
        print(f"Run tests time: {test_time:4.2f} sec.")
</t>
<t tx="ekr.20191125065230.1">def reload_helper():
    
    reload_t1 = time.process_time()
    if 'reload' in flags:
        print('**Reloading**')
        import imp
        imp.reload(leoAst)
    reload_t2 = time.process_time()
    return reload_t2 - reload_t1
</t>
<t tx="ekr.20191125065806.1">def filter_reports(description):
    """Add trace-times only if we are running a 'file:' test."""
    aList = [z for z in reports if z != 'trace-times']
    if 'trace-times' in reports and description.startswith('file:'):
        aList.append('trace-times')
    return aList

def run_tests():
    
    fails = []
    t1 = time.process_time()
    for contents, description in tests:
        # Test runner catches all exceptions.
        if 'show-test-description' in flags:
            print(f"Running {description}...")
        x = leoAst.TestRunner()
        ok = x.run_tests(contents, description, filter_reports(description))
        if not ok:
            # if 'show-test-description' not in flags:
                # print(f"\nFailed: {description}")
            fails.append(description)
    t2 = time.process_time()
    test_time = t2 - t1
    return fails, test_time
</t>
<t tx="ekr.20191125144727.1">print('hi')</t>
<t tx="ekr.20191125151346.1"></t>
<t tx="ekr.20191125161333.1">if 1:
    a = 'class' if cond else 'def'
    # find_pattern = prefix + ' ' + word
    '1'
else:
    '2'
</t>
<t tx="ekr.20191125165640.1">func(f"{b if not cond1 else ''}")</t>
<t tx="ekr.20191125235415.1">print('a''.''b')
print('c')</t>
<t tx="ekr.20191125235556.1">"""
The master unit test for leoAst.TokenOrderGenerator and related classes.
"""
import time
import leo.core.leoAst as leoAst
&lt;&lt; helper functions &gt;&gt;
# Flags for *this* code.
flags = [
    'all'
    'show-test-kind',
    'show-test-time',
    'summarize',
]
g.cls() ###
#
# Reports when 'all' is False
reports = [
    # 'verbose-fail',
    # 'trace-mode',  # Sets TOG.trace_mode.
    # 'trace-times',  # Only for'file:' tests.
    # 'dump-all',
    # 'asttokens',
    # 'test-links',
    'contents',
    # 'tokens',
    # 'raw-tree',
    'tree',
]
# Make tests, run tests, and summarize.
reload_time = 0.0
show_status()
tests = make_tests()
fails, test_time = run_tests()
summarize()
assert not fails, f"failed {len(fails)} tests"</t>
<t tx="ekr.20191125235556.2"># All helpers have access to symbols defined at the top level,
# namely flags, reports, reload_time and test_time.

@others</t>
<t tx="ekr.20191125235556.3">def filter_reports(description):

    if 'all' in flags:
        return [z for z in reports if z == 'asttokens']
    # Set 'trace-times' only for 'file:' tests.
    aList = [z for z in reports if z != 'trace-times']
    if 'trace-times' in reports and description.startswith('file:'):
        reports2.append('trace-times')
    return aList</t>
<t tx="ekr.20191125235556.4">def make_tests():
    """
    Return a list of tuples (contents, description) for all of Leo's core .py files.
    """
    import glob
    import os
    core_directory = g.os_path_finalize_join(g.app.loadDir, '..', 'core')
    assert os.path.exists(core_directory), core_directory
    paths = glob.glob(core_directory + os.path.sep + 'leo*.py')
    tests = []
    for path in paths:
        assert os.path.exists(path), path
        with open(path, 'r') as f:
            contents = f.read()
        description = path
        tests.append((contents, description))   
    return tests
</t>
<t tx="ekr.20191125235556.5">def reload_helper():
    
    reload_t1 = time.process_time()
    if 'reload' in flags:
        print('**Reloading**')
        import imp
        imp.reload(leoAst)
    reload_t2 = time.process_time()
    return reload_t2 - reload_t1
</t>
<t tx="ekr.20191125235556.6">def run_tests():
    
    fails = []
    t1 = time.process_time()
    for contents, description in tests:
        # Test runner catches all exceptions.
        print(f"Running {description}...\n")
        x = leoAst.TestRunner()
        ok = x.run_tests(contents, description, filter_reports(description))
        if not ok:
            fails.append(description)
            ### print(f"\nFAIL: {description}")
    t2 = time.process_time()
    test_time = t2 - t1
    return fails, test_time
</t>
<t tx="ekr.20191125235556.7">def show_status():
    if 'show-test-kind' in flags:
        kind = '*all*' if 'all' in flags else '*selected'
        print(f"Running *{kind}*' unit tests...\n")
    if 'asttokens' in reports:
        print('\nUsing asttokens, *not* the TOG classes')
    if 'reload' in flags and 'show-reload-time' in flags:
        print(f"\n   Reload time: {reload_time:4.2f} sec.")
</t>
<t tx="ekr.20191125235556.8">def summarize():
    if 'summarize' in flags:
        status = 'FAIL' if fails else 'PASS'
        if fails:
            g.printObj(fails, tag='Failed tests')
        print(f"\n{status} Ran {len(tests)} test{g.plural(len(tests))}")
    if 'show-test-time' in flags:
        print(f"Run tests time: {test_time:4.2f} sec.")
</t>
<t tx="ekr.20191126002258.1"># Use 'no-sync' option to compare parse trees.

# Single f-strings.
'p1' ;
f'f1' ;
f'x1{e1}y1' ;
f'x2{e2+1}y2{e2+2}z2' ;

# Concatentated strings...
'p2', 'p3' ;
f'f2' 'f3' ;
f'x3{e3+1}y3' f'x4{e4+2}y4' ;
f'x5{e5+1}y5{e5+1}z5' f'x6{e6+1}y6{e6+1}z6' ;</t>
<t tx="ekr.20191126084046.1"></t>
<t tx="ekr.20191126143046.1">self.s = ''
self.i = 0
</t>
<t tx="ekr.20191127134236.1"></t>
<t tx="ekr.20191127134726.1"># leoFind.py: line 861
one(f"{'B'}" ": ")
</t>
<t tx="ekr.20191127172440.1">print("a\"b")</t>
<t tx="ekr.20191128013505.1"></t>
<t tx="ekr.20191128104415.1"></t>
<t tx="ekr.20191128141922.1">print(f"test {a}={2}")
print('done')</t>
<t tx="ekr.20191129033147.1"># Use 'no-sync' option to compare parse trees.

print('p1', 'p2')
print(f'f2' 'p3')
print(f'f3' f'f4')
print('end')
</t>
<t tx="ekr.20191129033245.1"># Use 'no-sync' option to compare parse trees.

print(f'x1{e1}y1', 'p1')
print(f'x2{e2}y2', f'f2')
print(f'x3{e3}y3', f'x4{e4}y4')

print(f"{'B'}" ":") # leoFind.py: line 861
print('end')</t>
<t tx="ekr.20191129033359.1"># Use 'no-sync' option to compare parse trees.

print(f'x1{e1}y1{e2}z1', 'p1')
print(f'x2{e3}y2{e3}z2', f'f2')
print(f'x3{e4}y3{e5}z3', f'x4{e6}y4{e7}z4')
print('end')</t>
<t tx="ekr.20191129035530.1">a = 'p1'
b = f'f1')
c = f'f2{e1}'
d = f'f3{e2}x{e3}'

print('p1')
print(f'f1')
print(f'f2{e1}')
print(f'f3{e2}x{e3}')
print('end')
</t>
<t tx="ekr.20191129042702.1">'p1' ;
f'f1' ;
'done' ;</t>
</tnodes>
</leo_file>
