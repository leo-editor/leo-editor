#@+leo-ver=5-thin
#@+node:ekr.20170302123956.1: * @file ../doc/leoAttic.txt
# This is Leo's final resting place for dead code.
# New in Leo 6.7.5. The attic will contain only code retired in the present release.

#@@language python
#@@killbeautify
#@+all
#@+node:ekr.20220917052540.1: ** retire read-outline-only
#@+node:ekr.20031218072017.2839: *3* c_file.readOutlineOnly
@g.commander_command('read-outline-only')
def readOutlineOnly(self: Self, event: Event = None) -> None:
    """Open a Leo outline, but do not read any derived files."""
    c = self
    c.endEditing()
    fileName = g.app.gui.runOpenFileDialog(c,
        title="Read Outline Only",
        filetypes=[("Leo files", "*.leo *.leojs *.db"), ("All files", "*")],
        defaultextension=".leo")
    if not fileName:
        return
    try:
        # pylint: disable=assignment-from-no-return
        # Can't use 'with" because readOutlineOnly closes the file.
        theFile = open(fileName, 'r')
        g.chdir(fileName)
        c = g.app.newCommander(fileName)
        frame = c.frame
        frame.deiconify()
        frame.lift()
        c.fileCommands.readOutlineOnly(theFile, fileName)  # closes file.
    except Exception:
        g.es("can not open:", fileName)
#@+node:ekr.20031218072017.3030: *3* fc.readOutlineOnly
def readOutlineOnly(self, theFile: Any, fileName: str) -> VNode:
    c = self.c
    # Set c.openDirectory
    theDir = g.os_path_dirname(fileName)
    if theDir:
        c.openDirectory = c.frame.openDirectory = theDir
    v, ratio = self.getLeoFile(theFile, fileName, readAtFileNodesFlag=False)
    c.redraw()
    c.frame.deiconify()
    junk, junk, secondary_ratio = self.frame.initialRatios()
    c.frame.resizePanesToRatio(ratio, secondary_ratio)
    return v
#@+node:ekr.20230913131425.1: ** retire ctext importer & writer
#@+node:tbrown.20140801105909.47549: *3* ../plugins/importers/ctext.py
from __future__ import annotations
import re
from typing import TYPE_CHECKING
from leo.core import leoGlobals as g  # Required
from leo.plugins.importers.base_importer import Importer

if TYPE_CHECKING:
    from leo.core.leoCommands import Commands as Cmdr
    from leo.core.leoNodes import Position, VNode

@others

def do_import(c: Cmdr, parent: Position, s: str) -> None:
    """The importer callback for ctext."""
    CText_Importer(c).import_from_string(parent, s)

importer_dict = {
    '@auto': ['@auto-ctext'],
    'extensions': ['.ctext'],  # A made-up extension for unit tests.
    'func': do_import,
}
@language python
@tabwidth -4
#@+node:tbrown.20140801105909.47551: *4* class CText_Importer(Importer)
class CText_Importer(Importer):
    << ctext docstring >>

    language = 'plain'  # A reasonable default.

    @others
#@+node:ekr.20161130053507.1: *5* << ctext docstring >>
"""
Read/Write simple text files with hierarchy embedded in headlines::

    Leading text in root node of subtree

    Etc. etc.

    ### A level one node #####################################

    This would be the text in this level one node.

    And this.

    ### Another level one node ###############################

    Another one

    #### A level 2 node ######################################

    See what we did there - one more '#' - this is a subnode.

Leading / trailing whitespace may not be preserved.  '-' and '/'
are used in place of '#' for SQL and JavaScript.

"""
#@+node:tbrown.20140801105909.47553: *5* ctext_i.import_from_string
def import_from_string(self, parent: Position, s: str) -> None:
    """CText_Importer.import_from_string."""
    c = self.c
    root = parent.copy()
    ft = c.importCommands.fileType.lower()
    cchar = (
        '#' if g.unitTesting else
        '-' if ft == '.sql' else
        '/' if ft == '.js' else '#'
    )
    header_pat = re.compile(fr"^\s*({cchar}{{3,}})(.*?){cchar}*\s*$")
    lines_dict: dict[VNode, list[str]] = {root.v: []}
    parents: list[Position] = [root]
    for line in g.splitLines(s):
        if m := header_pat.match(line):
            level = len(m.group(1)) - 2
            assert level >= 1, m.group(1)
            parents = parents[:level]
            self.create_placeholders(level, lines_dict, parents)
            parent = parents[-1]
            child = parent.insertAsLastChild()
            child.h = m.group(2).strip()
            lines_dict[child.v] = []
            parents.append(child)
        else:
            parent = parents[-1]
            lines_dict[parent.v].append(line)

    for p in root.self_and_subtree():
        p.b = ''.join(lines_dict[p.v])

    # Importers should dirty neither nodes nor the outline.
    for p in root.self_and_subtree():
        p.clearDirty()
#@+node:tbrown.20140804103545.29975: *3* ../plugins/writers/ctext.py
@language python
@tabwidth -4
from leo.core import leoGlobals as g  # Required.
from leo.core.leoNodes import Position
import leo.plugins.writers.basewriter as basewriter
@others
writer_dict = {
    '@auto': ['@auto-ctext',],
    'class': CTextWriter,
}
@language python
@tabwidth -4
#@+node:tbrown.20140804103545.29977: *4* class CTextWriter(BaseWriter)
class CTextWriter(basewriter.BaseWriter):
    @others
#@+node:tbrown.20140804103545.29978: *5* put_node
def put_node(self, p: Position, level: int = 0) -> None:
    self.put(p.b.strip() + '\n\n')
    for child in p.children():
        txt = self.cchar * 3 + self.cchar * level + ' ' + child.h.strip() + ' '
        txt += self.cchar * max(0, 75 - len(txt))
        self.put(txt + '\n\n')
        self.put_node(child, level + 1)
#@+node:tbrown.20140804103545.29979: *5* write
def write(self, root: Position) -> None:

    h = root.h.lower()
    self.cchar = (
        '#' if g.unitTesting else
        '%' if h.startswith('.tex') else
        '-' if h.startswith('.sql') else
        '/' if h.startswith('.js') else '#'
    )
    self.put_node(root, 0)
#@+node:ekr.20220812144913.1: *3* class TestCTextWriter(BaseTestWriter)
class TestCTextWriter (BaseTestWriter):
    """Test cases for the ctext writer plugin."""
    @others
#@+node:ekr.20220812144243.1: *4* TestCTextWriter.test_1
def test_1(self):

    c, root = self.c, self.c.p
    child = root.insertAsLastChild()
    child.h = 'h'
    x = CTextWriter(c)
    x.write(root)
#@+node:ekr.20230913144248.1: ** retire g.SherlockTracer
#@+node:ekr.20121128031949.12605: *3* class g.SherlockTracer
class SherlockTracer:
    """
    A stand-alone tracer class with many of Sherlock's features.

    This class should work in any environment containing the re, os and sys modules.

    The arguments in the pattern lists determine which functions get traced
    or which stats get printed. Each pattern starts with "+", "-", "+:" or
    "-:", followed by a regular expression::

    "+x"  Enables tracing (or stats) for all functions/methods whose name
          matches the regular expression x.
    "-x"  Disables tracing for functions/methods.
    "+:x" Enables tracing for all functions in the **file** whose name matches x.
    "-:x" Disables tracing for an entire file.

    Enabling and disabling depends on the order of arguments in the pattern
    list. Consider the arguments for the Rope trace::

    patterns=['+.*','+:.*',
        '-:.*\\lib\\.*','+:.*rope.*','-:.*leoGlobals.py',
        '-:.*worder.py','-:.*prefs.py','-:.*resources.py',])

    This enables tracing for everything, then disables tracing for all
    library modules, except for all rope modules. Finally, it disables the
    tracing for Rope's worder, prefs and resources modules.

    Being able to zero in on the code of interest can be a big help in
    studying other people's code. This is a non-invasive method: no tracing
    code needs to be inserted anywhere.

    Usage:

    g.SherlockTracer(patterns).run()
    """
    @others
#@+node:ekr.20121128031949.12602: *4* sherlock.__init__
def __init__(
    self,
    patterns: list[Any],
    indent: bool = True,
    show_args: bool = True,
    show_return: bool = True,
    verbose: bool = True,
) -> None:
    """SherlockTracer ctor."""
    self.bad_patterns: list[str] = []  # List of bad patterns.
    self.indent = indent  # True: indent calls and returns.
    self.contents_d: dict[str, list] = {}  # Keys are file names, values are file lines.
    self.n = 0  # The frame level on entry to run.
    self.stats: dict[str, dict] = {}  # Keys are full file names, values are dicts.
    self.patterns: list[Any] = None  # A list of regex patterns to match.
    self.pattern_stack: list[str] = []
    self.show_args = show_args  # True: show args for each function call.
    self.show_return = show_return  # True: show returns from each function.
    self.trace_lines = True  # True: trace lines in enabled functions.
    self.verbose = verbose  # True: print filename:func
    self.set_patterns(patterns)
    try:  # Don't assume g.app exists.
        from leo.core.leoQt import QtCore
        if QtCore:
            # pylint: disable=no-member
            QtCore.pyqtRemoveInputHook()
    except Exception:
        pass
#@+node:ekr.20140326100337.16844: *4* sherlock.__call__
def __call__(self, frame: Any, event: Any, arg: Any) -> Any:
    """Exists so that self.dispatch can return self."""
    return self.dispatch(frame, event, arg)
#@+node:ekr.20140326100337.16846: *4* sherlock.bad_pattern
def bad_pattern(self, pattern: Any) -> None:
    """Report a bad Sherlock pattern."""
    if pattern not in self.bad_patterns:
        self.bad_patterns.append(pattern)
        print(f"\nignoring bad pattern: {pattern}\n")
#@+node:ekr.20140326100337.16847: *4* sherlock.check_pattern
def check_pattern(self, pattern: str) -> bool:
    """Give an error and return False for an invalid pattern."""
    try:
        for prefix in ('+:', '-:', '+', '-'):
            if pattern.startswith(prefix):
                re.match(pattern[len(prefix) :], 'xyzzy')
                return True
        self.bad_pattern(pattern)
        return False
    except Exception:
        self.bad_pattern(pattern)
        return False
#@+node:ekr.20121128031949.12609: *4* sherlock.dispatch
def dispatch(self, frame: Any, event: Any, arg: Any) -> Any:
    """The dispatch method."""
    if event == 'call':
        self.do_call(frame, arg)
    elif event == 'return' and self.show_return:
        self.do_return(frame, arg)
    elif event == 'line' and self.trace_lines:
        self.do_line(frame, arg)
    # Queue the SherlockTracer instance again.
    return self
#@+node:ekr.20121128031949.12603: *4* sherlock.do_call & helper
def do_call(self, frame: Any, unused_arg: Any) -> None:
    """Trace through a function call."""
    frame1 = frame
    code = frame.f_code
    file_name = code.co_filename
    locals_ = frame.f_locals
    function_name = code.co_name
    try:
        full_name = self.get_full_name(locals_, function_name)
    except Exception:
        full_name = function_name
    if not self.is_enabled(file_name, full_name, self.patterns):
        # 2020/09/09: Don't touch, for example, __ methods.
        return
    n = 0  # The number of callers of this def.
    while frame:
        frame = frame.f_back
        n += 1
    indent = ' ' * max(0, n - self.n) if self.indent else ''
    path = f"{os.path.basename(file_name):>20}" if self.verbose else ''
    leadin = '+' if self.show_return else ''
    args_list = self.get_args(frame1)
    if self.show_args and args_list:
        args_s = ','.join(args_list)
        args_s2 = f"({args_s})"
        if len(args_s2) > 100:
            print(f"{path}:{indent}{leadin}{full_name}")
            g.printObj(args_list, indent=len(indent) + 22)
        else:
            print(f"{path}:{indent}{leadin}{full_name}{args_s2}")
    else:
        print(f"{path}:{indent}{leadin}{full_name}")
    # Always update stats.
    d = self.stats.get(file_name, {})
    d[full_name] = 1 + d.get(full_name, 0)
    self.stats[file_name] = d
#@+node:ekr.20130111185820.10194: *5* sherlock.get_args
def get_args(self, frame: Any) -> list[str]:
    """Return a list of string "name=val" for each arg in the function call."""
    code = frame.f_code
    locals_ = frame.f_locals
    name = code.co_name
    n = code.co_argcount
    if code.co_flags & 4:
        n = n + 1
    if code.co_flags & 8:
        n = n + 1
    result = []
    for i in range(n):
        name = code.co_varnames[i]
        if name != 'self':
            arg = locals_.get(name, '*undefined*')
            if arg:
                if isinstance(arg, (list, tuple)):
                    val_s = ','.join([self.show(z) for z in arg if self.show(z)])
                    val = f"[{val_s}]"
                elif isinstance(arg, str):
                    val = arg
                else:
                    val = self.show(arg)
                if val:
                    result.append(f"{name}={val}")
    return result
#@+node:ekr.20140402060647.16845: *4* sherlock.do_line (not used)
bad_fns: list[str] = []

def do_line(self, frame: Any, arg: Any) -> None:
    """print each line of enabled functions."""
    if 1:
        return
    code = frame.f_code
    file_name = code.co_filename
    locals_ = frame.f_locals
    name = code.co_name
    full_name = self.get_full_name(locals_, name)
    if not self.is_enabled(file_name, full_name, self.patterns):
        return
    n = frame.f_lineno - 1  # Apparently, the first line is line 1.
    d = self.contents_d
    lines = d.get(file_name)
    if not lines:
        print(file_name)
        try:
            with open(file_name) as f:
                s = f.read()
        except Exception:
            if file_name not in self.bad_fns:
                self.bad_fns.append(file_name)
                print(f"open({file_name}) failed")
            return
        lines = g.splitLines(s)
        d[file_name] = lines
    line = lines[n].rstrip() if n < len(lines) else '<EOF>'
    if 0:
        print(f"{name:3} {line}")
    else:
        print(f"{g.shortFileName(file_name)} {n} {full_name} {line}")
#@+node:ekr.20130109154743.10172: *4* sherlock.do_return & helper
def do_return(self, frame: Any, arg: Any) -> None:  # Arg *is* used below.
    """Trace a return statement."""
    code = frame.f_code
    fn = code.co_filename
    locals_ = frame.f_locals
    name = code.co_name
    self.full_name = self.get_full_name(locals_, name)
    if not self.is_enabled(fn, self.full_name, self.patterns):
        return
    n = 0
    while frame:
        frame = frame.f_back
        n += 1
    path = f"{os.path.basename(fn):>20}" if self.verbose else ''
    if name and name == '__init__':
        try:
            ret1 = locals_ and locals_.get('self', None)
            self.put_ret(ret1, n, path)
        except NameError:
            self.put_ret(f"<{ret1.__class__.__name__}>", n, path)
    else:
        self.put_ret(arg, n, path)
#@+node:ekr.20220605141445.1: *5* sherlock.put_ret
def put_ret(self, arg: Any, n: int, path: str) -> None:
    """Print arg, the value returned by a "return" statement."""
    indent = ' ' * max(0, n - self.n + 1) if self.indent else ''
    try:
        if isinstance(arg, types.GeneratorType):
            ret = '<generator>'
        elif isinstance(arg, (tuple, list)):
            ret_s = ','.join([self.show(z) for z in arg])
            if len(ret_s) > 40:
                g.printObj(arg, indent=len(indent))
                ret = ''
            else:
                ret = f"[{ret_s}]"
        elif arg:
            ret = self.show(arg)
            if len(ret) > 100:
                ret = f"\n    {ret}"
        else:
            ret = '' if arg is None else repr(arg)
        print(f"{path}:{indent}-{self.full_name} -> {ret}")
    except Exception:
        exctype, value = sys.exc_info()[:2]
        try:  # Be extra careful.
            arg_s = f"arg: {arg!r}"
        except Exception:
            arg_s = ''  # arg.__class__.__name__
        print(
            f"{path}:{indent}-{self.full_name} -> "
            f"{exctype.__name__}, {value} {arg_s}"
        )
#@+node:ekr.20121128111829.12185: *4* sherlock.fn_is_enabled
def fn_is_enabled(self, func: Any, patterns: list[str]) -> bool:
    """Return True if tracing for the given function is enabled."""
    if func in self.ignored_functions:
        return False

    def ignore_function() -> None:
        if func not in self.ignored_functions:
            self.ignored_functions.append(func)
            print(f"Ignore function: {func}")
    #
    # New in Leo 6.3. Never trace dangerous functions.
    table = (
        '_deepcopy.*',
        # Unicode primitives.
        'encode\b', 'decode\b',
        # System functions
        '.*__next\b',
        '<frozen>', '<genexpr>', '<listcomp>',
        # '<decorator-gen-.*>',
        'get\b',
        # String primitives.
        'append\b', 'split\b', 'join\b',
        # File primitives...
        'access_check\b', 'expanduser\b', 'exists\b', 'find_spec\b',
        'abspath\b', 'normcase\b', 'normpath\b', 'splitdrive\b',
    )
    g.trace('=====', func)
    for z in table:
        if re.match(z, func):
            ignore_function()
            return False
    #
    # Legacy code.
    try:
        enabled, pattern = False, None
        for pattern in patterns:
            if pattern.startswith('+:'):
                if re.match(pattern[2:], func):
                    enabled = True
            elif pattern.startswith('-:'):
                if re.match(pattern[2:], func):
                    enabled = False
        return enabled
    except Exception:
        self.bad_pattern(pattern)
        return False
#@+node:ekr.20130112093655.10195: *4* sherlock.get_full_name
def get_full_name(self, locals_: Any, name: str) -> str:
    """Return class_name::name if possible."""
    full_name = name
    try:
        user_self = locals_ and locals_.get('self', None)
        if user_self:
            full_name = user_self.__class__.__name__ + '::' + name
    except Exception:
        pass
    return full_name
#@+node:ekr.20121128111829.12183: *4* sherlock.is_enabled
ignored_files: list[str] = []  # List of files.
ignored_functions: list[str] = []  # List of files.

def is_enabled(
    self,
    file_name: str,
    function_name: str,
    patterns: list[str] = None,
) -> bool:
    """Return True if tracing for function_name in the given file is enabled."""
    #
    # New in Leo 6.3. Never trace through some files.
    if not os:
        return False  # Shutting down.
    base_name = os.path.basename(file_name)
    if base_name in self.ignored_files:
        return False

    def ignore_file() -> None:
        if base_name not in self.ignored_files:
            self.ignored_files.append(base_name)

    def ignore_function() -> None:
        if function_name not in self.ignored_functions:
            self.ignored_functions.append(function_name)

    if f"{os.sep}lib{os.sep}" in file_name:
        ignore_file()
        return False
    if base_name.startswith('<') and base_name.endswith('>'):
        ignore_file()
        return False
    #
    # New in Leo 6.3. Never trace dangerous functions.
    table = (
        '_deepcopy.*',
        # Unicode primitives.
        'encode\b', 'decode\b',
        # System functions
        '.*__next\b',
        '<frozen>', '<genexpr>', '<listcomp>',
        # '<decorator-gen-.*>',
        'get\b',
        # String primitives.
        'append\b', 'split\b', 'join\b',
        # File primitives...
        'access_check\b', 'expanduser\b', 'exists\b', 'find_spec\b',
        'abspath\b', 'normcase\b', 'normpath\b', 'splitdrive\b',
    )
    for z in table:
        if re.match(z, function_name):
            ignore_function()
            return False
    #
    # Legacy code.
    enabled = False
    if patterns is None:
        patterns = self.patterns
    for pattern in patterns:
        try:
            if pattern.startswith('+:'):
                if re.match(pattern[2:], file_name):
                    enabled = True
            elif pattern.startswith('-:'):
                if re.match(pattern[2:], file_name):
                    enabled = False
            elif pattern.startswith('+'):
                if re.match(pattern[1:], function_name):
                    enabled = True
            elif pattern.startswith('-'):
                if re.match(pattern[1:], function_name):
                    enabled = False
            else:
                self.bad_pattern(pattern)
        except Exception:
            self.bad_pattern(pattern)
    return enabled
#@+node:ekr.20121128111829.12182: *4* sherlock.print_stats
def print_stats(self, patterns: list[str] = None) -> None:
    """Print all accumulated statisitics."""
    print('\nSherlock statistics...')
    if not patterns:
        patterns = ['+.*', '+:.*',]
    for fn in sorted(self.stats.keys()):
        d = self.stats.get(fn)
        if self.fn_is_enabled(fn, patterns):
            result = sorted(d.keys())  # type:ignore
        else:
            result = [key for key in sorted(d.keys())  # type:ignore
                if self.is_enabled(fn, key, patterns)]
        if result:
            print('')
            fn = fn.replace('\\', '/')
            parts = fn.split('/')
            print('/'.join(parts[-2:]))
            for key in result:
                print(f"{d.get(key):4} {key}")
#@+node:ekr.20121128031949.12614: *4* sherlock.run
# Modified from pdb.Pdb.set_trace.

def run(self, frame: Any = None) -> None:
    """Trace from the given frame or the caller's frame."""
    print("SherlockTracer.run:patterns:\n%s" % '\n'.join(self.patterns))
    if frame is None:
        frame = sys._getframe().f_back
    # Compute self.n, the number of frames to ignore.
    self.n = 0
    while frame:
        frame = frame.f_back
        self.n += 1
    # Pass self to sys.settrace to give easy access to all methods.
    sys.settrace(self)
#@+node:ekr.20140322090829.16834: *4* sherlock.push & pop
def push(self, patterns: list[str]) -> None:
    """Push the old patterns and set the new."""
    self.pattern_stack.append(self.patterns)  # type:ignore
    self.set_patterns(patterns)
    print(f"SherlockTracer.push: {self.patterns}")

def pop(self) -> None:
    """Restore the pushed patterns."""
    if self.pattern_stack:
        self.patterns = self.pattern_stack.pop()  # type:ignore
        print(f"SherlockTracer.pop: {self.patterns}")
    else:
        print('SherlockTracer.pop: pattern stack underflow')
#@+node:ekr.20140326100337.16845: *4* sherlock.set_patterns
def set_patterns(self, patterns: list[str]) -> None:
    """Set the patterns in effect."""
    self.patterns = [z for z in patterns if self.check_pattern(z)]
#@+node:ekr.20140322090829.16831: *4* sherlock.show
def show(self, item: Any) -> str:
    """return the best representation of item."""
    if not item:
        return repr(item)
    if isinstance(item, dict):
        return 'dict'
    if isinstance(item, str):
        s = repr(item)
        if len(s) <= 20:
            return s
        return s[:17] + '...'
    s = repr(item)
    # A Hack for mypy:
    if s.startswith("<object object"):
        s = "_dummy"
    return s
#@+node:ekr.20121128093229.12616: *4* sherlock.stop
def stop(self) -> None:
    """Stop all tracing."""
    sys.settrace(None)
#@+node:ekr.20231009061250.1: ** retire all eval commands
# mod_scripting.py contained the EvalController class.

# mod_http.py (!) used the EvalController class.
#@+node:ekr.20231009082535.1: *3* --- from mod_scripting.py
#@+node:ekr.20231009062818.1: *4* docstring relating to eval commands
"""
Eval Commands
-------------

The mod_scripting plugin creates the following 5 eval* commands:

eval
----

Evaluates the selected text, if any, and remember the result in c.vs, a global namespace.
For example::

    a = 10

sets:

    c.vs['a'] = 10

This command prints the result of the last expression or assignment in the log pane
and select the next line of the body pane. Handy for executing line by line.

eval-last
---------

Inserts the result of the last eval in the body.
Suppose you have this text::

    The cat is 7 years, or 7*365 days old.

To replace 7*365 with 2555, do the following::

    select 7*367
    eval
    delete 7*365
    do eval-last

eval-replace
------------

Evaluates the expression and replaces it with the computed value.
For example, the example above can be done as follows::


    select 7*367
    eval-replace

eval-last-pretty
----------------

Like eval-last, but format with pprint.pformat.

eval-block
----------

Evaluates a series of blocks of code in the body, separated like this::

    # >>>
    code to run
    # <<<
    output of code
    # >>>
    code to run
    # <<<
    output of code
    ...

For example::

    import datetime
    datetime.datetime.now()
    # >>>
    2018-03-21 21:46:13.582835
    # <<<
    datetime.datetime.now()+datetime.timedelta(days=1000)
    # >>>
    2020-12-15 21:46:34.403814
    # <<<

eval-block inserts the separators, blocks can be re-run by placing the cursor in
them and doing eval-block, and the cursor is placed in the next block, so you
can go back up, change something, then quickly re-execute everything.
"""
#@+node:ekr.20180328085038.1: *4* class EvalController
class EvalController:
    """A class defining all eval-* commands."""
    @others
#@+node:ekr.20180328130835.1: *5* eval.Birth
def __init__(self, c: Cmdr) -> None:
    """Ctor for EvalController class."""
    self.answers: list[tuple[str, str]] = []
    self.c = c
    self.d: dict[str, Any] = {}
    self.globals_d: dict[str, Any] = {'c': c, 'g': g, 'p': c.p}
    self.locals_d: dict[str, Any] = {}
    self.legacy = c.config.getBool('legacy-eval', default=True)
    if g.app.ipk:
        # Use the IPython namespace.
        self.c.vs = g.app.ipk.namespace
    elif self.legacy:
        self.c.vs = self.d
    else:
        self.c.vs = self.globals_d
    # allow the auto-completer to complete in this namespace
    # Updated by do_exec.
    self.c.keyHandler.autoCompleter.namespaces.append(self.c.vs)
    self.last_result = None
    self.old_stderr: bool = None
    self.old_stdout: bool = None
#@+node:ekr.20180328092221.1: *5* eval.Commands
#@+node:ekr.20180328085426.2: *6* eval
@eval_cmd("eval")
def eval_command(self, event: Event) -> None:
    << eval docstring >>
    c = self.c
    if c == event.get('c'):
        s = self.get_selected_lines()
        if self.legacy and s is None:
            return
        # Updates self.last_answer if there is exactly one answer.
        self.eval_text(s)
#@+node:ekr.20180328100519.1: *7* << eval docstring >>
"""
Execute the selected text, if any, or the line containing the cursor.

Select next line of text.

Tries hard to capture the result of from the last expression in the
selected text::

    import datetime
    today = datetime.date.today()

will capture the value of ``today`` even though the last line is a
statement, not an expression.

Stores results in ``c.vs['_last']`` for insertion
into body by ``eval-last`` or ``eval-last-pretty``.

Removes common indentation (``textwrap.dedent()``) before executing,
allowing execution of indented code.

``g``, ``c``, and ``p`` are available to executing code, assignments
are made in the ``c.vs`` namespace and persist for the life of ``c``.
"""
#@+node:ekr.20180328085426.3: *6* eval-block
@eval_cmd("eval-block")
def eval_block(self, event: Event) -> None:
    << eval-block docstring >>
    c = self.c
    if c != event.get('c'):
        return
    pos = 0
    lines: list[str] = []
    current_seen = False
    current: bool
    source: str
    output: str
    for current, source, output in self.get_blocks():
        lines.append(source)
        lines.append("# >>>" + (" *" if current_seen else ""))
        if current:
            old_log = c.frame.log.logCtrl.getAllText()
            self.eval_text(source)
            new_log = c.frame.log.logCtrl.getAllText()[len(old_log) :]
            lines.append(new_log.strip())
            if not self.legacy:
                if self.last_result:
                    lines.append(self.last_result)
            pos = len('\n'.join(lines)) + 7
            current_seen = True
        else:
            lines.append(output)
        lines.append("# <<<")
    c.p.b = '\n'.join(lines) + '\n'
    c.frame.body.wrapper.setInsertPoint(pos)
    c.redraw()
    c.bodyWantsFocusNow()
#@+node:ekr.20180328100415.1: *7* << eval-block docstring >>
"""
In the body, "# >>>" marks the end of a code block, and "# <<<" marks
the end of an output block.  E.g.::

a = 2
# >>>
4
# <<<
b = 2.0*a
# >>>
4.0
# <<<

``eval-block`` evaluates the current code block, either the code block
the cursor's in, or the code block preceding the output block the cursor's
in.  Subsequent output blocks are marked "# >>> *" to show they may need
re-evaluation.

Note: you don't really need to type the "# >>>" and "# <<<" markers
because ``eval-block`` will add them as needed.  So just type the
first code block and run ``eval-block``.

"""
#@+node:ekr.20180328085426.5: *6* eval-last
@eval_cmd("eval-last")
def eval_last(self, event: Event, text: str=None) -> None:
    """
    Insert the last result from ``eval``.

    Inserted as a string, so ``"1\n2\n3\n4"`` will cover four lines and
    insert no quotes, for ``repr()`` style insertion use ``last-pretty``.
    """
    c = self.c
    if c != event.get('c'):
        return
    if self.legacy:
        text = str(c.vs.get('_last'))
    else:
        if not text and not self.last_result:
            return
        if not text:
            text = str(self.last_result)
    w = c.frame.body.wrapper
    i = w.getInsertPoint()
    w.insert(i, text + '\n')
    w.setInsertPoint(i + len(text) + 1)
    c.setChanged()
#@+node:ekr.20180328085426.6: *6* eval-last-pretty
@eval_cmd("eval-last-pretty")
def vs_last_pretty(self, event: Event) -> None:
    """
    Insert the last result from ``eval``.

    Formatted by ``pprint.pformat()``, so ``"1\n2\n3\n4"`` will appear as
    '``"1\n2\n3\n4"``', see all ``last``.
    """
    c = self.c
    if c != event.get('c'):
        return
    if self.legacy:
        text = str(c.vs.get('_last'))
    else:
        text = self.last_result
    if text:
        text = pprint.pformat(text)
        self.eval_last(event, text=text)
#@+node:ekr.20180328085426.4: *6* eval-replace
@eval_cmd("eval-replace")
def eval_replace(self, event: Event) -> None:
    """
    Execute the selected text, if any.
    Undoably replace it with the result.
    """
    c = self.c
    if c != event.get('c'):
        return
    w = c.frame.body.wrapper
    s = w.getSelectedText()
    if not s.strip():
        g.es_print('no selected text')
        return
    self.eval_text(s)
    if self.legacy:
        last = c.vs.get('_last')
    else:
        last = self.last_result
    if not last:
        return
    s = pprint.pformat(last)
    i, j = w.getSelectionRange()
    new_text = c.p.b[:i] + s + c.p.b[j:]
    bunch = c.undoer.beforeChangeNodeContents(c.p)
    w.setAllText(new_text)
    c.p.b = new_text
    w.setInsertPoint(i + len(s))
    c.undoer.afterChangeNodeContents(c.p, 'Insert result', bunch)
    c.setChanged()
#@+node:ekr.20180328151652.1: *5* eval.Helpers
#@+node:ekr.20180328090830.1: *6* eval.eval_text & helpers
def eval_text(self, s: str) -> Optional[str]:
    """Evaluate string s."""
    s = textwrap.dedent(s)
    if not s.strip():
        return None
    self.redirect()
    if self.legacy:
        blocks = re.split('\n(?=[^\\s])', s)
        ans = self.old_exec(blocks, s)
        self.show_legacy_answer(ans, blocks)
        return ans  # needed by mod_http
    self.new_exec(s)
    self.show_answers()
    self.unredirect()
    return None
#@+node:ekr.20180329130626.1: *7* eval.new_exec
def new_exec(self, s: str) -> None:
    try:
        self.answers = []
        self.locals_d = {}
        exec(s, self.globals_d, self.locals_d)
        for key in self.locals_d:
            val = self.locals_d.get(key)
            self.globals_d[key] = val
            self.answers.append((key, val))
        if len(self.answers) == 1:
            key, val = self.answers[0]
            self.last_result = val
        else:
            self.last_result = None
    except Exception:
        g.es_exception()
#@+node:ekr.20180329130623.1: *7* eval.old_exec
def old_exec(self, blocks: list[str], txt: str) -> str:

    # pylint: disable=eval-used
    c = self.c
    leo_globals = {'c': c, 'g': g, 'p': c.p}
    all_done, ans = False, None
    try:
        # Execute all but the last 'block'
        exec('\n'.join(blocks[:-1]), leo_globals, c.vs)  # Compatible with Python 3.x.
        all_done = False
    except SyntaxError:
        # Splitting the last block caused syntax error
        try:
            # Is the whole thing a single expression?
            ans = eval(txt, leo_globals, c.vs)
        except SyntaxError:
            try:
                exec(txt, leo_globals, c.vs)
            except Exception:
                g.es_exception()
        all_done = True  # Either way, the last block will be used.
    if not all_done:  # last block still needs using
        try:
            ans = eval(blocks[-1], leo_globals, c.vs)
        except SyntaxError:
            try:
                exec(txt, leo_globals, c.vs)
            except Exception:
                g.es_exception()
    return ans
#@+node:ekr.20180328130526.1: *7* eval.redirect & unredirect
def redirect(self) -> None:
    c = self.c
    if c.config.getBool('eval-redirect'):
        self.old_stderr = g.stdErrIsRedirected()
        self.old_stdout = g.stdOutIsRedirected()
        if not self.old_stderr:
            g.redirectStderr()
        if not self.old_stdout:
            g.redirectStdout()

def unredirect(self) -> None:
    c = self.c
    if c.config.getBool('eval-redirect'):
        if not self.old_stderr:
            g.restoreStderr()
        if not self.old_stdout:
            g.restoreStdout()
#@+node:ekr.20180328132748.1: *7* eval.show_answers
def show_answers(self) -> None:
    """ Show all new values computed by do_exec."""
    if len(self.answers) > 1:
        g.es('')
    for answer in self.answers:
        key, val = answer
        g.es(f"{key} = {val}")
#@+node:ekr.20180329154232.1: *7* eval.show_legacy_answer
def show_legacy_answer(self, ans: str, blocks: list[str]) -> str:

    cvs = self.c.vs
    if ans is None:  # see if last block was a simple "var =" assignment
        key = blocks[-1].split('=', 1)[0].strip()
        if key in cvs:
            ans = cvs[key]
    if ans is None:  # see if whole text was a simple /multi-line/ "var =" assignment
        key = blocks[0].split('=', 1)[0].strip()
        if key in cvs:
            ans = cvs[key]
    cvs['_last'] = ans
    if ans is not None:
        # annoying to echo 'None' to the log during line by line execution
        txt = str(ans)
        lines = txt.split('\n')
        if len(lines) > 10:
            txt = '\n'.join(lines[:5] + ['<snip>'] + lines[-5:])
        if len(txt) > 500:
            txt = txt[:500] + ' <truncated>'
        g.es(txt)
    return ans
#@+node:ekr.20180329125626.1: *6* eval.exec_then_eval (not used yet)
def exec_then_eval(self, code: str, ns: dict) -> str:
    # From Milan Melena.
    import ast
    block = ast.parse(code, mode='exec')
    if block.body and isinstance(block.body[-1], ast.Expr):
        last = ast.Expression(block.body.pop().value)
        exec(compile(block, '<string>', mode='exec'), ns)
        # pylint: disable=eval-used
        return eval(compile(last, '<string>', mode='eval'), ns)
    exec(compile(block, '<string>', mode='exec'), ns)
    return ""
#@+node:tbrown.20170516194332.1: *6* eval.get_blocks
def get_blocks(self) -> Generator:
    """get_blocks - iterate code blocks

    :return: (current, source, output)
    :rtype: (bool, str, str)
    """
    c = self.c
    pos = c.frame.body.wrapper.getInsertPoint()
    chrs = 0
    lines = c.p.b.split('\n')
    block: dict[str, list] = {'source': [], 'output': []}
    reading = 'source'
    seeking_current = True
    # if the last non-blank line isn't the end of a possibly empty
    # output block, make it one
    if [i for i in lines if i.strip()][-1] != "# <<<":
        lines.append("# <<<")
    while lines:
        line = lines.pop(0)
        chrs += len(line) + 1
        if line.startswith("# >>>"):
            reading = 'output'
            continue
        if line.startswith("# <<<"):
            current = seeking_current and (chrs >= pos + 1)
            if current:
                seeking_current = False
            yield current, '\n'.join(block['source']), '\n'.join(block['output'])
            block = {'source': [], 'output': []}
            reading = 'source'
            continue
        block[reading].append(line)
#@+node:ekr.20180328145035.1: *6* eval.get_selected_lines
def get_selected_lines(self) -> str:

    c, p = self.c, self.c.p
    w = c.frame.body.wrapper
    body = w.getAllText()
    i = w.getInsertPoint()
    if w.hasSelection():
        if self.legacy:
            i1, i2 = w.getSelectionRange()
        else:
            j, k = w.getSelectionRange()
            i1, junk = g.getLine(body, j)
            junk, i2 = g.getLine(body, k)
        s = body[i1:i2]
    else:
        if self.legacy:
            k = w.getInsertPoint()
            junk, i2 = g.getLine(body, k)
            w.setSelectionRange(k, i2)
            return None
        i1, i2 = g.getLine(body, i)
        s = body[i1:i2].strip()
    # Select next line for next eval.
    if self.legacy:
        i = j = i2
        j += 1
        while j < len(body) and body[j] != '\n':
            j += 1
        w.setSelectionRange(i, j)
    else:
        if not body.endswith('\n'):
            if i >= len(p.b):
                i2 += 1
            p.b = p.b + '\n'
        ins = min(len(p.b), i2)
        w.setSelectionRange(i1, ins, insert=ins, s=p.b)
    return s
#@+node:ekr.20231009082307.15: *3* --- from mod_http.py
#@+node:ekr.20231009082307.16: *4* class ExecHandler
class ExecHandler:
    """
    Quasi-RPC GET based interface
    """
    @others
#@+node:ekr.20231009082307.17: *5* __init__
def __init__(self, request_handler):
    self.request_handler = request_handler
#@+node:ekr.20231009082307.18: *5* get_response
def get_response(self):
    """Return the file like 'f' that leo_interface.send_head makes"""
    # self.request_handler.path.startswith('/_/exec/')

    if not g.app.config.getBool("http-allow-remote-exec"):
        return None  # fail deliberately

    c = g.app and g.app.log and g.app.log.c
    if c and config.enable is None:
        if c.config.isLocalSetting('http-allow-remote-exec', 'bool'):
            g.issueSecurityWarning('@bool http-allow-remote-exec')
            config.enable = False
        else:
            config.enable = True

    parsed_url = urlparse.urlparse(self.request_handler.path)
    query = urlparse.parse_qs(parsed_url.query)

    enc = query.get("enc", ["str"])[0]

    if parsed_url.path.startswith('/_/exec/commanders/'):
        ans = [i.fileName() for i in g.app.commanders()]
        if enc != 'json':
            ans = '\n'.join(ans)  # type:ignore
    else:
        ans = self.proc_cmds()

    f = StringIO()
    f.mime_type = query.get("mime_type", ["text/plain"])[0]
    enc = query.get("enc", ["str"])[0]
    if enc == 'json':
        f.write(json.dumps(ans))
    elif enc == 'repr':
        f.write(repr(ans))
    else:
        f.write(str(ans))
    return f

#@+node:ekr.20231009082307.19: *5* proc_cmds (mod_http.py)
def proc_cmds(self):

    parsed_url = urlparse.urlparse(self.request_handler.path)
    query = urlparse.parse_qs(parsed_url.query)
    # work out which commander to use, zero index int, full path name, or file name
    c_idx = query.get('c', [0])[0]
    # pylint: disable=literal-comparison
    if c_idx != 0:
        try:
            c_idx = int(c_idx)
        except ValueError:
            paths = [i.fileName() for i in g.app.commanders()]
            if c_idx in paths:
                c_idx = paths.index(c_idx)
            else:
                paths = [os.path.basename(i) for i in paths]
                c_idx = paths.index(c_idx)
    ans = None
    c = g.app.commanders()[c_idx]
    if c and c.evalController:
        for cmd in query['cmd']:
            ans = c.evalController.eval_text(cmd)
    return ans  # the last answer, if multiple commands run
#@+node:ekr.20240206153344.1: ** retire setup.py & setup.cfg
#@+node:ekr.20221129164042.1: *3* setup.cfg
@language config
[bdist_wheel]

# Leo only supports Python 3, so universal should be zero.
universal=0

[flake8]

exclude =
    .git,
    __pycache__,
    leo/build,
    leo/dist,
    leo/doc,
    leo/extensions,
    leo/external,
    leo/modes,
    leo/plugins,
    leo/scripts,
    leo/test,
    leo/unittests,

extend-ignore =

    # Don't check B020. It conflicts with Leo idioms like `for p in p.children():
    # Found for loop that reassigns the iterable it is iterating with each iterable value.
    B020
    
    # Don't complain on every save. Let reindent handle this later.
    # blank line contains whitespace
    W293

    # Harmless...
    
    # expected an indented block (comment)
    E115

    # unexpected indentation (comment)
    E116
    
    # over-indented (comment)
    E117
    
    # continuation line over-indented for visual indent
    E127
    
    # continuation line missing indentation or outdented.
    E122
    
    # closing bracket does not match visual indentation.
    E124
    
    # continuation line with same indent as next logical line
    E125
    
    # continuation line under-indented for visual indent.
    E128
    
    # visually indented line with same indent as next logical line
    E129
    
    # continuation line unaligned for hanging indent.
    E131

    # whitespace before '['
    E211
    
    # multiple spaces before operator
    E221
    
    # multiple spaces after operator
    E222
    
    # missing whitespace around operator
    E225
    
    # missing whitespace after ','
    E231
    
    # unexpected spaces around keyword / parameter equals
    E251
    
    # at least two spaces before inline comment
    E261
    
    # missing whitespace around parameter equals.
    E252
    
    # inline comment should start with '# '
    E262
    
    # block comment should start with '# '.
    E265
    
    # too many leading '#' for block comment
    E266
    
    # multiple spaces before keyword
    E272
    
    # missing whitespace after keyword.
    E275

    # expected 1 blank line, found 0.
    E301
    
    # expected 2 blank lines, found 1.
    E302
    
    # whitespace before ':'
    E203
    
    # too many blank lines (2)
    E303
    
    # expected 2 blank lines after class or function definition, found 0.
    E305
    
    # expected 1 blank line before a nested definition, found 0.
    E306

    # Line too long.
    E501
#@+node:ekr.20240206153315.1: *3* setup.py
"""
setup.py for leo.

Run with `python -m build` from the leo-editor directory.
"""
<< imports >>
<< define classifiers >>
<< define install_requires >>
@others  # Define helpers
production = True
testing = False
# Dashes are not allowed.
version = '6.7.7'  ##version Should match version in leoVersion.py
entry_points = define_entry_points()
long_description = get_readme_contents()
assert is_valid_version(version), version
if testing:
    dump_entry_points()
    test_is_valid_version()
if production:
    setuptools.setup(
        name='leo',  # Add username?.
        version=version,
        author='Edward K. Ream',
        author_email='edreamleo@gmail.com',
        url='https://leo-editor.github.io/leo-editor',
        license='MIT License',
        description='An IDE, PIM and Outliner',  # becomes 'Summary' in pkg-info
        long_description=long_description,
        long_description_content_type="text/markdown",  # PEP566
        platforms=['Linux', 'Windows', 'MacOS'],
        download_url='https://leo-editor.github.io/leo-editor/download.html',
        classifiers=classifiers,
        packages=setuptools.find_packages(),
        include_package_data=True,  # also include MANIFEST files in wheels
        setup_requires=[],  # No longer needed with PEP-518 and pip >v10.
        install_requires=install_requires,
        entry_points=entry_points,
        python_requires='>=3.9',
    )
print('setup.py: done')
@language python
@tabwidth -4
#@+node:ekr.20240206153315.2: *4* << imports >>
import platform
import re
import sys
import traceback
# Third-part tools.
import setuptools  # Prefer setuptools over distutils.
#@+node:ekr.20240206153315.3: *4* << define classifiers >>
classifiers = [
    'Development Status :: 6 - Mature',
    'Intended Audience :: End Users/Desktop',
    'Intended Audience :: Developers',
    'License :: OSI Approved :: MIT License',
    'Natural Language :: English',
    'Operating System :: MacOS',
    'Operating System :: Microsoft :: Windows',
    'Operating System :: POSIX :: Linux',
    'Programming Language :: Python :: 3 :: Only',
    'Topic :: Software Development',
    'Topic :: Text Editors',
    'Topic :: Text Processing',
]
#@+node:ekr.20240206153315.4: *4* << define install_requires >>
install_requires = [
    
    'PyQt5 >= 5.15',  # #2884: require v5.15. #1217: require v5.12+.
    'PyQtWebEngine',
    'build >= 0.6.0',  # simple PEP 517 package builder
    'docutils',  # used by Sphinx, rST plugin
    'flexx',  # for LeoWapp browser gui
    'meta',  # for livecode.py plugin, which is enabled by default
    'nbformat',  # for Jupyter notebook integration
    'pylint', 'pyflakes',
    'pyenchant',  # The spell tab.
    'pyshortcuts >= 1.7',  # desktop integration (#1243)
    'sphinx',  # rST plugin
    'tk',  # tkinter.

    # For leoAst.py and leoTokens.py.
    'asttokens',  # abstract syntax tree text parsing
    'black',  # coding syntax standards

    # #3603: windows-curses doesn't work with Python 3.12.
    # This issue has now been fixed.
    'windows-curses; platform_system=="Windows"',  # for console mode on Windows
]
#@+node:ekr.20240206153315.5: *4* define_entry_points
def define_entry_points(entry_points=None):
    """
    Return a dict defining scripts that get installed to PYTHONHOME/Scripts.
    """
    entry_points = {
        'console_scripts': [
            'leo-c = leo.core.runLeo:run_console',
            'leo-console = leo.core.runLeo:run_console',
        ],
        'gui_scripts': ['leo = leo.core.runLeo:run'],
    }
    # Add leo-messages wrapper for windows platform
    if platform.system() == 'Windows':
        x = entry_points['console_scripts']
        x.append('leo-m = leo.core.runLeo:run')
        x.append('leo-messages = leo.core.runLeo:run')
        entry_points.update({'console_scripts': x})
    return entry_points
#@+node:ekr.20240206153315.6: *4* dump_entry_points
def dump_entry_points():
    """Dump the entry_points list."""
    global entry_points
    print(f"setup.py: entry_points. OS = {platform.os.name}, system = {platform.system()}")
    for key in entry_points:
        aList = entry_points.get(key)
        list_s = '\n    '.join(aList)
        print(f"  {key}: [\n    {list_s}\n  ]")
#@+node:ekr.20240206153315.7: *4* get_readme_contents
def get_readme_contents():
    """Return the contents of README.md."""
    with open('README.md') as f:
        return f.read()
#@+node:ekr.20240206153315.8: *4* is_valid_version
pattern = re.compile(r'[0-9]+\.[0-9]+(\.[0-9]+)?((b|rc)[0-9]+)?')

def is_valid_version(version):
    """"
    Return True if version has the form '5.7b2', 'v4.3', etc.

    See PEP 440: https://www.python.org/dev/peps/pep-0440/

    For Leo, valid versions shall have the form: `N1.N2(.N3)?(bN|rcN)?`
    where N is any integer.

    In particular, neither alternative spellings nor alpha releases are valid.
    """
    m = pattern.match(version)
    return bool(m and len(m.group(0)) == len(version))
#@+node:ekr.20240206153315.9: *4* print_exception
def print_exception():
    """Like g.es_exception."""
    typ, val, tb = sys.exc_info()
    # val is the second argument to the raise statement.
    lines = traceback.format_exception(typ, val, tb)
    for line in lines:
        print(line)
#@+node:ekr.20240206153315.10: *4* test is_valid_version
def test_is_valid_version():
    """
    A unit test for is_valid_version.

    However, `python -m setup` won't work :-)
    """
    table = (
        '1.2', '3.4.5', '6.7b1', '8.9rc3',  # good.
        'v1.2', '3.4a1', '5.6-b1',  # bad
    )
    for s in table:
        ok = is_valid_version(s)
        print(f"{ok!s:5} {s}")
#@+node:ekr.20210901140645.22: *3* TestSyntax.test_syntax_of_setup_py
def test_syntax_of_setup_py(self):
    fn = g.finalize_join(g.app.loadDir, '..', '..', 'setup.py')
    # Only run this test if setup.py exists: it may not in the actual distribution.
    if not g.os_path_exists(fn):
        self.skipTest('setup.py not found')  # pragma: no cover
    s, e = g.readFileIntoString(fn)
    assert self.check_syntax(fn, s)
#@+node:ekr.20240129094501.1: ** unused from leoTokens.py
#@+node:ekr.20240105145241.41: *3* tbo: Parser
# Parsers pre-scan input tokens, adding context to tokens that need help.
#@+node:ekr.20240106181128.1: *4* tbo.parse_annotation
def parse_annotation(self, i1: int, end: int) -> int:
    """Parse the annotation of a function definition arg."""

    # Scan the ':'
    self.expect_op(i1, ':')
    self.set_context(i1, 'annotation')
    i = self.next(i1)

    # Scan to the next ',' or '=', ignoring inner parens.
    i3 = self.find_delim(i, end, [',', '=', ')'])
    self.expect_ops(i3, [',', '=', ')'])

    # Set the contexts of inner ops.
    for i4 in range(i1 + 1, i3 - 1):
        if self.is_ops(i4, ['=', ':']):
            self.set_context(i4, 'annotation')
    return i3
#@+node:ekr.20240106173638.1: *4* tbo.parse_arg
def parse_arg(self, i1: int, end: int) -> int:
    """Parse a single function definition argument."""

    # Scan optional  * and ** operators.
    i = i1
    if self.is_ops(i, ['*', '**']):
        self.set_context(i1, 'arg')
        i = self.next(i)
        # Handle *,
        if self.is_op(i, ','):
            i = self.next(i)
            return i

    # Scan the argument's name.
    self.expect(i, 'name')
    i = self.next(i)

    # Scan an optional annotation.
    has_annotation = self.is_op(i, ':')
    if has_annotation:
        self.set_context(i, 'annotation')
        i = self.parse_annotation(i, end)

    # Scan an optional initializer.
    if self.is_op(i, '='):
        if has_annotation:
            self.set_context(i, 'annotation')
        else:
            self.set_context(i, 'initializer')
        i = self.parse_initializer(i, end)  ###, has_annotation=has_annotation)

    # Scan the optional comma.
    if self.is_op(i, ','):
        i = self.next(i)
    return i
#@+node:ekr.20240106172905.1: *4* tbo.parse_args (changed sanity check)
def parse_args(self, i1: int, end: int) -> int:
    """
    Parse a comma-separated list of function definition arguments.

    Return the index of ending ')' token.
    """

    # Sanity checks.
    assert i1 < end, (i1, end)
    self.expect_op(i1, '(')
    self.expect_op(end, ')')

    # Scan the '('
    i = self.next(i1)

    # Scan each argument.
    while i < end:
        progress = i
        if self.is_op(i, ')'):
            break
        i = self.parse_arg(i, end)
        if self.is_op(i, ')'):
            break
        if progress >= i:  # pragma: no cover
            self.oops('parse_args: no progress')

    # Scan the ')'
    self.expect_op(i, ')')

    # An important sanity check.
    ### assert i == end, repr((i, end))  ### Test of error recovery.
    assert i <= end, repr((i, end))  ### Experimental Was '=='
    return i
#@+node:ekr.20240107092559.1: *4* tbo.parse_call_arg
def parse_call_arg(self, i1: int, end: int) -> int:
    """
    Scan a single function definition argument.

    Set context for every '=' operator.
    """

    trace = False  # Leave for now.
    if trace and False:
        print('')
        self.trace(i1, tag='before')

    # Handle leading * and ** args.
    if self.is_ops(i1, ['*', '**']):
        self.set_context(i1, 'arg')
        i = self.next(i1)
    else:
        i = i1

    # Step one: scan one argument up to a possible initializer.
    while i < end:
        progress = i
        token = self.tokens[i]
        kind, value = token.kind, token.value

        if kind == 'name':
            _is_complex, i = self.parse_name(i, end)
        elif kind == 'op':
            if value in ',=)':
                break
            i = self.parse_op(i, end)
        else:
            i = self.next(i)

        assert i is not None, (token)
        assert progress < i, (i, token)

    if trace:
        print('')
        self.trace(i, tag='after')

    # Step 2. Handle the initializer if present.
    if self.is_op(i, ','):
        i = self.next(i)
    elif self.is_op(i, '='):
        # Handle the initializer, setting context for '='
        self.set_context(i, 'initializer')
        i = self.parse_initializer(i, end)
        self.expect_ops(i, [',', ')'])
        if self.is_op(i, ','):
            i = self.next(i)

    return i
#@+node:ekr.20240107092458.1: *4* tbo.parse_call_args
def parse_call_args(self, i1: int, end: int) -> int:
    """Scan a comma-separated list of function definition arguments."""

    # Scan the '('
    self.expect_op(i1, '(')

    # Find the matching '('.
    i2 = self.find_close_paren(i1)  # Does not skip the ')'.
    self.expect_op(i2, ')')

    # Scan each argument.
    i = i1 + 1
    while i < i2:
        progress = i
        if self.is_op(i, ')'):
            break
        i = self.parse_call_arg(i, i2)  # Sets context.
        if self.is_op(i, ')'):
            break
        if progress >= i:  # pragma: no cover
            self.oops('parse_call_args: no progress')

    return i2  # Do not skip the ')'.
#@+node:ekr.20240124012707.1: *4* tbo.parse_name
def parse_name(self, i: int, end: int) -> tuple[bool, int]:
    """
    Parse a name, including possible function calls.

    Return (is_complex, i)
    """

    # self.trace(i, end)  ###

    self.expect_name(i)

    is_complex = False
    token = self.tokens[i]
    if token.value in self.expression_keywords:
        i = self.next(i)
    else:
        # token.value *can* be in self.keywords. For example, re.match.
        i = self.next(i)
        token = self.tokens[i]
        if i < end:
            if token.value == '(':
                i = self.parse_call(i, end)
                is_complex = True
            elif token.value == '[':
                i = self.parse_slice(i, end)
                is_complex = True
    return is_complex, i
#@+node:ekr.20240124012746.1: *4* tbo.parse_op
def parse_op(self, i: int, end: int) -> int:
    """
    Parse an operator, including grouping operators.

    Return the index of the *following* token.

    """
    if self.is_op(i, '['):
        i = self.parse_slice(i, end)
    elif self.is_op(i, '{'):
        i = self.parse_dict_or_set(i, end)
    elif self.is_op(i, '('):
        i = self.parse_parenthesized_expr(i, end)
    else:
        i = self.next(i)
    return i
#@+node:ekr.20240113054641.1: *4* tbo.parse_statement & statement helpers
def parse_statement(self, i: int) -> int:
    """
    Scan the next statement, including docstrings.
    """
    # All statements add 'end-statement' context "near" the end of the line:
    # - Simple statements call find_end_of_line.
    # - Compound statements call find_delim(i, len(self.tokens), [':'])
    token = self.tokens[i]
    if token.kind == 'name':
        if token.value == 'from':
            return self.parse_from(i)
        if token.value == 'import':
            return self.parse_import(i)
        if token.value in self.compound_statements:
            return self.parse_compound_statement(i)
        if token.value in self.simple_statements:
            return self.parse_simple_statement(i)
        # An expression.
        return self.parse_outer_expression(i)
    if (token.kind, token.value) == ('op', '@'):
        return self.parse_decorator(i)
    # Ensure progress.
    i = self.next(i)
    return i
#@+node:ekr.20240107091700.1: *5* tbo.parse_call
def parse_call(self, i1: int, end: int) -> int:
    """Parse a function call"""

    if 0:  # trace.
        print('')
        self.trace(i1, end)

    # Find i1 and i2, the boundaries of the argument list.
    self.expect_op(i1, '(')

    # Scan the arguments.
    i = self.parse_call_args(i1, end)

    # Sanity check.
    self.expect_op(i, ')')
    i = self.next(i)
    return i
#@+node:ekr.20240115101846.1: *5* tbo.parse_class_or_def
def parse_class_or_def(self, i: int) -> int:
    """Set context for a class or def statement."""
    self.expect(i, 'name')  # The 'class' or 'def' keyword.
    token = self.tokens[i]
    keyword_value = token.value
    assert keyword_value in ('class', 'def'), f"Expecting 'class' or 'def'. Got {token!r}"

    # Scan the define name of the 'class' or 'def'.
    i = self.next(i)
    self.expect(i, 'name')

    # The defined name is *not* a function call.
    self.set_context(i, 'class/def')

    # Caller will handle the rest of the 'class' statement.
    if keyword_value == 'class':
        # Find the trailing ':'!
        i = self.find_delim(i, len(self.tokens), [':'])
        self.expect_op(i, ':')

        # Set the context.
        self.set_context(i, 'end-statement')

        # Move past the ':' token.
        return self.next(i)

    # Handle the rest of the 'def' statement.

    # Find the '(' and ')' tokens.
    i = self.next(i)
    self.expect_op(i, '(')
    i1 = i
    i2 = self.find_close_paren(i)  # Does not skip the ')'.
    self.expect_op(i2, ')')

    # Scan the arguments, setting context.
    i = self.parse_args(i1, i2)

    # Set the context of the trailing ':' token.
    i = self.find_delim(i, len(self.tokens), [':'])
    self.expect_op(i, ':')
    self.set_context(i, 'end-statement')

    # Move past the ':' token.
    return self.next(i)
#@+node:ekr.20240108062349.1: *5* tbo.parse_compound_statement
def parse_compound_statement(self, i: int) -> int:
    """
    Scan a compound statement, adding 'end-statement' context to the
    trailing ':' token.
    """

    # Scan the keyword.
    self.expect(i, 'name')

    # Special case for 'async':
    keyword = self.tokens[i].value
    if keyword not in self.compound_statements:  # pragma: no cover
        self.oops(f"Not a compound keyword: {keyword!r}")

    if keyword == 'async':
        i = self.next(i)
        keyword = self.tokens[i].value

    if keyword in ('class', 'def'):
        return self.parse_class_or_def(i)

    # Skip the keyword.
    i = self.next(i)

    # Find the trailing ':' and set the context.
    end = self.find_delim(i, len(self.tokens), [':'])
    self.expect_op(end, ':')
    self.set_context(end, 'end-statement')

    # A harmless hack: treat the rest of the statement as an expression.
    self.parse_expr(i, end)

    # Scan the ':'.
    return self.next(end)
#@+node:ekr.20240115074103.1: *5* tbo.parse_decorator
def parse_decorator(self, i: int) -> int:

    # Scan the @
    self.expect_op(i, '@')
    i = self.next(i)

    # Scan the name.
    self.expect(i, 'name')
    i = self.next(i)

    # Set context for any arguments as if they were call arguments.
    if self.is_op(i, '('):
        i = self.parse_call_args(i, len(self.tokens))
        self.expect_op(i, ')')
        i = self.next(i)
    return i

#@+node:ekr.20240120202324.1: *5* tbo.parse_expr & helpers
def parse_expr(self, i: int, end: int) -> int:
    """
    Parse an expression spanning self.tokens[i:end],
    looking for the following token patterns:

    - A function call:  'name', '(', ..., ')'.
    - A slice:          '[', ..., ']'.
    - A dictionary:     '{', ..., '}'.

    Set the appropriate context for all inner expressions.
    """

    trace = False  ### For now.
    if trace:
        print('')
        self.trace(i, end)

    # Scan an arbitrary expression, bounded only by end.
    while i < end:
        progress = i
        token = self.tokens[i]
        kind, value = token.kind, token.value

        if kind == 'name':
            _is_complex, i = self.parse_name(i, end)
        elif kind == 'op':
            if value == '(':
                i = self.parse_parenthesized_expr(i, end)
            elif value == '[':
                i = self.parse_slice(i, end)
            elif value == '{':
                i = self.parse_dict_or_set(i, end)
            else:
                i = self.next(i)
        else:
            i = self.next(i)

        assert i is not None, token
        assert progress < i, (i, token)
    return end
#@+node:ekr.20240121073127.1: *6* tbo.parse_dict_or_set
def parse_dict_or_set(self, i1: int, end: int) -> int:
    """
    Parse a '{', ..., '}'.

    Set context for '=' tokens to 'dict' only within dictionaries.
    """

    # Scan the '{'.
    self.expect_op(i1, '{')

    # Find the matching '}'
    i = self.next(i1)
    i2 = self.find_delim(i, end, ['}'])
    self.expect_op(i2, '}')

    # Sanity check.
    assert i2 <= end, (repr(i2), repr(end))

    # Search for ':' at the top level.
    colon_i = self.find_delim(i, i2, [':'])
    if colon_i is None:
        # The opening '{' starts a non-empty set.
        # Note: the Tokenizer converts all f-strings tokens to a single 'string' token.
        pass
    else:
        # The opening '{' starts a non-empty dict.
        self.parse_dict(i1, i2)
    return self.next(i2)
#@+node:ekr.20240121073925.1: *6* tbo.parse_dict
def parse_dict(self, i1: int, end: int) -> None:
    """
    Parse '[', ..., ']'.

    Set the context of all outer-level ':' tokens to 'dict'.

    Return None.
    """

    # Initial sanity checks.
    self.expect_op(i1, '{')
    self.expect_op(end, '}')

    # Set context for all outer-level ':' tokens to 'dict'.
    i = self.next(i1)
    while i < end:
        i = self.find_delim(i, end, [':'])
        if i is None:
            break
        self.set_context(i, 'dict')
        i = self.next(i)
#@+node:ekr.20240121071949.1: *6* tbo.parse_parenthesized_expr
def parse_parenthesized_expr(self, i: int, end: int) -> int:
    """
    Parse a parenthesized expression.

    Such expressions have no context.
    """
    # Scan the '('.
    self.expect_op(i, '(')

    # Find the matching ')'.
    i = self.next(i)
    i2 = self.find_delim(i, end, [')'])
    self.expect_op(i2, ')')

    # Parse the insides of the expresssion.
    self.parse_expr(i + 1, i2 - 1)

    # Sanity check.
    assert i2 <= end, (repr(i2), repr(end))
    return i2
#@+node:ekr.20240121024213.1: *6* tbo.parse_slice
def parse_slice(self, i1: int, end: int) -> int:
    """
    Parse '[', ..., ']'.

    Set the context for ':' tokens to 'simple-slice' or 'complex-slice'.
    """

    # self.trace(i1, end)  ###

    # Scan the '['.
    self.expect_op(i1, '[')
    i = self.next(i1)

    i2 = self.find_delim(i, end, [']'])
    self.expect_op(i2, ']')
    assert i2 <= end, (repr(i2), repr(end))

    # Context data.
    colons: list[int] = []  # List of outer ':' to be given context.
    final_context: str = 'simple-slice'  # May become 'complex-slice'.
    inter_colon_tokens = 0

    def update_context(i: int) -> None:
        nonlocal colons, final_context, inter_colon_tokens

        # Ignore '.' tokens and the preceding 'name' token.
        if self.is_op(i, '.'):
            prev = self.prev(i)
            if self.is_name(prev):
                inter_colon_tokens -= 1
            return

        # *Now* we can update the effective complexity of the slice.
        inter_colon_tokens += 1
        if inter_colon_tokens > 1:
            final_context = 'complex-slice'

    # Parse the slice to discover possible inner function calls!
    while i <= i2:  # Required.
        progress = i
        token = self.tokens[i]
        kind, value = token.kind, token.value

        if kind == 'name':
            is_complex, i = self.parse_name(i, end)
            if is_complex:
                final_context = 'complex-slice'
        elif kind == 'op':
            if value == ']':
                # An outer ']'
                i = self.next(i)
                break
            if value == ':':
                # An outer ':'.
                colons.append(i)
                inter_colon_tokens = 0
                i = self.next(i)
            elif value in '+-':
                # Don't update context for unary ops.
                if not self.is_unary_op(i, value):
                    update_context(i)
                i = self.next(i)
            elif value == '(':
                i = self.parse_parenthesized_expr(i, end)
                final_context = 'complex-slice'
            elif value == '[':
                i = self.parse_slice(i, end)
                final_context = 'complex-slice'
            elif value == '{':
                i = self.parse_dict_or_set(i, end)
                final_context = 'complex-slice'
            else:
                update_context(i)
                i = self.next(i)
        else:
            i = self.next(i)

        assert i is not None, (token)
        assert progress < i, (i, token)

    # Set the context of all outer-level ':' tokens.
    for colon_i in colons:
        self.set_context(colon_i, final_context)

    return self.next(i2)  # Ignore i.
#@+node:ekr.20240107143500.1: *5* tbo.parse_from
def parse_from(self, i: int) -> int:
    """
    Parse a `from x import` statement, setting context for leading '.'
    tokens.
    """
    # Find the end of the 'from' statement.
    i = self.index
    end = self.find_end_of_line(i)

    # Add 'from' context to all '.' tokens.
    while i and i < end:
        if self.is_op(i, '.'):
            self.set_context(i, 'from')
        i = self.next(i)

    return end
#@+node:ekr.20240108083829.1: *5* tbo.parse_import
def parse_import(self, i: int) -> int:
    # Find the end of the import statement.
    i = self.index
    end = self.find_end_of_line(i)

    # Add 'import' context to all '.' operators.
    while i and i < end:
        if self.is_op(i, '.'):
            self.set_context(i, 'import')
        i = self.next(i)
    return end
#@+node:ekr.20240106181215.1: *5* tbo.parse_initializer
def parse_initializer(self, i1: int, end: int) -> int:
    """
    Scan an initializer in a function call or function definition argument.
    """

    if 0:  ### trace.
        self.trace(i1)

    # Scan the '='.
    self.expect_op(i1, '=')

    # Sanity check.
    context = self.tokens[i1].context
    assert context in ('initializer', 'annotation'), repr(context)

    i = self.next(i1)

    # Scan an arbitrary expression, separated by ',' or ')'.
    while i <= end:
        progress = i
        token = self.tokens[i]
        kind, value = token.kind, token.value

        if kind == 'name':
            _is_complex, i = self.parse_name(i, end)
        elif kind == 'op':
            if value in ',)':
                break
            i = self.parse_op(i, end)
        else:
            i = self.next(i)

        assert i is not None, token
        assert progress < i, (i, token)

    # Same expect as in the caller.
    self.expect_ops(i, [',', ')'])
    return i
#@+node:ekr.20240116040636.1: *5* tbo.parse_outer_expression
def parse_outer_expression(self, i1: int) -> int:
    << docstring: tbo.parse_outer_expression >>

    token = self.tokens[i1]
    assert self.is_significant_token(token), (token, g.callers())

    # Just scan outer strings and f-strings at the outer level.
    if token.kind in self.string_kinds:
        return self.next(i1)

    # Find the bounds of the expression.
    # This prepass simplifies all the inner logic.

    # Because expressions are scanned twice the token_ratio in
    # the top-level orange_command should be > 2.0.

    end = self.find_end_of_line(i1)
    i = self.parse_expr(i1, end)
    return i
#@+node:ekr.20240120201047.1: *6* << docstring: tbo.parse_outer_expression >>
"""
Parse a line that *isn't* a simple or compound statement.
See https://docs.python.org/3/reference/expressions.html

This method's only purpose is to mark the context of '=' and ':'
tokens. It can ignore unrelated subtleties of Python's grammar.

1. If the next significant token is a string, just scan it.

2. Otherwise, we are looking *only* for the following token patterns:

- A function call:  'name', '(', ... ')'.
- A slice:          '[', <expr>? ':' <expr> ':'? <expr>? ']'.
- A dictionary:     '{', <expr> ':' <expr> '}'.

Notes:

1. <expr> denotes an inner expression.
2. ? denotes optional tokens or expressions.
3. '=' tokens within function calls always have 'initializer' context.
"""
#@+node:ekr.20240109032639.1: *5* tbo.parse_simple_statement
def parse_simple_statement(self, i: int) -> int:
    """
    Scan to the end of a simple statement like an `import` statement.
    """

    # Sanity check.
    self.expect_name(i)
    end = self.find_end_of_line(i)
    token = self.get_token(end)
    assert token.context == 'end-statement'

    # Treat the rest of the *statement* as an expression.
    i2 = self.next(i)
    self.parse_expr(i2, end)
    return end
#@+node:ekr.20240113054629.1: *4* tbo.parse_statements (top-level of parser)
def parse_statements(self) -> None:
    """
    parse_statements: scan (parse) the entire file.

    This is the entry point for a "good enough" recursive-descent
    parser who's *only* purpose is to add context to a few kinds
    of tokens.  See set_context for details.
    """
    i = self.index
    assert i == 0, repr(i)

    # Skip the encoding token.
    i = self.next(i)
    while i is not None:
        # Set the 'index' and 'token' ivars for each statement.
        self.index = i
        self.token = self.tokens[i]

        # Parse the statement.
        i = self.parse_statement(i)
#@+node:ekr.20240129094058.1: *3* ratio code from orange_command
# Report any unusual scanned/total ratio.
scanned, tokens = tbo.n_scanned_tokens, len(tbo.tokens)
token_ratio: float = scanned / tokens

# Check the ratio: calls to tbo.next/total tokens.
# This check verifies that there are no serious performance issues.
# For Leo's sources, this ratio ranges between 0.48 and 1.51.
if token_ratio > 2.5:
# A useful performance measure.
    print('')
    g.trace(
        f"Unexpected token ratio in {g.shortFileName(filename)}\n"
        f"scanned: {scanned:<5} total: {tokens:<5} ratio: {token_ratio:4.2f}"
    )
elif 0:  # Print all ratios.
    print(
        f"scanned: {scanned:<5} total: {tokens:<5} ratio: {token_ratio:4.2f} "
        f"{g.shortFileName(filename)}"
    )
#@+node:ekr.20240129094959.1: *3* from tbo: Scanning
if 0:
    @others
#@+node:ekr.20240114022135.1: *4* tbo.find_close_paren
def find_close_paren(self, i1: int) -> Optional[int]:
    """Find the  ')' matching this '(' token."""

    self.expect_op(i1, '(')
    i = self.next(i1)
    level = 0
    while i < len(self.tokens):
        if self.is_op(i, '('):
            level += 1
        elif self.is_op(i, ')'):
            if level == 0:
                return i
            if level <= 0:
                self.oops(f"Unbalanced parens: {self.token.line!r}")  # pragma: no cover
            level -= 1
        i += 1
    self.oops("Unmatched '('")  # pragma: no cover
    return None  # pragma: no cover
#@+node:ekr.20240114063347.1: *4* tbo.find_delim
def find_delim(self, i1: int, end: int, delims: list) -> int:
    """
    Find the next delimiter token, skipping inner expressions.
    Return None if not found. It's not necessarily an error.

    The *caller* is responsible for scanning an opening '(', '[', or '{'
    token when the delims list contains ')', ']', or '}'.

    The *caller* is responsible for scanning the token at the returned index.
    """
    trace = False  ###
    if trace:  # pragma: no cover
        g.trace(f" {i1:3} {g.callers(1):25} {delims} {self.get_token_line(i1)}")

    # We expect only the following 'op' delims: ',', '=', ')' and ':'.
    for z in delims:
        if z not in ',=)}]:':
            self.oops(f"Invalid delim: {z!r}")  # pragma: no cover

    # Skip tokens until one of the delims is found.
    i = i1
    while i <= end:  # '<=' is required.
        token = self.tokens[i]
        if token.kind == 'op':
            value = token.value
            if value in delims:
                if trace:  ### pragma: no cover
                    token = self.tokens[i]
                    token_s = f"{token.kind:} {token.value!r}"
                    g.trace(f" {i:3} Returns {token_s}\n")
                return i
            if value == '[':
                i = self.skip_past_matching_delim(i, '[', ']')
            elif value == '(':
                i = self.skip_past_matching_delim(i, '(', ')')
            elif value == '{':
                i = self.skip_past_matching_delim(i, '{', '}')
            else:
                ### i = self.next(i)
                i += 1
        else:
            ### i = self.next(i)
            i += 1
    return None
#@+node:ekr.20240110062055.1: *4* tbo.find_end_of_line
def find_end_of_line(self, i: int) -> Optional[int]:
    """
    Return the index the next 'newline', 'nl' or 'endmarker' token,
    skipping inner expressions.

    Set the context of found token to 'end-statement.
    Do *not* set the context of any other token.
    """
    while i < len(self.tokens):
        token = self.tokens[i]
        if token.kind in ('newline', 'nl', 'endmarker'):
            self.set_context(i, 'end-statement')
            return i
        if token.kind == 'op':
            value = token.value
            if value == '[':
                i = self.skip_past_matching_delim(i, '[', ']')
            elif value == '(':
                i = self.skip_past_matching_delim(i, '(', ')')
            elif value == '{':
                i = self.skip_past_matching_delim(i, '{', '}')
            else:
                ### i = self.next(i)
                i += 1
        else:
            ### i = self.next(i)
            i += 1
    self.oops("no matching ')'")  # pragma: no cover
    return len(self.tokens)  # pragma: no cover
#@+node:ekr.20240127053011.1: *4* tbo.get_tokens_after
def get_tokens_after(self, i: int) -> str:
    """Return the string containing the values of self.tokens[i:]."""
    try:
        tokens = self.tokens[i:]
    except Exception as e:  # pragma: no cover
        self.oops(f"Invalid index: {i!r}: {e}")
    end = self.find_end_of_line(i)
    return repr(''.join([z.value for z in tokens[i:end]]))
#@+node:ekr.20240125082325.1: *4* tbo.is_name
def is_name(self, i: int) -> bool:

    if i is None:  # pragma: no cover
        return False
    token = self.tokens[i]
    return token.kind == 'name'
#@+node:ekr.20240106172054.1: *4* tbo.is_op & is_ops
def is_op(self, i: int, value: str) -> bool:

    if i is None:  # pragma: no cover
        return False
    token = self.tokens[i]
    return token.kind == 'op' and token.value == value

def is_ops(self, i: int, values: list[str]) -> bool:

    if i is None:  # pragma: no cover
        return False
    token = self.tokens[i]
    return token.kind == 'op' and token.value in values
#@+node:ekr.20240115072231.1: *4* tbo.skip_past_matching_delim
def skip_past_matching_delim(self, i: int, delim1: str, delim2: str) -> int:
    """
    Skip from delim1 *past* the matching delim2.
    Raise InternalBeautifierError if a matching delim2 is not found.
    """
    self.expect_op(i, delim1)
    i = self.next(i)
    while i < len(self.tokens):
        progress = i
        token = self.tokens[i]
        if token.kind == 'op':
            value = token.value
            if value == delim2:
                return i + 1  # Skip the closing delim
            if value == '[':
                i = self.skip_past_matching_delim(i, '[', ']')
            elif value == '(':
                i = self.skip_past_matching_delim(i, '(', ')')
            elif value == '{':
                i = self.skip_past_matching_delim(i, '{', '}')
            else:
                i = self.next(i)
        else:
            i = self.next(i)
        if progress >= i:
            self.oops('no progress!')
    self.oops(f"no matching {delim2!r}")  # pragma: no cover
    return None  # pragma: no cover
#@+node:ekr.20240131065522.1: *3* from tbo: Checking & dumping
#@+node:ekr.20240106090914.1: *4* tbo.expect
def expect(self, i: int, kind: str, value: str = None) -> None:
    """Raise an exception if self.tokens[i] is not as expected."""
    try:
        token = self.tokens[i]
    except Exception as e:  # pragma: no cover
        self.oops(f"At index {i!r}: Expected{kind!r}:{value!r}, got {e}")

    if token.kind != kind or (value and token.value != value):
        self.oops(f"Expected {kind!r}:{value!r}, got {token!r}")  # pragma: no cover
#@+node:ekr.20240116042811.1: *4* tbo.expect_name
def expect_name(self, i: int) -> None:
    """Raise an exception if self.tokens[i] is not as expected."""
    try:
        token = self.tokens[i]
    except Exception as e:  # pragma: no cover
        self.oops(f"At index {i!r}: Expected 'name', got {e}")

    if token.kind != 'name':
        self.oops(f"Expected 'name', got {token!r}")  # pragma: no cover
#@+node:ekr.20240114015808.1: *4* tbo.expect_op
def expect_op(self, i: int, value: str) -> None:
    """Raise an exception if self.tokens[i] is not as expected."""
    try:
        token = self.tokens[i]
    except Exception as e:  # pragma: no cover
        self.oops(f"At index {i!r}: Expected 'op':{value!r}, got {e!r}")

    if (token.kind, token.value) != ('op', value):
        self.oops(f"Expected 'op':{value!r}, got {token!r}")  # pragma: no cover
#@+node:ekr.20240114013952.1: *4* tbo.expect_ops
def expect_ops(self, i: int, values: list) -> None:
    """Raise an exception if self.tokens[i] is not as expected."""
    try:
        token = self.tokens[i]
    except Exception as e:  # pragma: no cover
        self.oops(f"At index {i!r}: Expected 'op' in {values!r}, got {e!r}")

    if token.kind != 'op' or token.value not in values:
        self.oops(f"Expected 'op' in {values!r}, got {token!r}")  # pragma: no cover
#@+node:ekr.20240127051941.1: *4* tbo.get_token
def get_token(self, i: int) -> InputToken:
    """Return the token at i, with full error checking."""
    try:
        token = self.tokens[i]
    except Exception as e:  # pragma: no cover
        self.oops(f"Invalid index: {i!r}: {e}")
    return token
#@+node:ekr.20240125182219.1: *4* tbo.trace & helper
def trace(self, i: int, i2: Optional[int] = None, *, tag: str = None) -> None:  # pragma: no cover
    """
    Print i, token, and get_token_line(i).

    A surprisingly useful debugging utility.
    """

    token = self.tokens[i]
    indices_s = f"{i}" if i2 is None else f"i: {i} i2: {i2}"

    # Adjust widths below as necessary.
    print(
        f"{g.callers(1)}: {tag or ''}\n"
        f"  callers: {g.callers()}\n"
        f"  indices: {indices_s} token: {token.kind:}:{token.show_val(30)}\n"
        f"     line: {self.get_token_line(i)!r}\n"
    )
#@+node:ekr.20240124094344.1: *5* tbo.get_token_line
def get_token_line(self, i: int) -> str:  # pragma: no cover
    """return self.tokens[i].line"""
    try:
        token = self.tokens[i]
    except Exception as e:
        self.oops(f"Bad token index {i!r}: {e}")

    return token.line.rstrip()
#@+node:ekr.20240201023006.1: *3* deleted unit tests
#@+node:ekr.20240124230807.1: *4* TestTBO.xxx_test_assignment
def xxx_test_assignment(self):

    # From leoFileCommands.py.
    # The oops occurred in the *first* line because of the *second* line.
    contents = """
        c.hiddenRootNode.children = rootChildren
        (w, h, x, y, r1, r2, encoding) = fc.getWindowGeometryFromDb(conn)
    """
    contents, tokens = self.make_data(contents)
    expected = self.blacken(contents).rstrip() + '\n'
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
#@+node:ekr.20240105153425.52: *4* TestTBO.xxx_test_bug_1851
def xxx_test_bug_1851(self):

    contents = r'''\
def foo(a1):
    pass
'''
    contents, tokens = self.make_data(contents)
    expected = contents.rstrip() + '\n'
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
#@+node:ekr.20240116155903.1: *4* TestTBO.xxx_test_colorizer_fail
def xxx_test_colorizer_fail(self):

    contents = '''

        # leoColorizer.py: line 2268
        if j > len(s) and not dots:
            j = len(s) + 1

            def span(s: str) -> int:
                # Note: bindings are frozen by this def.
                return self.restart_match_span(s, kind,
                    delegate=delegate,
                )
    '''

    contents, tokens = self.make_data(contents)
    # dump_tokens(tokens)
    expected = contents
    results = self.beautify(contents, tokens)
    if False and results != expected:
        g.printObj(expected, tag='Expected (same as Contents)')
        g.printObj(results, tag='Results')
    self.maxDiff = None
    self.assertEqual(results, expected)
#@+node:ekr.20240112134732.1: *4* TestTBO.xxx_test_def_colons
def xxx_test_def_colons(self):

    contents = '''
        self.configDict: dict[str, Any] = {}
        self.configUnderlineDict: dict[str, bool] = {}
    '''
    contents, tokens = self.make_data(contents)
    # dump_tokens(tokens)
    expected = contents
    results = self.beautify(contents, tokens)
    if results != expected:  # pragma: no cover
        # g.printObj(contents, tag='Contents')
        g.printObj(expected, tag='Expected (same as Contents)')
        g.printObj(results, tag='Results')

    self.maxDiff = None
    self.assertEqual(results, expected)

#@+node:ekr.20240114100847.1: *4* TestTBO.xxx_test_def_square_brackets
def xxx_test_def_square_brackets(self):

    contents = (
        """def checkForDuplicateShortcuts(self, c: Cmdr, d: dict[str, str]) -> None:\n"""
    )
    contents, tokens = self.make_data(contents)
    # dump_tokens(tokens)
    expected = contents
    results = self.beautify(contents, tokens)
    if False and results != expected:
        # g.printObj(contents, tag='Contents')
        g.printObj(expected, tag='Expected (same as Contents)')
        g.printObj(results, tag='Results')

    self.maxDiff = None
    self.assertEqual(results, expected)
#@+node:ekr.20240116072845.1: *4* TestTBO.xxx_test_expressions
def xxx_test_expressions(self):

    contents = """skip_count = max(0, (len(target) - 1))\n"""

    contents, tokens = self.make_data(contents)
    expected = self.blacken(contents)
    results = self.beautify(contents, tokens)
    if expected != results:  # pragma: no cover
        g.printObj(expected, tag='Explected (blackened)')
        g.printObj(results, tag='Results')
    self.assertEqual(results, expected)
#@+node:ekr.20240124092041.1: *4* TestTBO.xxx_test_fstrings
def xxx_test_fstrings(self):

    # leoApp.py, line 885.
    contents = """signon = [f"Leo {leoVer}"]\n"""
    contents, tokens = self.make_data(contents)
    expected = self.blacken(contents)
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
#@+node:ekr.20240126062946.1: *4* TestTBO.xxx_test_function_call_with_parens
def xxx_test_function_calls_with_parens(self):

    # LeoFrame.py, line 1650.

    # Test 1: The 'if' statement on a single line.
    contents1 = """
        if (g.doHook("select1", c = c, new_p=p)):
            return
        """

    # Black would remove the outer parens.
    expected1 = textwrap.dedent(
        """
        if (g.doHook("select1", c=c, new_p=p)):
            return
        """).strip() + '\n'

    # Test 2: The 'if' statement spans several lines.
    contents2 = """
        if (
            whatever and g.doHook(
                "select1", c = c, new_p=p)
        ):
            return
        """

    # Black would remove the outer parens.
    expected2 = textwrap.dedent(
        """
        if (
            whatever and g.doHook(
                "select1", c=c, new_p=p)
        ):
            return
        """).strip() + '\n'

    table = (
        (contents1, expected1),
        (contents2, expected2),
    )
    for contents, expected in table:
        contents, tokens = self.make_data(contents)
        results = self.beautify(contents, tokens)
        if results != expected:
            if 1:
                g.printObj(contents, tag='Contents')
                g.printObj(expected, tag='Expected (blackened)')
                g.printObj(results, tag='Results')
            self.assertEqual(expected, results)
#@+node:ekr.20240107080413.1: *4* TestTBO.xxx_test_function_calls
def xxx_test_function_calls(self):

    # Put recent failures first.
    table = (
        # leoserver.py, line 584.
        """timeline.sort(key=lambda x: x[0].gnx, reverse=True)""",

        # leoserver.py, line 42 and other lines.
        """jsonPackage = json.dumps(package, separators=(',', ':'), cls=SetEncoder)""",

        # #1429: https://github.com/leo-editor/leo-editor/issues/1429
        """
        version = str(version2.Version.coerce(tag, partial=True))
        """,

        # LeoApp.py, line 1872.
        """
        if path.startswith(tag):
            return self.computeBindingLetter(c, path=path[len(tag) :])
        """,

        # LeoApp.py, line 3416.
        """
        if groupedEntries:
            dirCount: dict[str, Any] = {}
            for fileName in rf.getRecentFiles()[:n]:
                dirName, baseName = g.os_path_split(fileName)
        """,

        # leoPersistence, line 526.
        """
        return '-->'.join(reversed(
            [self.expected_headline(p2) for p2 in p.self_and_parents(copy=False)]))
        """,

        # leoNodes, line 881.
        """
        path_part = '-->'.join(list(reversed([z.h for z in self.self_and_parents(copy=False)])))
        """,

        # leoserver.py, line 2302.
        """
        result = [
            self._get_position_d(p, c) for p in c.all_positions(copy=False)
        ]
        """,

        # leoApp, line 1657
        """
        if True:
            home = os.getenv(home[1:-1], default=None)
        """,
    )
    for i, contents in enumerate(table):
        contents, tokens = self.make_data(contents)
        expected = contents  # Black would join lines.
        results = self.beautify(contents, tokens)
        if results != expected:
            # dump_tokens(tokens)
            if 1:
                print('')
                # g.printObj(contents, tag='Contents')
                g.printObj(expected, tag='Expected (same as contents)')
                g.printObj(results, tag='Results')
            self.assertEqual(expected, results)
#@+node:ekr.20240105153425.57: *4* TestTBO.xxx_test_function_defs
def xxx_test_function_defs(self):

    long_table = (
        # Case 1.
        """
            def f1(a=2 + 5):
                pass
        """,
        # Case 2
        """
            def f2(a):
                pass
        """,
        # Case 3.
        """
        def f3(a: int = 2):
            pass
        """,
        # Case 4.
        '''
        def should_kill_beautify(p):
            """Return True if p.b contains @killbeautify"""
            return 'killbeautify' in g.get_directives_dict(p)
        ''',
        # Case 5 (new)
        '''
            def reloadSettings():
                pass
        ''',
        # Case 6 (new)
        '''
            def tuple_init(stack: Sequence[str] = ('root',)) -> Generator:
                pass
        ''',
        # Case 7 (new)
    )
    short_table = (
        '''
            def tuple_init(stack: Sequence[str] = ('root',)) -> Generator:
                pass
        ''',
    )
    assert long_table and short_table
    table = long_table
    for i, contents in enumerate(table):
        contents, tokens = self.make_data(contents)
        expected = self.blacken(contents).rstrip() + '\n'
        results = self.beautify(contents, tokens)
        if expected != results:  # pragma: no cover
            g.printObj(expected, tag='Explected (blackened)')
            g.printObj(results, tag='Results')
        self.assertEqual(results, expected)
#@+node:ekr.20240105153425.63: *4* TestTBO.xxx_test_leading_stars
def xxx_test_leading_stars(self):

    # #2533.
    contents = """
        def f(
            arg1,
            *args,
            **kwargs
        ):
            pass
    """
    contents, tokens = self.make_data(contents)
    if 1:  # w/o join:
        expected = contents
    else:  # with join.
        expected = """
            def f(arg1, *args, **kwargs):
                pass
        """
    results = self.beautify(contents, tokens)
    self.assertEqual(expected, results)
#@+node:ekr.20240105153425.64: *4* TestTBO.xxx_test_leading_stars_one_line
def xxx_test_leading_stars_one_line(self):

    # #2533.
    contents = """
        def f(arg1, *args, **kwargs):
            pass
    """
    contents, tokens = self.make_data(contents)
    results = self.beautify(contents, tokens)
    self.assertEqual(contents, results)
#@+node:ekr.20240105153425.71: *4* TestTBO.xxx_test_return
def xxx_test_return(self):

    contents = """return []"""
    expected = self.blacken(contents)
    contents, tokens = self.make_data(contents)
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
#@+node:ekr.20240105153425.72: *4* TestTBO.xxx_test_single_quoted_string
def xxx_test_single_quoted_string(self):

    contents = """print('hi')"""
    # blacken suppresses string normalization.
    expected = self.blacken(contents)
    contents, tokens = self.make_data(contents)
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
#@+node:ekr.20240105153425.78: *4* TestTBO.xxx_test_ternary
def xxx_test_ternary(self):

    contents = """print(2 if name == 'class' else 1)"""
    contents, tokens = self.make_data(contents)
    expected = contents
    results = self.beautify(contents, tokens)
    self.assertEqual(results, expected)
#@-all
#@@nosearch
#@-leo
